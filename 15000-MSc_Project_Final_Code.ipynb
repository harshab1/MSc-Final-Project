{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import re\n",
    "import pickle \n",
    "from unicodedata import normalize\n",
    "import numpy\n",
    "from numpy import array\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "from numpy import argmax\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN,GRU,LSTM\n",
    "from keras.layers import Dense, Embedding, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Archive:  fra-eng.zip',\n",
       " 'replace _about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL',\n",
       " '(EOF or read error, treating as \"[N]one\" ...)']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetching and decompressing the dataset \n",
    "!!curl -O http://www.manythings.org/anki/fra-eng.zip\n",
    "!!unzip fra-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function performs two tasks:\n",
    "\n",
    "# 1. Loading the text data preserving the Unicode french characters and then\n",
    "#Â 2. Each line of the text file contain English sentence and its French translation seperated by tab character. \n",
    "\n",
    "def loading_and_pairs(file):\n",
    "    \n",
    "\t# opening the text file in read only mode with unicode encoding\n",
    "\tf = open(file = file, mode = 'rt', encoding = 'utf-8')\n",
    "    \n",
    "\t# reading the text from the opened file\n",
    "\ttext = f.read()\n",
    "    \n",
    "\t# finally closing the file\n",
    "\tf.close()\n",
    "    \n",
    "    # Obtaining each line in the file\n",
    "\tl = text.strip().split('\\n')\n",
    "    \n",
    "    # Obtaining the pairs of English sentence and its french translation\n",
    "\tpairs = [l_each.split('\\t') for l_each in  l]\n",
    "    \n",
    "\treturn pairs   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the lines by removing all the non-printable characters, punctuation characters\n",
    "# Given a list of lines cleaning them\n",
    "\n",
    "def pairs_clean(lines_from_text):\n",
    "    \n",
    "\tnew_cleaned = list()\n",
    "    \n",
    "\t# using regular expression for removing non-printable characters\n",
    "\tregular_non_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    \n",
    "\t# using regular expression for removing punctuation characters and obtaining translation table\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "\tfor pair in lines_from_text:\n",
    "        \n",
    "\t\tclean_pair = list()\n",
    "\t\tfor new_line in pair:\n",
    "            \n",
    "\t\t\t# normalization to remove canonical and compatibility related issues\n",
    "\t\t\tnew_line = normalize('NFD', new_line).encode('ascii', 'ignore')\n",
    "\t\t\tnew_line = new_line.decode('UTF-8')\n",
    "            \n",
    "\t\t\t# tokenizing the white space\n",
    "\t\t\tnew_line = new_line.split()\n",
    "            \n",
    "\t\t\t# normalizing the text to lowercase\n",
    "\t\t\tnew_line = [word.lower() for word in new_line]\n",
    "            \n",
    "\t\t\t# removing punctuation from each token using regular expression table \n",
    "\t\t\tnew_line = [word.translate(table) for word in new_line]\n",
    "            \n",
    "\t\t\t# removing non-printable characters using the above regular expression\n",
    "\t\t\tnew_line = [regular_non_print.sub('', w) for w in new_line]\n",
    "            \n",
    "\t\t\t# removing the non-alphabetic tokens such as numbers\n",
    "\t\t\tnew_line = [word for word in new_line if word.isalpha()]\n",
    "            \n",
    "\t\t\t# store as string\n",
    "\t\t\tclean_pair.append(' '.join(new_line))\n",
    "\t\tnew_cleaned.append(clean_pair)\n",
    "\treturn array(new_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the list of clean sentences to file\n",
    "def saving_data(sentences, file):\n",
    "\tpickle.dump( sentences, open(file = file, mode = 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the text dataset\n",
    "text_file = 'fra.txt'\n",
    "text_pairs = loading_and_pairs(text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the sentences\n",
    "cleaned_pairs = pairs_clean(text_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the cleaned pairs to file\n",
    "saving_data(cleaned_pairs, 'eng-fre.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There a total of 190206 pairs of tranlations in the dataset\n"
     ]
    }
   ],
   "source": [
    "# Total number of translation sentences\n",
    "print(\"There a total of {} pairs of tranlations in the dataset\".format(cleaned_pairs.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English : weve studied french for several years\n",
      "French : nous avons etudie le francais pendant plusieurs annees \n",
      "\n",
      "English : i go there every year\n",
      "French : jy vais tous les ans \n",
      "\n",
      "English : i want to thank you for your time\n",
      "French : je veux vous remercier pour votre temps \n",
      "\n",
      "English : please help yourself to the cookies\n",
      "French : veuillez vous servir des biscuits \n",
      "\n",
      "English : this dictionary is mine\n",
      "French : ce dictionnaire est a moi \n",
      "\n",
      "English : i dont think i can help you\n",
      "French : je ne pense pas pouvoir vous aider \n",
      "\n",
      "English : im willing to risk that\n",
      "French : je suis pret a prendre ce risque \n",
      "\n",
      "English : please show me the menu\n",
      "French : montrezmoi le menu sil vous plait \n",
      "\n",
      "English : how did you get that\n",
      "French : dou tienstu ceci \n",
      "\n",
      "English : i dont believe it happened that way\n",
      "French : je ne crois pas que ca se soit passe de cette maniere \n",
      "\n",
      "English : when did you start dating\n",
      "French : quand avezvous commence a sortir ensemble \n",
      "\n",
      "English : dublin is in ireland\n",
      "French : dublin est en irlande \n",
      "\n",
      "English : the first time you meet people you should be careful about how near you stand to them\n",
      "French : la premiere fois que tu rencontres une personne tu devrais etre attentive a la proximite avec laquelle tu te tiens par rapport a elle \n",
      "\n",
      "English : you always have the right to refuse treatment however i must explain the potential consequences if that will be your choice\n",
      "French : vous avez toujours le droit de refuser tout traitement cependant je dois vous informer des consequences potentielles de cette decision \n",
      "\n",
      "English : tom isnt a translator\n",
      "French : tom nest pas traducteur \n",
      "\n",
      "English : you may now kiss the bride\n",
      "French : vous pouvez maintenant embrasser la mariee \n",
      "\n",
      "English : would you like to give it a try\n",
      "French : aimeraistu lessayer \n",
      "\n",
      "English : hey dont do that\n",
      "French : he ne faites pas ca \n",
      "\n",
      "English : i saw some small animals running away in all directions\n",
      "French : jai vu de petits animaux courir dans tous les sens \n",
      "\n",
      "English : i come from saitama\n",
      "French : je suis originaire de saitama \n",
      "\n",
      "English : tom asked mary to read him the letter she had gotten from john\n",
      "French : tom demanda a mary de lui lire la lettre quelle avait recue de john \n",
      "\n",
      "English : wasnt he your boyfriend\n",
      "French : netaitil pas ton petit ami \n",
      "\n",
      "English : i was hoping i wouldnt have to work on sunday\n",
      "French : jesperais ne pas devoir travailler le dimanche \n",
      "\n",
      "English : you should go home\n",
      "French : tu devrais rentrer chez toi \n",
      "\n",
      "English : what are they complaining about\n",
      "French : de quoi se plaignentils \n",
      "\n",
      "English : wheres your family tom\n",
      "French : ou est ta famille tom \n",
      "\n",
      "English : we agreed to the plan\n",
      "French : nous tombames daccord sur le projet \n",
      "\n",
      "English : he has done many things for poor people\n",
      "French : il a fait beaucoup de choses pour les pauvres \n",
      "\n",
      "English : whats your blood type\n",
      "French : quel est ton groupe sanguin \n",
      "\n",
      "English : im not going away\n",
      "French : je ne men vais pas \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Looking at few sentences\n",
    "\n",
    "random_sample_list = random.sample(range(0, cleaned_pairs.shape[0]), 30)\n",
    "for i in random_sample_list:\n",
    "\tprint('English : %s\\nFrench : %s \\n' % (cleaned_pairs[i,0], cleaned_pairs[i,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from pickle import dump\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the previously cleaned data \n",
    "def loading_sentences(filename):\n",
    "\treturn pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the raw dataset\n",
    "text_dataset = loading_sentences('eng-fre.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are over 190000 pairs of sentences\n",
    "# It will take long time for training and testing the model\n",
    "# Hence dataset size is reduced\n",
    "num_sentences = 15000 #clean_pairs.shape[0]\n",
    "reduced_dataset_15000 = text_dataset[:num_sentences, :]\n",
    "train_size = numpy.rint(0.7 * num_sentences)\n",
    "validation_size = numpy.rint(0.1 * num_sentences)\n",
    "test_size = numpy.rint(0.2 * num_sentences)\n",
    "\n",
    "# randomly shuffling the dataset\n",
    "shuffle(reduced_dataset_15000)\n",
    "\n",
    "# spliting the reduced dataset into train, validation and test\n",
    "split_1 = int(train_size)\n",
    "split_2 = int(train_size+validation_size)\n",
    "split_3 = int(train_size+validation_size+test_size)\n",
    "train, validation, test = reduced_dataset_15000[:split_1], reduced_dataset_15000[split_1:split_2], reduced_dataset_15000[split_2:split_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10500, 3), (1500, 3), (3000, 3))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, validation.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the reduced dataset to training, validation and testing data\n",
    "\n",
    "saving_data(text_dataset, 'eng-fre-total.pkl')\n",
    "saving_data(train, 'eng-fre-train.pkl')\n",
    "saving_data(validation, 'eng-fre-validation.pkl')\n",
    "saving_data(test, 'eng-fre-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using keras tokenize class, for mapping the words to integers needed for modeling\n",
    "\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the maximum sentence length from the list of phrases \n",
    "\n",
    "def max_length(input_lines):\n",
    "\treturn max(len(each_line.split()) for each_line in input_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding and padding the input and output sequences\n",
    "\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "\t# Each input and output sequence are  encoded to integers\n",
    "\ttext_encoded_integers = tokenizer.texts_to_sequences(lines)\n",
    "\t# Obtained sequences are padded with 0 values at the end to make their lenght as maxmim phrase length\n",
    "\tpadding_sequences = pad_sequences(text_encoded_integers, maxlen=length, padding='post')\n",
    "\treturn padding_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output target sequences (English sentences) has to be one hot encoded as the model will \n",
    "# predicts probability of each word in the vocabulary as output. \n",
    "\n",
    "def encode_output(output_sequences, vocabulary_size):\n",
    "\toutput_list = list()\n",
    "\tfor output_sequence in output_sequences:\n",
    "\t\tcat = to_categorical(output_sequence, num_classes=vocabulary_size)\n",
    "\t\toutput_list.append(cat)\n",
    "\tencoded_output = array(output_list)\n",
    "\tencoded_output = encoded_output.reshape(output_sequences.shape[0], output_sequences.shape[1], vocabulary_size)\n",
    "\treturn encoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse mapping an predicted sequence of integers to a words by looking up tokenizer\n",
    "\n",
    "def reverse_mapping(output_integer, tokenizer):\n",
    "\tfor w, i in tokenizer.word_index.items():\n",
    "\t\tif i == output_integer:\n",
    "\t\t\treturn w\n",
    "\treturn None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping the sequence of integers for generating string of words\n",
    "\n",
    "def predict_sequence(rnn_model, tokenizer, original_data):\n",
    "\tprediction = rnn_model.predict(original_data, verbose=0)[0]\n",
    "\tintegers = [argmax(vector) for vector in prediction]\n",
    "\ttarget_list = list()\n",
    "\tfor i in integers:\n",
    "\t\tword = reverse_mapping(i, tokenizer)\n",
    "\t\tif word is None:\n",
    "\t\t\tbreak\n",
    "\t\ttarget_list.append(word)\n",
    "\treturn ' '.join(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the performance of each model using BLEU score by comparing predicted result to original/expected sequences \n",
    "\n",
    "\n",
    "def model_evaluation(model_name, tokenizer_used, sources_text, raw_text_dataset):\n",
    "\tactual_value, predicted_value = list(), list()\n",
    "\tbleu_scores = []\n",
    "\tfor i, j in enumerate(sources_text):\n",
    "\t\t# translating the encoded input text sequence\n",
    "\t\tj = j.reshape((1, j.shape[0]))\n",
    "\t\ttranslation = predict_sequence(model_name, eng_tokenizer, j)\n",
    "\t\traw_target, raw_src = raw_text_dataset[i][0], raw_text_dataset[i][1]\n",
    "        \n",
    "        # Printing 50 French to English translations by the model\n",
    "        \n",
    "\t\tif i < 50:\n",
    "\t\t\tprint('French(Source) : %s\\nTarget : %s\\nPredicted : %s \\n' % (raw_src, raw_target, translation))\n",
    "        \n",
    "        \n",
    "\t\tactual_value.append([raw_target.split()])\n",
    "\t\tpredicted_value.append(translation.split())\n",
    "        \n",
    "    # Calculating BLEU score\n",
    "\tbleu_score = corpus_bleu(actual_value, predicted_value, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    \n",
    "\t# Print BLEU score\n",
    "\tprint('BLEU score: %f' % bleu_score)\n",
    "    \n",
    "\treturn bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the total, train, validation and test datasets \n",
    "dataset = loading_sentences('eng-fre-total.pkl')\n",
    "train = loading_sentences('eng-fre-train.pkl')\n",
    "valid = loading_sentences('eng-fre-validation.pkl')\n",
    "test = loading_sentences('eng-fre-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(190206, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_dataset_15000.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the French tokenizer\n",
    "french_tokenizer = create_tokenizer(reduced_dataset_15000[:, 1])\n",
    "french_maximum_length = max_length(reduced_dataset_15000[:, 1])\n",
    "french_vocabulary_size = len(french_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French Vocabulary Size: 5753\n",
      "French Maximum Sentence Length: 10\n"
     ]
    }
   ],
   "source": [
    "print('French Vocabulary Size:', french_vocabulary_size)\n",
    "print('French Maximum Sentence Length:', french_maximum_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the English tokenizer\n",
    "eng_tokenizer = create_tokenizer(reduced_dataset_15000[:, 0])\n",
    "english_maximum_length = max_length(reduced_dataset_15000[:, 0])\n",
    "english_vocabulary_size = len(eng_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 2776\n",
      "English Maximum Sentence Length: 5\n"
     ]
    }
   ],
   "source": [
    "print('English Vocabulary Size:',english_vocabulary_size)\n",
    "print('English Maximum Sentence Length:', english_maximum_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 1,\n",
       " 'it': 2,\n",
       " 'you': 3,\n",
       " 'tom': 4,\n",
       " 'im': 5,\n",
       " 'a': 6,\n",
       " 'is': 7,\n",
       " 'me': 8,\n",
       " 'its': 9,\n",
       " 'he': 10,\n",
       " 'was': 11,\n",
       " 'youre': 12,\n",
       " 'are': 13,\n",
       " 'go': 14,\n",
       " 'we': 15,\n",
       " 'be': 16,\n",
       " 'to': 17,\n",
       " 'that': 18,\n",
       " 'this': 19,\n",
       " 'were': 20,\n",
       " 'dont': 21,\n",
       " 'the': 22,\n",
       " 'do': 23,\n",
       " 'get': 24,\n",
       " 'not': 25,\n",
       " 'can': 26,\n",
       " 'ill': 27,\n",
       " 'have': 28,\n",
       " 'up': 29,\n",
       " 'they': 30,\n",
       " 'my': 31,\n",
       " 'no': 32,\n",
       " 'here': 33,\n",
       " 'did': 34,\n",
       " 'come': 35,\n",
       " 'in': 36,\n",
       " 'need': 37,\n",
       " 'like': 38,\n",
       " 'she': 39,\n",
       " 'your': 40,\n",
       " 'let': 41,\n",
       " 'out': 42,\n",
       " 'all': 43,\n",
       " 'love': 44,\n",
       " 'thats': 45,\n",
       " 'him': 46,\n",
       " 'take': 47,\n",
       " 'want': 48,\n",
       " 'us': 49,\n",
       " 'got': 50,\n",
       " 'keep': 51,\n",
       " 'hes': 52,\n",
       " 'help': 53,\n",
       " 'am': 54,\n",
       " 'one': 55,\n",
       " 'stop': 56,\n",
       " 'lets': 57,\n",
       " 'on': 58,\n",
       " 'theyre': 59,\n",
       " 'toms': 60,\n",
       " 'look': 61,\n",
       " 'how': 62,\n",
       " 'who': 63,\n",
       " 'stay': 64,\n",
       " 'well': 65,\n",
       " 'see': 66,\n",
       " 'please': 67,\n",
       " 'saw': 68,\n",
       " 'what': 69,\n",
       " 'try': 70,\n",
       " 'must': 71,\n",
       " 'back': 72,\n",
       " 'home': 73,\n",
       " 'just': 74,\n",
       " 'so': 75,\n",
       " 'know': 76,\n",
       " 'lost': 77,\n",
       " 'now': 78,\n",
       " 'cant': 79,\n",
       " 'leave': 80,\n",
       " 'feel': 81,\n",
       " 'had': 82,\n",
       " 'too': 83,\n",
       " 'felt': 84,\n",
       " 'hate': 85,\n",
       " 'will': 86,\n",
       " 'busy': 87,\n",
       " 'alone': 88,\n",
       " 'good': 89,\n",
       " 'happy': 90,\n",
       " 'there': 91,\n",
       " 'away': 92,\n",
       " 'give': 93,\n",
       " 'down': 94,\n",
       " 'has': 95,\n",
       " 'very': 96,\n",
       " 'them': 97,\n",
       " 'again': 98,\n",
       " 'wait': 99,\n",
       " 'work': 100,\n",
       " 'may': 101,\n",
       " 'at': 102,\n",
       " 'ive': 103,\n",
       " 'hurt': 104,\n",
       " 'for': 105,\n",
       " 'run': 106,\n",
       " 'her': 107,\n",
       " 'safe': 108,\n",
       " 'nice': 109,\n",
       " 'eat': 110,\n",
       " 'ok': 111,\n",
       " 'job': 112,\n",
       " 'car': 113,\n",
       " 'make': 114,\n",
       " 'careful': 115,\n",
       " 'ready': 116,\n",
       " 'time': 117,\n",
       " 'right': 118,\n",
       " 'made': 119,\n",
       " 'big': 120,\n",
       " 'call': 121,\n",
       " 'mine': 122,\n",
       " 'everyone': 123,\n",
       " 'drink': 124,\n",
       " 'fun': 125,\n",
       " 'of': 126,\n",
       " 'left': 127,\n",
       " 'didnt': 128,\n",
       " 'over': 129,\n",
       " 'broke': 130,\n",
       " 'ask': 131,\n",
       " 'tired': 132,\n",
       " 'easy': 133,\n",
       " 'never': 134,\n",
       " 'went': 135,\n",
       " 'old': 136,\n",
       " 'upset': 137,\n",
       " 'off': 138,\n",
       " 'bring': 139,\n",
       " 'whos': 140,\n",
       " 'say': 141,\n",
       " 'still': 142,\n",
       " 'done': 143,\n",
       " 'hurry': 144,\n",
       " 'won': 145,\n",
       " 'sad': 146,\n",
       " 'win': 147,\n",
       " 'bad': 148,\n",
       " 'cold': 149,\n",
       " 'going': 150,\n",
       " 'swim': 151,\n",
       " 'dog': 152,\n",
       " 'came': 153,\n",
       " 'wrong': 154,\n",
       " 'bed': 155,\n",
       " 'early': 156,\n",
       " 'looks': 157,\n",
       " 'talk': 158,\n",
       " 'yours': 159,\n",
       " 'check': 160,\n",
       " 'open': 161,\n",
       " 'money': 162,\n",
       " 'isnt': 163,\n",
       " 'an': 164,\n",
       " 'crazy': 165,\n",
       " 'sit': 166,\n",
       " 'read': 167,\n",
       " 'found': 168,\n",
       " 'lot': 169,\n",
       " 'walk': 170,\n",
       " 'hard': 171,\n",
       " 'sure': 172,\n",
       " 'cool': 173,\n",
       " 'care': 174,\n",
       " 'watch': 175,\n",
       " 'ahead': 176,\n",
       " 'new': 177,\n",
       " 'forget': 178,\n",
       " 'some': 179,\n",
       " 'trust': 180,\n",
       " 'move': 181,\n",
       " 'die': 182,\n",
       " 'with': 183,\n",
       " 'late': 184,\n",
       " 'drunk': 185,\n",
       " 'quiet': 186,\n",
       " 'sing': 187,\n",
       " 'enough': 188,\n",
       " 'likes': 189,\n",
       " 'buy': 190,\n",
       " 'sick': 191,\n",
       " 'start': 192,\n",
       " 'said': 193,\n",
       " 'naive': 194,\n",
       " 'kidding': 195,\n",
       " 'shes': 196,\n",
       " 'loves': 197,\n",
       " 'tell': 198,\n",
       " 'could': 199,\n",
       " 'miss': 200,\n",
       " 'his': 201,\n",
       " 'way': 202,\n",
       " 'hear': 203,\n",
       " 'free': 204,\n",
       " 'quit': 205,\n",
       " 'sleep': 206,\n",
       " 'does': 207,\n",
       " 'wont': 208,\n",
       " 'find': 209,\n",
       " 'remember': 210,\n",
       " 'fat': 211,\n",
       " 'fine': 212,\n",
       " 'called': 213,\n",
       " 'pay': 214,\n",
       " 'door': 215,\n",
       " 'drive': 216,\n",
       " 'funny': 217,\n",
       " 'sorry': 218,\n",
       " 'stupid': 219,\n",
       " 'book': 220,\n",
       " 'more': 221,\n",
       " 'prepared': 222,\n",
       " 'should': 223,\n",
       " 'eyes': 224,\n",
       " 'died': 225,\n",
       " 'close': 226,\n",
       " 'wasnt': 227,\n",
       " 'think': 228,\n",
       " 'id': 229,\n",
       " 'calm': 230,\n",
       " 'why': 231,\n",
       " 'working': 232,\n",
       " 'mary': 233,\n",
       " 'shot': 234,\n",
       " 'cry': 235,\n",
       " 'drop': 236,\n",
       " 'break': 237,\n",
       " 'son': 238,\n",
       " 'beat': 239,\n",
       " 'hows': 240,\n",
       " 'mean': 241,\n",
       " 'french': 242,\n",
       " 'fast': 243,\n",
       " 'lock': 244,\n",
       " 'mad': 245,\n",
       " 'boy': 246,\n",
       " 'knows': 247,\n",
       " 'girls': 248,\n",
       " 'cat': 249,\n",
       " 'life': 250,\n",
       " 'thanks': 251,\n",
       " 'itll': 252,\n",
       " 'lucky': 253,\n",
       " 'clean': 254,\n",
       " 'looked': 255,\n",
       " 'grab': 256,\n",
       " 'dead': 257,\n",
       " 'hold': 258,\n",
       " 'liked': 259,\n",
       " 'sign': 260,\n",
       " 'about': 261,\n",
       " 'lie': 262,\n",
       " 'knew': 263,\n",
       " 'show': 264,\n",
       " 'whats': 265,\n",
       " 'angry': 266,\n",
       " 'rich': 267,\n",
       " 'wake': 268,\n",
       " 'turn': 269,\n",
       " 'serious': 270,\n",
       " 'use': 271,\n",
       " 'stand': 272,\n",
       " 'caught': 273,\n",
       " 'tie': 274,\n",
       " 'lazy': 275,\n",
       " 'nothing': 276,\n",
       " 'hired': 277,\n",
       " 'follow': 278,\n",
       " 'these': 279,\n",
       " 'luck': 280,\n",
       " 'once': 281,\n",
       " 'yourself': 282,\n",
       " 'first': 283,\n",
       " 'took': 284,\n",
       " 'day': 285,\n",
       " 'today': 286,\n",
       " 'by': 287,\n",
       " 'lonely': 288,\n",
       " 'great': 289,\n",
       " 'anybody': 290,\n",
       " 'works': 291,\n",
       " 'cut': 292,\n",
       " 'lying': 293,\n",
       " 'hang': 294,\n",
       " 'seem': 295,\n",
       " 'ignore': 296,\n",
       " 'weird': 297,\n",
       " 'man': 298,\n",
       " 'fired': 299,\n",
       " 'finished': 300,\n",
       " 'listen': 301,\n",
       " 'push': 302,\n",
       " 'terrific': 303,\n",
       " 'relax': 304,\n",
       " 'needs': 305,\n",
       " 'room': 306,\n",
       " 'coming': 307,\n",
       " 'fair': 308,\n",
       " 'play': 309,\n",
       " 'trying': 310,\n",
       " 'live': 311,\n",
       " 'shut': 312,\n",
       " 'relaxed': 313,\n",
       " 'fish': 314,\n",
       " 'hot': 315,\n",
       " 'cute': 316,\n",
       " 'gave': 317,\n",
       " 'shy': 318,\n",
       " 'enjoy': 319,\n",
       " 'joking': 320,\n",
       " 'gun': 321,\n",
       " 'cheated': 322,\n",
       " 'tried': 323,\n",
       " 'inside': 324,\n",
       " 'touch': 325,\n",
       " 'beer': 326,\n",
       " 'true': 327,\n",
       " 'needed': 328,\n",
       " 'nervous': 329,\n",
       " 'loved': 330,\n",
       " 'outside': 331,\n",
       " 'met': 332,\n",
       " 'hope': 333,\n",
       " 'nuts': 334,\n",
       " 'tall': 335,\n",
       " 'weak': 336,\n",
       " 'bit': 337,\n",
       " 'hungry': 338,\n",
       " 'fit': 339,\n",
       " 'paid': 340,\n",
       " 'lied': 341,\n",
       " 'winning': 342,\n",
       " 'told': 343,\n",
       " 'hand': 344,\n",
       " 'really': 345,\n",
       " 'doctor': 346,\n",
       " 'tomll': 347,\n",
       " 'speak': 348,\n",
       " 'cook': 349,\n",
       " 'around': 350,\n",
       " 'kids': 351,\n",
       " 'fight': 352,\n",
       " 'and': 353,\n",
       " 'ate': 354,\n",
       " 'sleepy': 355,\n",
       " 'drank': 356,\n",
       " 'seems': 357,\n",
       " 'awful': 358,\n",
       " 'gone': 359,\n",
       " 'write': 360,\n",
       " 'strong': 361,\n",
       " 'sweet': 362,\n",
       " 'where': 363,\n",
       " 'both': 364,\n",
       " 'unlucky': 365,\n",
       " 'smart': 366,\n",
       " 'anyone': 367,\n",
       " 'wine': 368,\n",
       " 'naked': 369,\n",
       " 'problem': 370,\n",
       " 'wonderful': 371,\n",
       " 'normal': 372,\n",
       " 'joke': 373,\n",
       " 'alive': 374,\n",
       " 'scared': 375,\n",
       " 'key': 376,\n",
       " 'excited': 377,\n",
       " 'agree': 378,\n",
       " 'wants': 379,\n",
       " 'helped': 380,\n",
       " 'best': 381,\n",
       " 'hat': 382,\n",
       " 'vote': 383,\n",
       " 'cried': 384,\n",
       " 'study': 385,\n",
       " 'famous': 386,\n",
       " 'tea': 387,\n",
       " 'talking': 388,\n",
       " 'fix': 389,\n",
       " 'tight': 390,\n",
       " 'hurts': 391,\n",
       " 'correct': 392,\n",
       " 'ran': 393,\n",
       " 'reading': 394,\n",
       " 'wanted': 395,\n",
       " 'soon': 396,\n",
       " 'idea': 397,\n",
       " 'leaving': 398,\n",
       " 'young': 399,\n",
       " 'patient': 400,\n",
       " 'saved': 401,\n",
       " 'god': 402,\n",
       " 'seated': 403,\n",
       " 'seen': 404,\n",
       " 'guess': 405,\n",
       " 'moving': 406,\n",
       " 'dogs': 407,\n",
       " 'bored': 408,\n",
       " 'food': 409,\n",
       " 'quickly': 410,\n",
       " 'better': 411,\n",
       " 'awake': 412,\n",
       " 'much': 413,\n",
       " 'pardon': 414,\n",
       " 'yes': 415,\n",
       " 'rude': 416,\n",
       " 'invited': 417,\n",
       " 'date': 418,\n",
       " 'awesome': 419,\n",
       " 'two': 420,\n",
       " 'fake': 421,\n",
       " 'as': 422,\n",
       " 'deal': 423,\n",
       " 'rest': 424,\n",
       " 'fell': 425,\n",
       " 'pretty': 426,\n",
       " 'plan': 427,\n",
       " 'boss': 428,\n",
       " 'nobody': 429,\n",
       " 'everybody': 430,\n",
       " 'change': 431,\n",
       " 'last': 432,\n",
       " 'cats': 433,\n",
       " 'cruel': 434,\n",
       " 'married': 435,\n",
       " 'smiling': 436,\n",
       " 'dumped': 437,\n",
       " 'lunch': 438,\n",
       " 'asleep': 439,\n",
       " 'school': 440,\n",
       " 'tv': 441,\n",
       " 'arent': 442,\n",
       " 'youve': 443,\n",
       " 'loosen': 444,\n",
       " 'confused': 445,\n",
       " 'sat': 446,\n",
       " 'join': 447,\n",
       " 'mind': 448,\n",
       " 'silly': 449,\n",
       " 'tricked': 450,\n",
       " 'focused': 451,\n",
       " 'short': 452,\n",
       " 'bus': 453,\n",
       " 'smoke': 454,\n",
       " 'count': 455,\n",
       " 'hi': 456,\n",
       " 'sent': 457,\n",
       " 'cover': 458,\n",
       " 'afraid': 459,\n",
       " 'stuck': 460,\n",
       " 'honest': 461,\n",
       " 'theyll': 462,\n",
       " 'catch': 463,\n",
       " 'blind': 464,\n",
       " 'hit': 465,\n",
       " 'smell': 466,\n",
       " 'seat': 467,\n",
       " 'trapped': 468,\n",
       " 'kill': 469,\n",
       " 'tough': 470,\n",
       " 'hero': 471,\n",
       " 'crying': 472,\n",
       " 'camera': 473,\n",
       " 'welcome': 474,\n",
       " 'nearby': 475,\n",
       " 'often': 476,\n",
       " 'doors': 477,\n",
       " 'spoke': 478,\n",
       " 'merciful': 479,\n",
       " 'fined': 480,\n",
       " 'hello': 481,\n",
       " 'hug': 482,\n",
       " 'ours': 483,\n",
       " 'changed': 484,\n",
       " 'dancing': 485,\n",
       " 'only': 486,\n",
       " 'wife': 487,\n",
       " 'myself': 488,\n",
       " 'certain': 489,\n",
       " 'walked': 490,\n",
       " 'send': 491,\n",
       " 'clever': 492,\n",
       " 'water': 493,\n",
       " 'lose': 494,\n",
       " 'dying': 495,\n",
       " 'smiled': 496,\n",
       " 'missed': 497,\n",
       " 'liar': 498,\n",
       " 'broken': 499,\n",
       " 'kissed': 500,\n",
       " 'set': 501,\n",
       " 'those': 502,\n",
       " 'after': 503,\n",
       " 'dance': 504,\n",
       " 'kind': 505,\n",
       " 'unbelievable': 506,\n",
       " 'betrayed': 507,\n",
       " 'skinny': 508,\n",
       " 'pick': 509,\n",
       " 'might': 510,\n",
       " 'driving': 511,\n",
       " 'dream': 512,\n",
       " 'stunned': 513,\n",
       " 'jealous': 514,\n",
       " 'skiing': 515,\n",
       " 'fool': 516,\n",
       " 'upstairs': 517,\n",
       " 'slow': 518,\n",
       " 'kid': 519,\n",
       " 'innocent': 520,\n",
       " 'save': 521,\n",
       " 'jump': 522,\n",
       " 'kiss': 523,\n",
       " 'shoe': 524,\n",
       " 'along': 525,\n",
       " 'yelling': 526,\n",
       " 'brave': 527,\n",
       " 'worried': 528,\n",
       " 'crafty': 529,\n",
       " 'many': 530,\n",
       " 'direct': 531,\n",
       " 'losing': 532,\n",
       " 'rid': 533,\n",
       " 'coffee': 534,\n",
       " 'laughed': 535,\n",
       " 'memorize': 536,\n",
       " 'smile': 537,\n",
       " 'woke': 538,\n",
       " 'hates': 539,\n",
       " 'slowly': 540,\n",
       " 'teacher': 541,\n",
       " 'envious': 542,\n",
       " 'gear': 543,\n",
       " 'punctual': 544,\n",
       " 'answer': 545,\n",
       " 'deaf': 546,\n",
       " 'positive': 547,\n",
       " 'bald': 548,\n",
       " 'attack': 549,\n",
       " 'thirsty': 550,\n",
       " 'guilty': 551,\n",
       " 'talked': 552,\n",
       " 'cops': 553,\n",
       " 'sneaky': 554,\n",
       " 'helps': 555,\n",
       " 'our': 556,\n",
       " 'duty': 557,\n",
       " 'foolish': 558,\n",
       " 'black': 559,\n",
       " 'someone': 560,\n",
       " 'slept': 561,\n",
       " 'running': 562,\n",
       " 'followed': 563,\n",
       " 'fighting': 564,\n",
       " 'discreet': 565,\n",
       " 'warm': 566,\n",
       " 'face': 567,\n",
       " 'taste': 568,\n",
       " 'worked': 569,\n",
       " 'meant': 570,\n",
       " 'stoned': 571,\n",
       " 'doesnt': 572,\n",
       " 'boring': 573,\n",
       " 'hated': 574,\n",
       " 'age': 575,\n",
       " 'obey': 576,\n",
       " 'wet': 577,\n",
       " 'men': 578,\n",
       " 'friend': 579,\n",
       " 'touched': 580,\n",
       " 'finish': 581,\n",
       " 'decide': 582,\n",
       " 'boat': 583,\n",
       " 'fear': 584,\n",
       " 'perfect': 585,\n",
       " 'fly': 586,\n",
       " 'heard': 587,\n",
       " 'brief': 588,\n",
       " 'closely': 589,\n",
       " 'small': 590,\n",
       " 'retired': 591,\n",
       " 'seriously': 592,\n",
       " 'resist': 593,\n",
       " 'started': 594,\n",
       " 'ugly': 595,\n",
       " 'step': 596,\n",
       " 'armed': 597,\n",
       " 'confident': 598,\n",
       " 'fire': 599,\n",
       " 'put': 600,\n",
       " 'outrank': 601,\n",
       " 'thank': 602,\n",
       " 'promised': 603,\n",
       " 'struggled': 604,\n",
       " 'song': 605,\n",
       " 'thorough': 606,\n",
       " 'amazing': 607,\n",
       " 'congratulations': 608,\n",
       " 'dope': 609,\n",
       " 'starved': 610,\n",
       " 'friendly': 611,\n",
       " 'owe': 612,\n",
       " 'burned': 613,\n",
       " 'forgive': 614,\n",
       " 'warn': 615,\n",
       " 'thin': 616,\n",
       " 'wish': 617,\n",
       " 'music': 618,\n",
       " 'eggs': 619,\n",
       " 'terrible': 620,\n",
       " 'exhausted': 621,\n",
       " 'six': 622,\n",
       " 'something': 623,\n",
       " 'fed': 624,\n",
       " 'loaded': 625,\n",
       " 'surrendered': 626,\n",
       " 'red': 627,\n",
       " 'finicky': 628,\n",
       " 'kept': 629,\n",
       " 'word': 630,\n",
       " 'useless': 631,\n",
       " 'doubts': 632,\n",
       " 'messed': 633,\n",
       " 'trusts': 634,\n",
       " 'bother': 635,\n",
       " 'ashamed': 636,\n",
       " 'minute': 637,\n",
       " 'creative': 638,\n",
       " 'prudent': 639,\n",
       " 'dirty': 640,\n",
       " 'behind': 641,\n",
       " 'end': 642,\n",
       " 'strict': 643,\n",
       " 'real': 644,\n",
       " 'dressed': 645,\n",
       " 'shoot': 646,\n",
       " 'rather': 647,\n",
       " 'dizzy': 648,\n",
       " 'guys': 649,\n",
       " 'smaller': 650,\n",
       " 'insane': 651,\n",
       " 'weve': 652,\n",
       " 'escaped': 653,\n",
       " 'sue': 654,\n",
       " 'eating': 655,\n",
       " 'shaken': 656,\n",
       " 'game': 657,\n",
       " 'bell': 658,\n",
       " 'lawyer': 659,\n",
       " 'flinched': 660,\n",
       " 'hers': 661,\n",
       " 'forgot': 662,\n",
       " 'pull': 663,\n",
       " 'cheerful': 664,\n",
       " 'canceled': 665,\n",
       " 'believed': 666,\n",
       " 'youll': 667,\n",
       " 'head': 668,\n",
       " 'long': 669,\n",
       " 'shame': 670,\n",
       " 'girl': 671,\n",
       " 'argue': 672,\n",
       " 'milk': 673,\n",
       " 'unhappy': 674,\n",
       " 'act': 675,\n",
       " 'dinner': 676,\n",
       " 'hide': 677,\n",
       " 'worry': 678,\n",
       " 'ruined': 679,\n",
       " 'plans': 680,\n",
       " 'meat': 681,\n",
       " 'guy': 682,\n",
       " 'horse': 683,\n",
       " 'grateful': 684,\n",
       " 'refuse': 685,\n",
       " 'shoes': 686,\n",
       " 'begin': 687,\n",
       " 'dumb': 688,\n",
       " 'any': 689,\n",
       " 'cake': 690,\n",
       " 'singing': 691,\n",
       " 'bag': 692,\n",
       " 'raise': 693,\n",
       " 'simple': 694,\n",
       " 'excuse': 695,\n",
       " 'child': 696,\n",
       " 'smashed': 697,\n",
       " 'bright': 698,\n",
       " 'glad': 699,\n",
       " 'stinks': 700,\n",
       " 'ruthless': 701,\n",
       " 'quite': 702,\n",
       " 'clear': 703,\n",
       " 'ended': 704,\n",
       " 'cheat': 705,\n",
       " 'friends': 706,\n",
       " 'odd': 707,\n",
       " 'sharp': 708,\n",
       " 'injured': 709,\n",
       " 'oh': 710,\n",
       " 'smells': 711,\n",
       " 'everyones': 712,\n",
       " 'almost': 713,\n",
       " 'list': 714,\n",
       " 'through': 715,\n",
       " 'bite': 716,\n",
       " 'comes': 717,\n",
       " 'rested': 718,\n",
       " 'shall': 719,\n",
       " 'ski': 720,\n",
       " 'admire': 721,\n",
       " 'twins': 722,\n",
       " 'proof': 723,\n",
       " 'thief': 724,\n",
       " 'bet': 725,\n",
       " 'tomorrow': 726,\n",
       " 'sang': 727,\n",
       " 'cash': 728,\n",
       " 'divorced': 729,\n",
       " 'crashed': 730,\n",
       " 'wheres': 731,\n",
       " 'hugged': 732,\n",
       " 'silent': 733,\n",
       " 'curious': 734,\n",
       " 'heres': 735,\n",
       " 'runs': 736,\n",
       " 'shocked': 737,\n",
       " 'cancel': 738,\n",
       " 'dieting': 739,\n",
       " 'cooking': 740,\n",
       " 'phoned': 741,\n",
       " 'wealthy': 742,\n",
       " 'horrible': 743,\n",
       " 'destroy': 744,\n",
       " 'night': 745,\n",
       " 'failed': 746,\n",
       " 'chuckled': 747,\n",
       " 'aside': 748,\n",
       " 'fussy': 749,\n",
       " 'warned': 750,\n",
       " 'later': 751,\n",
       " 'laughing': 752,\n",
       " 'vulgar': 753,\n",
       " 'while': 754,\n",
       " 'lovely': 755,\n",
       " 'believe': 756,\n",
       " 'house': 757,\n",
       " 'speaking': 758,\n",
       " 'map': 759,\n",
       " 'annoy': 760,\n",
       " 'secret': 761,\n",
       " 'baffled': 762,\n",
       " 'disgusted': 763,\n",
       " 'hurried': 764,\n",
       " 'doll': 765,\n",
       " 'important': 766,\n",
       " 'quick': 767,\n",
       " 'prefer': 768,\n",
       " 'manage': 769,\n",
       " 'english': 770,\n",
       " 'happen': 771,\n",
       " 'creepy': 772,\n",
       " 'misled': 773,\n",
       " 'greedy': 774,\n",
       " 'cares': 775,\n",
       " 'chose': 776,\n",
       " 'smoking': 777,\n",
       " 'turned': 778,\n",
       " 'content': 779,\n",
       " 'cars': 780,\n",
       " 'volunteered': 781,\n",
       " 'studying': 782,\n",
       " 'agreed': 783,\n",
       " 'blue': 784,\n",
       " 'began': 785,\n",
       " 'clue': 786,\n",
       " 'grow': 787,\n",
       " 'exciting': 788,\n",
       " 'asked': 789,\n",
       " 'winter': 790,\n",
       " 'poor': 791,\n",
       " 'built': 792,\n",
       " 'selfish': 793,\n",
       " 'wrote': 794,\n",
       " 'horrified': 795,\n",
       " 'pizza': 796,\n",
       " 'throw': 797,\n",
       " 'maybe': 798,\n",
       " 'saint': 799,\n",
       " 'grew': 800,\n",
       " 'succeeded': 801,\n",
       " 'stood': 802,\n",
       " 'arrived': 803,\n",
       " 'children': 804,\n",
       " 'waited': 805,\n",
       " 'psychic': 806,\n",
       " 'rescued': 807,\n",
       " 'hideous': 808,\n",
       " 'pain': 809,\n",
       " 'objective': 810,\n",
       " 'risk': 811,\n",
       " 'fooled': 812,\n",
       " 'rang': 813,\n",
       " 'apologized': 814,\n",
       " 'everything': 815,\n",
       " 'teach': 816,\n",
       " 'depressed': 817,\n",
       " 'lit': 818,\n",
       " 'beg': 819,\n",
       " 'downstairs': 820,\n",
       " 'leak': 821,\n",
       " 'whose': 822,\n",
       " 'addicted': 823,\n",
       " 'party': 824,\n",
       " 'stubborn': 825,\n",
       " 'grumpy': 826,\n",
       " 'sounds': 827,\n",
       " 'amused': 828,\n",
       " 'fruit': 829,\n",
       " 'faster': 830,\n",
       " 'denied': 831,\n",
       " 'boston': 832,\n",
       " 'staying': 833,\n",
       " 'burn': 834,\n",
       " 'moved': 835,\n",
       " 'stopped': 836,\n",
       " 'precise': 837,\n",
       " 'proceed': 838,\n",
       " 'twin': 839,\n",
       " 'course': 840,\n",
       " 'command': 841,\n",
       " 'idiot': 842,\n",
       " 'shower': 843,\n",
       " 'continue': 844,\n",
       " 'goodbye': 845,\n",
       " 'helping': 846,\n",
       " 'soup': 847,\n",
       " 'respectful': 848,\n",
       " 'comment': 849,\n",
       " 'guns': 850,\n",
       " 'used': 851,\n",
       " 'handled': 852,\n",
       " 'blame': 853,\n",
       " 'share': 854,\n",
       " 'dry': 855,\n",
       " 'law': 856,\n",
       " 'scary': 857,\n",
       " 'wash': 858,\n",
       " 'aim': 859,\n",
       " 'high': 860,\n",
       " 'twice': 861,\n",
       " 'faint': 862,\n",
       " 'recovered': 863,\n",
       " 'differ': 864,\n",
       " 'obeyed': 865,\n",
       " 'arguing': 866,\n",
       " 'adores': 867,\n",
       " 'full': 868,\n",
       " 'own': 869,\n",
       " 'bike': 870,\n",
       " 'also': 871,\n",
       " 'beautiful': 872,\n",
       " 'from': 873,\n",
       " 'biased': 874,\n",
       " 'suits': 875,\n",
       " 'rice': 876,\n",
       " 'remain': 877,\n",
       " 'been': 878,\n",
       " 'object': 879,\n",
       " 'kite': 880,\n",
       " 'resigned': 881,\n",
       " 'art': 882,\n",
       " 'giddy': 883,\n",
       " 'crime': 884,\n",
       " 'outraged': 885,\n",
       " 'doing': 886,\n",
       " 'same': 887,\n",
       " 'mom': 888,\n",
       " 'sincere': 889,\n",
       " 'closer': 890,\n",
       " 'draw': 891,\n",
       " 'pale': 892,\n",
       " 'recess': 893,\n",
       " 'watchful': 894,\n",
       " 'plays': 895,\n",
       " 'explain': 896,\n",
       " 'sleeping': 897,\n",
       " 'town': 898,\n",
       " 'arm': 899,\n",
       " 'father': 900,\n",
       " 'timing': 901,\n",
       " 'pack': 902,\n",
       " 'sells': 903,\n",
       " 'screaming': 904,\n",
       " 'survive': 905,\n",
       " 'intrigued': 906,\n",
       " 'low': 907,\n",
       " 'obvious': 908,\n",
       " 'handle': 909,\n",
       " 'strange': 910,\n",
       " 'envy': 911,\n",
       " 'sense': 912,\n",
       " 'dreaming': 913,\n",
       " 'fail': 914,\n",
       " 'thatll': 915,\n",
       " 'cookies': 916,\n",
       " 'bath': 917,\n",
       " 'feels': 918,\n",
       " 'bury': 919,\n",
       " 'team': 920,\n",
       " 'blood': 921,\n",
       " 'bless': 922,\n",
       " 'already': 923,\n",
       " 'bought': 924,\n",
       " 'noticed': 925,\n",
       " 'yelled': 926,\n",
       " 'mature': 927,\n",
       " 'refused': 928,\n",
       " 'soccer': 929,\n",
       " 'ice': 930,\n",
       " 'sec': 931,\n",
       " 'green': 932,\n",
       " 'laugh': 933,\n",
       " 'chicken': 934,\n",
       " 'powerful': 935,\n",
       " 'says': 936,\n",
       " 'pregnant': 937,\n",
       " 'treat': 938,\n",
       " 'fixed': 939,\n",
       " 'control': 940,\n",
       " 'harder': 941,\n",
       " 'taxi': 942,\n",
       " 'japanese': 943,\n",
       " 'apologize': 944,\n",
       " 'waste': 945,\n",
       " 'fad': 946,\n",
       " 'sloshed': 947,\n",
       " 'desk': 948,\n",
       " 'nose': 949,\n",
       " 'clearly': 950,\n",
       " 'dark': 951,\n",
       " 'voted': 952,\n",
       " 'war': 953,\n",
       " 'fall': 954,\n",
       " 'watching': 955,\n",
       " 'pen': 956,\n",
       " 'split': 957,\n",
       " 'rush': 958,\n",
       " 'observant': 959,\n",
       " 'special': 960,\n",
       " 'offended': 961,\n",
       " 'ought': 962,\n",
       " 'writing': 963,\n",
       " 'unarmed': 964,\n",
       " 'screamed': 965,\n",
       " 'boys': 966,\n",
       " 'supportive': 967,\n",
       " 'became': 968,\n",
       " 'wide': 969,\n",
       " 'marry': 970,\n",
       " 'healthy': 971,\n",
       " 'locked': 972,\n",
       " 'moment': 973,\n",
       " 'seal': 974,\n",
       " 'seize': 975,\n",
       " 'unlocked': 976,\n",
       " 'wise': 977,\n",
       " 'donut': 978,\n",
       " 'till': 979,\n",
       " 'evidence': 980,\n",
       " 'remarried': 981,\n",
       " 'outdated': 982,\n",
       " 'artist': 983,\n",
       " 'feet': 984,\n",
       " 'mess': 985,\n",
       " 'learn': 986,\n",
       " 'leg': 987,\n",
       " 'cringed': 988,\n",
       " 'stayed': 989,\n",
       " 'paris': 990,\n",
       " 'climbing': 991,\n",
       " 'cheap': 992,\n",
       " 'pipe': 993,\n",
       " 'allow': 994,\n",
       " 'cows': 995,\n",
       " 'looking': 996,\n",
       " 'fact': 997,\n",
       " 'always': 998,\n",
       " 'ears': 999,\n",
       " 'cheers': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word index of English tokenizer\n",
    "eng_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'je': 1,\n",
       " 'tom': 2,\n",
       " 'suis': 3,\n",
       " 'a': 4,\n",
       " 'pas': 5,\n",
       " 'nous': 6,\n",
       " 'il': 7,\n",
       " 'cest': 8,\n",
       " 'vous': 9,\n",
       " 'jai': 10,\n",
       " 'est': 11,\n",
       " 'de': 12,\n",
       " 'ne': 13,\n",
       " 'le': 14,\n",
       " 'la': 15,\n",
       " 'un': 16,\n",
       " 'me': 17,\n",
       " 'en': 18,\n",
       " 'ca': 19,\n",
       " 'tu': 20,\n",
       " 'les': 21,\n",
       " 'une': 22,\n",
       " 'que': 23,\n",
       " 'estce': 24,\n",
       " 'sommes': 25,\n",
       " 'etes': 26,\n",
       " 'qui': 27,\n",
       " 'es': 28,\n",
       " 'fait': 29,\n",
       " 'elle': 30,\n",
       " 'sont': 31,\n",
       " 'ils': 32,\n",
       " 'tout': 33,\n",
       " 'ce': 34,\n",
       " 'va': 35,\n",
       " 'des': 36,\n",
       " 'bien': 37,\n",
       " 'soyez': 38,\n",
       " 'ma': 39,\n",
       " 'du': 40,\n",
       " 'besoin': 41,\n",
       " 'elles': 42,\n",
       " 'ete': 43,\n",
       " 'te': 44,\n",
       " 'y': 45,\n",
       " 'faut': 46,\n",
       " 'ici': 47,\n",
       " 'etesvous': 48,\n",
       " 'jaime': 49,\n",
       " 'ai': 50,\n",
       " 'mon': 51,\n",
       " 'se': 52,\n",
       " 'sois': 53,\n",
       " 'nest': 54,\n",
       " 'veux': 55,\n",
       " 'fais': 56,\n",
       " 'aller': 57,\n",
       " 'moi': 58,\n",
       " 'jetais': 59,\n",
       " 'etait': 60,\n",
       " 'cela': 61,\n",
       " 'lai': 62,\n",
       " 'on': 63,\n",
       " 'lair': 64,\n",
       " 'partir': 65,\n",
       " 'train': 66,\n",
       " 'peux': 67,\n",
       " 'au': 68,\n",
       " 'personne': 69,\n",
       " 'faire': 70,\n",
       " 'avons': 71,\n",
       " 'cetait': 72,\n",
       " 'sest': 73,\n",
       " 'reste': 74,\n",
       " 'estu': 75,\n",
       " 'sens': 76,\n",
       " 'mal': 77,\n",
       " 'deteste': 78,\n",
       " 'toi': 79,\n",
       " 'maintenant': 80,\n",
       " 'jadore': 81,\n",
       " 'ceci': 82,\n",
       " 'vu': 83,\n",
       " 'restez': 84,\n",
       " 'puisje': 85,\n",
       " 'ont': 86,\n",
       " 'votre': 87,\n",
       " 'trop': 88,\n",
       " 'plait': 89,\n",
       " 'dois': 90,\n",
       " 'jen': 91,\n",
       " 'bon': 92,\n",
       " 'plus': 93,\n",
       " 'monde': 94,\n",
       " 'estil': 95,\n",
       " 'comment': 96,\n",
       " 'faites': 97,\n",
       " 'men': 98,\n",
       " 'prends': 99,\n",
       " 'voir': 100,\n",
       " 'tous': 101,\n",
       " 'vais': 102,\n",
       " 'arrete': 103,\n",
       " 'maison': 104,\n",
       " 'tres': 105,\n",
       " 'sil': 106,\n",
       " 'nai': 107,\n",
       " 'comme': 108,\n",
       " 'lui': 109,\n",
       " 'ton': 110,\n",
       " 'dans': 111,\n",
       " 'quelle': 112,\n",
       " 'avec': 113,\n",
       " 'sur': 114,\n",
       " 'as': 115,\n",
       " 'viens': 116,\n",
       " 'tes': 117,\n",
       " 'dun': 118,\n",
       " 'chez': 119,\n",
       " 'laisse': 120,\n",
       " 'dit': 121,\n",
       " 'aime': 122,\n",
       " 'prenez': 123,\n",
       " 'allez': 124,\n",
       " 'perdu': 125,\n",
       " 'juste': 126,\n",
       " 'seul': 127,\n",
       " 'voiture': 128,\n",
       " 'arretez': 129,\n",
       " 'nouveau': 130,\n",
       " 'heureux': 131,\n",
       " 'toutes': 132,\n",
       " 'pour': 133,\n",
       " 'avez': 134,\n",
       " 'porte': 135,\n",
       " 'gagne': 136,\n",
       " 'eu': 137,\n",
       " 'si': 138,\n",
       " 'sais': 139,\n",
       " 'jamais': 140,\n",
       " 'chercher': 141,\n",
       " 'calme': 142,\n",
       " 'temps': 143,\n",
       " 'deux': 144,\n",
       " 'tellement': 145,\n",
       " 'attention': 146,\n",
       " 'bonne': 147,\n",
       " 'ta': 148,\n",
       " 'tranquille': 149,\n",
       " 'laissezmoi': 150,\n",
       " 'grand': 151,\n",
       " 'regarde': 152,\n",
       " 'malade': 153,\n",
       " 'dune': 154,\n",
       " 'travail': 155,\n",
       " 'aider': 156,\n",
       " 'tomber': 157,\n",
       " 'laissemoi': 158,\n",
       " 'jy': 159,\n",
       " 'prie': 160,\n",
       " 'parle': 161,\n",
       " 'tai': 162,\n",
       " 'beaucoup': 163,\n",
       " 'senti': 164,\n",
       " 'rien': 165,\n",
       " 'nager': 166,\n",
       " 'quel': 167,\n",
       " 'confiance': 168,\n",
       " 'nen': 169,\n",
       " 'venez': 170,\n",
       " 'termine': 171,\n",
       " 'yeux': 172,\n",
       " 'blesse': 173,\n",
       " 'vraiment': 174,\n",
       " 'atil': 175,\n",
       " 'marche': 176,\n",
       " 'vite': 177,\n",
       " 'pris': 178,\n",
       " 'occupe': 179,\n",
       " 'mange': 180,\n",
       " 'peut': 181,\n",
       " 'aije': 182,\n",
       " 'trouve': 183,\n",
       " 'continuez': 184,\n",
       " 'chien': 185,\n",
       " 'continue': 186,\n",
       " 'pret': 187,\n",
       " 'non': 188,\n",
       " 'daccord': 189,\n",
       " 'coup': 190,\n",
       " 'demande': 191,\n",
       " 'triste': 192,\n",
       " 'froid': 193,\n",
       " 'mes': 194,\n",
       " 'venir': 195,\n",
       " 'simplement': 196,\n",
       " 'essaye': 197,\n",
       " 'dispose': 198,\n",
       " 'vie': 199,\n",
       " 'regardez': 200,\n",
       " 'sagit': 201,\n",
       " 'avait': 202,\n",
       " 'na': 203,\n",
       " 'suisje': 204,\n",
       " 'securite': 205,\n",
       " 'questce': 206,\n",
       " 'avezvous': 207,\n",
       " 'serai': 208,\n",
       " 'vrai': 209,\n",
       " 'fatigue': 210,\n",
       " 'lit': 211,\n",
       " 'livre': 212,\n",
       " 'fort': 213,\n",
       " 'essaie': 214,\n",
       " 'ou': 215,\n",
       " 'chaud': 216,\n",
       " 'fou': 217,\n",
       " 'encore': 218,\n",
       " 'boulot': 219,\n",
       " 'arme': 220,\n",
       " 'bizarre': 221,\n",
       " 'sait': 222,\n",
       " 'fils': 223,\n",
       " 'merci': 224,\n",
       " 'mort': 225,\n",
       " 'pouvons': 226,\n",
       " 'entrer': 227,\n",
       " 'quelquun': 228,\n",
       " 'ten': 229,\n",
       " 'astu': 230,\n",
       " 'vois': 231,\n",
       " 'fini': 232,\n",
       " 'travailler': 233,\n",
       " 'essayer': 234,\n",
       " 'laissez': 235,\n",
       " 'seule': 236,\n",
       " 'filles': 237,\n",
       " 'manger': 238,\n",
       " 'courir': 239,\n",
       " 'travaille': 240,\n",
       " 'sommesnous': 241,\n",
       " 'mourir': 242,\n",
       " 'parler': 243,\n",
       " 'main': 244,\n",
       " 'heureuse': 245,\n",
       " 'prudente': 246,\n",
       " 'marie': 247,\n",
       " 'parti': 248,\n",
       " 'prudent': 249,\n",
       " 'casse': 250,\n",
       " 'debout': 251,\n",
       " 'assez': 252,\n",
       " 'idiot': 253,\n",
       " 'sen': 254,\n",
       " 'jirai': 255,\n",
       " 'vieux': 256,\n",
       " 'adore': 257,\n",
       " 'avance': 258,\n",
       " 'tort': 259,\n",
       " 'occupee': 260,\n",
       " 'peu': 261,\n",
       " 'attrape': 262,\n",
       " 'chose': 263,\n",
       " 'vis': 264,\n",
       " 'raison': 265,\n",
       " 'laide': 266,\n",
       " 'pourquoi': 267,\n",
       " 'quoi': 268,\n",
       " 'lamour': 269,\n",
       " 'serieux': 270,\n",
       " 'pars': 271,\n",
       " 'manque': 272,\n",
       " 'prete': 273,\n",
       " 'connais': 274,\n",
       " 'gentil': 275,\n",
       " 'ferme': 276,\n",
       " 'faim': 277,\n",
       " 'ny': 278,\n",
       " 'tard': 279,\n",
       " 'francais': 280,\n",
       " 'dormir': 281,\n",
       " 'aucune': 282,\n",
       " 'idee': 283,\n",
       " 'joue': 284,\n",
       " 'retard': 285,\n",
       " 'pourrait': 286,\n",
       " 'facile': 287,\n",
       " 'aujourdhui': 288,\n",
       " 'gros': 289,\n",
       " 'chat': 290,\n",
       " 'pouvonsnous': 291,\n",
       " 'marcher': 292,\n",
       " 'venu': 293,\n",
       " 'tire': 294,\n",
       " 'fois': 295,\n",
       " 'mieux': 296,\n",
       " 'vue': 297,\n",
       " 'dis': 298,\n",
       " 'colere': 299,\n",
       " 'riche': 300,\n",
       " 'tot': 301,\n",
       " 'vos': 302,\n",
       " 'blague': 303,\n",
       " 'sans': 304,\n",
       " 'commence': 305,\n",
       " 'peur': 306,\n",
       " 'pleure': 307,\n",
       " 'dur': 308,\n",
       " 'chanter': 309,\n",
       " 'paye': 310,\n",
       " 'cette': 311,\n",
       " 'super': 312,\n",
       " 'aide': 313,\n",
       " 'biere': 314,\n",
       " 'voici': 315,\n",
       " 'passe': 316,\n",
       " 'enfants': 317,\n",
       " 'assis': 318,\n",
       " 'oublie': 319,\n",
       " 'attendez': 320,\n",
       " 'sentie': 321,\n",
       " 'semble': 322,\n",
       " 'lire': 323,\n",
       " 'veuillez': 324,\n",
       " 'age': 325,\n",
       " 'voila': 326,\n",
       " 'chance': 327,\n",
       " 'appele': 328,\n",
       " 'tete': 329,\n",
       " 'sontils': 330,\n",
       " 'mauvais': 331,\n",
       " 'attendre': 332,\n",
       " 'bouge': 333,\n",
       " 'timide': 334,\n",
       " 'attends': 335,\n",
       " 'etre': 336,\n",
       " 'mis': 337,\n",
       " 'avoir': 338,\n",
       " 'largent': 339,\n",
       " 'sentis': 340,\n",
       " 'veut': 341,\n",
       " 'rester': 342,\n",
       " 'faisle': 343,\n",
       " 'donne': 344,\n",
       " 'aussi': 345,\n",
       " 'the': 346,\n",
       " 'signe': 347,\n",
       " 'homme': 348,\n",
       " 'lecole': 349,\n",
       " 'mien': 350,\n",
       " 'femme': 351,\n",
       " 'garcon': 352,\n",
       " 'grande': 353,\n",
       " 'dieu': 354,\n",
       " 'gagner': 355,\n",
       " 'drole': 356,\n",
       " 'verre': 357,\n",
       " 'davantage': 358,\n",
       " 'camp': 359,\n",
       " 'dessus': 360,\n",
       " 'vin': 361,\n",
       " 'notre': 362,\n",
       " 'linterieur': 363,\n",
       " 'egalement': 364,\n",
       " 'allons': 365,\n",
       " 'payer': 366,\n",
       " 'sympa': 367,\n",
       " 'essayez': 368,\n",
       " 'quitte': 369,\n",
       " 'son': 370,\n",
       " 'danser': 371,\n",
       " 'arrive': 372,\n",
       " 'japprecie': 373,\n",
       " 'lheure': 374,\n",
       " 'oui': 375,\n",
       " 'my': 376,\n",
       " 'quelque': 377,\n",
       " 'pleurer': 378,\n",
       " 'chapeau': 379,\n",
       " 'par': 380,\n",
       " 'journee': 381,\n",
       " 'prendre': 382,\n",
       " 'petit': 383,\n",
       " 'medecin': 384,\n",
       " 'aucun': 385,\n",
       " 'nestce': 386,\n",
       " 'suffit': 387,\n",
       " 'cours': 388,\n",
       " 'lis': 389,\n",
       " 'probleme': 390,\n",
       " 'sontelles': 391,\n",
       " 'marrant': 392,\n",
       " 'heures': 393,\n",
       " 'pouvez': 394,\n",
       " 'lavezvous': 395,\n",
       " 'chambre': 396,\n",
       " 'faitesle': 397,\n",
       " 'fut': 398,\n",
       " 'folle': 399,\n",
       " 'saoul': 400,\n",
       " 'menti': 401,\n",
       " 'touche': 402,\n",
       " 'peuxtu': 403,\n",
       " 'faux': 404,\n",
       " 'sors': 405,\n",
       " 'derriere': 406,\n",
       " 'vote': 407,\n",
       " 'reviens': 408,\n",
       " 'plan': 409,\n",
       " 'chiens': 410,\n",
       " 'dire': 411,\n",
       " 'ferai': 412,\n",
       " 'chante': 413,\n",
       " 'sortir': 414,\n",
       " 'degage': 415,\n",
       " 'incroyable': 416,\n",
       " 'faible': 417,\n",
       " 'bonjour': 418,\n",
       " 'devenu': 419,\n",
       " 'alle': 420,\n",
       " 'sagitil': 421,\n",
       " 'entendu': 422,\n",
       " 'place': 423,\n",
       " 'partie': 424,\n",
       " 'voulons': 425,\n",
       " 'pied': 426,\n",
       " 'sommeil': 427,\n",
       " 'heros': 428,\n",
       " 'souviens': 429,\n",
       " 'pouvezvous': 430,\n",
       " 'chats': 431,\n",
       " 'lastu': 432,\n",
       " 'apporte': 433,\n",
       " 'prepare': 434,\n",
       " 'rentre': 435,\n",
       " 'quil': 436,\n",
       " 'dehors': 437,\n",
       " 'soin': 438,\n",
       " 'premier': 439,\n",
       " 'contrarie': 440,\n",
       " 'desole': 441,\n",
       " 'pres': 442,\n",
       " 'reveille': 443,\n",
       " 'bus': 444,\n",
       " 'connait': 445,\n",
       " 'aux': 446,\n",
       " 'entre': 447,\n",
       " 'normal': 448,\n",
       " 'arreter': 449,\n",
       " 'mary': 450,\n",
       " 'ecoute': 451,\n",
       " 'genial': 452,\n",
       " 'et': 453,\n",
       " 'aveugle': 454,\n",
       " 'quiconque': 455,\n",
       " 'court': 456,\n",
       " 'donnemoi': 457,\n",
       " 'dargent': 458,\n",
       " 'prets': 459,\n",
       " 'honte': 460,\n",
       " 'horreur': 461,\n",
       " 'cuisine': 462,\n",
       " 'semblez': 463,\n",
       " 'poisson': 464,\n",
       " 'vient': 465,\n",
       " 'fermez': 466,\n",
       " 'dejeuner': 467,\n",
       " 'horrible': 468,\n",
       " 'mienne': 469,\n",
       " 'contrariee': 470,\n",
       " 'verrouille': 471,\n",
       " 'gardez': 472,\n",
       " 'etions': 473,\n",
       " 'emporte': 474,\n",
       " 'pense': 475,\n",
       " 'forme': 476,\n",
       " 'naif': 477,\n",
       " 'crie': 478,\n",
       " 'carte': 479,\n",
       " 'jeune': 480,\n",
       " 'salut': 481,\n",
       " 'pretes': 482,\n",
       " 'occupees': 483,\n",
       " 'paresseux': 484,\n",
       " 'trompe': 485,\n",
       " 'hurler': 486,\n",
       " 'perdre': 487,\n",
       " 'appelle': 488,\n",
       " 'sortez': 489,\n",
       " 'pue': 490,\n",
       " 'ouvre': 491,\n",
       " 'libre': 492,\n",
       " 'lavons': 493,\n",
       " 'netais': 494,\n",
       " 'lont': 495,\n",
       " 'cool': 496,\n",
       " 'assiedstoi': 497,\n",
       " 'feu': 498,\n",
       " 'leve': 499,\n",
       " 'soit': 500,\n",
       " 'estelle': 501,\n",
       " 'paix': 502,\n",
       " 'souvent': 503,\n",
       " 'cuisiner': 504,\n",
       " 'fonctionne': 505,\n",
       " 'abandonne': 506,\n",
       " 'portes': 507,\n",
       " 'grave': 508,\n",
       " 'skier': 509,\n",
       " 'apprecie': 510,\n",
       " 'bras': 511,\n",
       " 'enfant': 512,\n",
       " 'verifie': 513,\n",
       " 'saistu': 514,\n",
       " 'propre': 515,\n",
       " 'conduis': 516,\n",
       " 'trouver': 517,\n",
       " 'detendstoi': 518,\n",
       " 'donnezmoi': 519,\n",
       " 'commencez': 520,\n",
       " 'vas': 521,\n",
       " 'asseyezvous': 522,\n",
       " 'patient': 523,\n",
       " 'garde': 524,\n",
       " 'mauvaise': 525,\n",
       " 'piege': 526,\n",
       " 'devons': 527,\n",
       " 'naive': 528,\n",
       " 'allonsy': 529,\n",
       " 'mignon': 530,\n",
       " 'souri': 531,\n",
       " 'tele': 532,\n",
       " 'bateau': 533,\n",
       " 'honnete': 534,\n",
       " 'plutot': 535,\n",
       " 'cafe': 536,\n",
       " 'meme': 537,\n",
       " 'combien': 538,\n",
       " 'dici': 539,\n",
       " 'service': 540,\n",
       " 'occupes': 541,\n",
       " 'narrive': 542,\n",
       " 'peutetre': 543,\n",
       " 'fatiguee': 544,\n",
       " 'cle': 545,\n",
       " 'tiens': 546,\n",
       " 'rapide': 547,\n",
       " 'vieille': 548,\n",
       " 'taider': 549,\n",
       " 'amusant': 550,\n",
       " 'compris': 551,\n",
       " 'fallait': 552,\n",
       " 'laissee': 553,\n",
       " 'coupable': 554,\n",
       " 'bois': 555,\n",
       " 'photo': 556,\n",
       " 'oubliez': 557,\n",
       " 'flics': 558,\n",
       " 'parviens': 559,\n",
       " 'promis': 560,\n",
       " 'dites': 561,\n",
       " 'fille': 562,\n",
       " 'telephone': 563,\n",
       " 'amoureuse': 564,\n",
       " 'avant': 565,\n",
       " 'dormi': 566,\n",
       " 'affaire': 567,\n",
       " 'dingue': 568,\n",
       " 'content': 569,\n",
       " 'gentille': 570,\n",
       " 'jarrive': 571,\n",
       " 'compte': 572,\n",
       " 'regarder': 573,\n",
       " 'javais': 574,\n",
       " 'jetez': 575,\n",
       " 'allezvous': 576,\n",
       " 'refuse': 577,\n",
       " 'plaisir': 578,\n",
       " 'sante': 579,\n",
       " 'descends': 580,\n",
       " 'fis': 581,\n",
       " 'difficile': 582,\n",
       " 'ceuxci': 583,\n",
       " 'decampe': 584,\n",
       " 'ah': 585,\n",
       " 'parole': 586,\n",
       " 'endormi': 587,\n",
       " 'hommes': 588,\n",
       " 'beau': 589,\n",
       " 'seconde': 590,\n",
       " 'nu': 591,\n",
       " 'peutil': 592,\n",
       " 'sincere': 593,\n",
       " 'fache': 594,\n",
       " 'parfait': 595,\n",
       " 'faisait': 596,\n",
       " 'sa': 597,\n",
       " 'labas': 598,\n",
       " 'tour': 599,\n",
       " 'pardon': 600,\n",
       " 'ans': 601,\n",
       " 'avais': 602,\n",
       " 'certain': 603,\n",
       " 'lentement': 604,\n",
       " 'baisse': 605,\n",
       " 'entrez': 606,\n",
       " 'faisons': 607,\n",
       " 'sucre': 608,\n",
       " 'reussi': 609,\n",
       " 'reve': 610,\n",
       " 'fus': 611,\n",
       " 'nourriture': 612,\n",
       " 'doucement': 613,\n",
       " 'soif': 614,\n",
       " 'stupide': 615,\n",
       " 'descendez': 616,\n",
       " 'diner': 617,\n",
       " 'vers': 618,\n",
       " 'blessee': 619,\n",
       " 'boire': 620,\n",
       " 'netes': 621,\n",
       " 'conduire': 622,\n",
       " 'chouette': 623,\n",
       " 'chanson': 624,\n",
       " 'engage': 625,\n",
       " 'mentir': 626,\n",
       " 'vues': 627,\n",
       " 'laissele': 628,\n",
       " 'lexterieur': 629,\n",
       " 'voulais': 630,\n",
       " 'mesure': 631,\n",
       " 'vire': 632,\n",
       " 'formidable': 633,\n",
       " 'haut': 634,\n",
       " 'devraisje': 635,\n",
       " 'cheval': 636,\n",
       " 'entendre': 637,\n",
       " 'demander': 638,\n",
       " 'musique': 639,\n",
       " 'ufs': 640,\n",
       " 'vasy': 641,\n",
       " 'attrapez': 642,\n",
       " 'lacet': 643,\n",
       " 'bouger': 644,\n",
       " 'sembles': 645,\n",
       " 'demandez': 646,\n",
       " 'petite': 647,\n",
       " 'rendu': 648,\n",
       " 'leau': 649,\n",
       " 'desolee': 650,\n",
       " 'invite': 651,\n",
       " 'compter': 652,\n",
       " 'trahi': 653,\n",
       " 'savons': 654,\n",
       " 'cheveux': 655,\n",
       " 'belle': 656,\n",
       " 'battre': 657,\n",
       " 'netait': 658,\n",
       " 'mechant': 659,\n",
       " 'ri': 660,\n",
       " 'apres': 661,\n",
       " 'triche': 662,\n",
       " 'mont': 663,\n",
       " 'forte': 664,\n",
       " 'file': 665,\n",
       " 'retraite': 666,\n",
       " 'las': 667,\n",
       " 'maider': 668,\n",
       " 'ouvrez': 669,\n",
       " 'fallut': 670,\n",
       " 'doit': 671,\n",
       " 'veulent': 672,\n",
       " 'nimporte': 673,\n",
       " 'sentait': 674,\n",
       " 'plein': 675,\n",
       " 'detendu': 676,\n",
       " 'quittez': 677,\n",
       " 'mec': 678,\n",
       " 'verifier': 679,\n",
       " 'taxi': 680,\n",
       " 'intelligent': 681,\n",
       " 'sy': 682,\n",
       " 'vatil': 683,\n",
       " 'vus': 684,\n",
       " 'cloche': 685,\n",
       " 'heureuses': 686,\n",
       " 'simple': 687,\n",
       " 'savezvous': 688,\n",
       " 'savoir': 689,\n",
       " 'sure': 690,\n",
       " 'route': 691,\n",
       " 'moment': 692,\n",
       " 'proximite': 693,\n",
       " 'rendre': 694,\n",
       " 'fiche': 695,\n",
       " 'tombe': 696,\n",
       " 'deguerpissez': 697,\n",
       " 'malin': 698,\n",
       " 'amoureux': 699,\n",
       " 'enerve': 700,\n",
       " 'riches': 701,\n",
       " 'lait': 702,\n",
       " 'douche': 703,\n",
       " 'vont': 704,\n",
       " 'tourne': 705,\n",
       " 'mille': 706,\n",
       " 'bientot': 707,\n",
       " 'etais': 708,\n",
       " 'ecoutez': 709,\n",
       " 'nuit': 710,\n",
       " 'libres': 711,\n",
       " 'monte': 712,\n",
       " 'bu': 713,\n",
       " 'toute': 714,\n",
       " 'saute': 715,\n",
       " 'nerveux': 716,\n",
       " 'ainsi': 717,\n",
       " 'prudentes': 718,\n",
       " 'retour': 719,\n",
       " 'preuves': 720,\n",
       " 'daide': 721,\n",
       " 'jalouse': 722,\n",
       " 'crois': 723,\n",
       " 'sauve': 724,\n",
       " 'decampez': 725,\n",
       " 'taime': 726,\n",
       " 'surs': 727,\n",
       " 'prefere': 728,\n",
       " 'sac': 729,\n",
       " 'mangez': 730,\n",
       " 'cassezvous': 731,\n",
       " 'aidemoi': 732,\n",
       " 'ivre': 733,\n",
       " 'morts': 734,\n",
       " 'menteur': 735,\n",
       " 'partons': 736,\n",
       " 'heure': 737,\n",
       " 'comprends': 738,\n",
       " 'revenez': 739,\n",
       " 'noir': 740,\n",
       " 'rendezvous': 741,\n",
       " 'minute': 742,\n",
       " 'courant': 743,\n",
       " 'plaisante': 744,\n",
       " 'droit': 745,\n",
       " 'cravate': 746,\n",
       " 'partez': 747,\n",
       " 'maman': 748,\n",
       " 'serais': 749,\n",
       " 'laije': 750,\n",
       " 'pauvre': 751,\n",
       " 'sorti': 752,\n",
       " 'devine': 753,\n",
       " 'egoiste': 754,\n",
       " 'avocat': 755,\n",
       " 'trente': 756,\n",
       " 'ses': 757,\n",
       " 'fous': 758,\n",
       " 'rhume': 759,\n",
       " 'idiote': 760,\n",
       " 'accord': 761,\n",
       " 'presente': 762,\n",
       " 'cesse': 763,\n",
       " 'mecs': 764,\n",
       " 'disparais': 765,\n",
       " 'maniere': 766,\n",
       " 'liste': 767,\n",
       " 'souris': 768,\n",
       " 'coupe': 769,\n",
       " 'cesser': 770,\n",
       " 'laissa': 771,\n",
       " 'calmes': 772,\n",
       " 'dure': 773,\n",
       " 'thomas': 774,\n",
       " 'sourd': 775,\n",
       " 'devez': 776,\n",
       " 'tenez': 777,\n",
       " 'sauves': 778,\n",
       " 'amuse': 779,\n",
       " 'demain': 780,\n",
       " 'brule': 781,\n",
       " 'amusee': 782,\n",
       " 'prudents': 783,\n",
       " 'nes': 784,\n",
       " 'lecart': 785,\n",
       " 'reveilletoi': 786,\n",
       " 'essayons': 787,\n",
       " 'rappelle': 788,\n",
       " 'bougez': 789,\n",
       " 'reculez': 790,\n",
       " 'aideznous': 791,\n",
       " 'chauve': 792,\n",
       " 'navais': 793,\n",
       " 'justice': 794,\n",
       " 'presque': 795,\n",
       " 'arriver': 796,\n",
       " 'doute': 797,\n",
       " 'grosse': 798,\n",
       " 'perdue': 799,\n",
       " 'rever': 800,\n",
       " 'vit': 801,\n",
       " 'nerveuse': 802,\n",
       " 'cruel': 803,\n",
       " 'detendue': 804,\n",
       " 'reparer': 805,\n",
       " 'discuter': 806,\n",
       " 'ressenti': 807,\n",
       " 'cellela': 808,\n",
       " 'malheureux': 809,\n",
       " 'licencie': 810,\n",
       " 'joindre': 811,\n",
       " 'talent': 812,\n",
       " 'rire': 813,\n",
       " 'viande': 814,\n",
       " 'immediatement': 815,\n",
       " 'chanceux': 816,\n",
       " 'type': 817,\n",
       " 'espoir': 818,\n",
       " 'envie': 819,\n",
       " 'jambes': 820,\n",
       " 'echoue': 821,\n",
       " 'mince': 822,\n",
       " 'pourrais': 823,\n",
       " 'vivant': 824,\n",
       " 'miens': 825,\n",
       " 'jentends': 826,\n",
       " 'devrais': 827,\n",
       " 'depechezvous': 828,\n",
       " 'jouer': 829,\n",
       " 'taije': 830,\n",
       " 'chaussure': 831,\n",
       " 'allezy': 832,\n",
       " 'confus': 833,\n",
       " 'bete': 834,\n",
       " 'credule': 835,\n",
       " 'largue': 836,\n",
       " 'darriver': 837,\n",
       " 'secret': 838,\n",
       " 'recule': 839,\n",
       " 'changer': 840,\n",
       " 'bravo': 841,\n",
       " 'toujours': 842,\n",
       " 'avions': 843,\n",
       " 'fit': 844,\n",
       " 'envoye': 845,\n",
       " 'pitie': 846,\n",
       " 'gaffe': 847,\n",
       " 'verrouillez': 848,\n",
       " 'sortie': 849,\n",
       " 'cellesci': 850,\n",
       " 'cassee': 851,\n",
       " 'assise': 852,\n",
       " 'fumer': 853,\n",
       " 'neuf': 854,\n",
       " 'voitures': 855,\n",
       " 'pousse': 856,\n",
       " 'sent': 857,\n",
       " 'lachemoi': 858,\n",
       " 'terminee': 859,\n",
       " 'appelez': 860,\n",
       " 'capte': 861,\n",
       " 'bourre': 862,\n",
       " 'parlez': 863,\n",
       " 'bleu': 864,\n",
       " 'equipe': 865,\n",
       " 'tienstoi': 866,\n",
       " 'soyons': 867,\n",
       " 'silencieux': 868,\n",
       " 'voix': 869,\n",
       " 'magnifique': 870,\n",
       " 'malchanceux': 871,\n",
       " 'raisonnable': 872,\n",
       " 'rouge': 873,\n",
       " 'breve': 874,\n",
       " 'lhiver': 875,\n",
       " 'siffler': 876,\n",
       " 'sale': 877,\n",
       " 'droite': 878,\n",
       " 'partit': 879,\n",
       " 'sentais': 880,\n",
       " 'laissezle': 881,\n",
       " 'emploi': 882,\n",
       " 'ecrit': 883,\n",
       " 'ville': 884,\n",
       " 'professeur': 885,\n",
       " 'etudie': 886,\n",
       " 'avonsnous': 887,\n",
       " 'laime': 888,\n",
       " 'cessez': 889,\n",
       " 'fumes': 890,\n",
       " 'pizza': 891,\n",
       " 'aimons': 892,\n",
       " 'fallu': 893,\n",
       " 'reveillee': 894,\n",
       " 'possible': 895,\n",
       " 'savais': 896,\n",
       " 'dy': 897,\n",
       " 'etaistu': 898,\n",
       " 'resister': 899,\n",
       " 'quest': 900,\n",
       " 'pousser': 901,\n",
       " 'minable': 902,\n",
       " 'tien': 903,\n",
       " 'conclu': 904,\n",
       " 'patron': 905,\n",
       " 'affaires': 906,\n",
       " 'mens': 907,\n",
       " 'confectionne': 908,\n",
       " 'bureau': 909,\n",
       " 'proces': 910,\n",
       " 'donner': 911,\n",
       " 'trahie': 912,\n",
       " 'peuvent': 913,\n",
       " 'famille': 914,\n",
       " 'jaimerais': 915,\n",
       " 'ski': 916,\n",
       " 'choix': 917,\n",
       " 'tenezvous': 918,\n",
       " 'deja': 919,\n",
       " 'neige': 920,\n",
       " 'cote': 921,\n",
       " 'laissees': 922,\n",
       " 'tirer': 923,\n",
       " 'rends': 924,\n",
       " 'laisses': 925,\n",
       " 'regime': 926,\n",
       " 'hors': 927,\n",
       " 'grimpe': 928,\n",
       " 'annule': 929,\n",
       " 'ami': 930,\n",
       " 'bonnes': 931,\n",
       " 'pause': 932,\n",
       " 'fruits': 933,\n",
       " 'celibataire': 934,\n",
       " 'etaitil': 935,\n",
       " 'navons': 936,\n",
       " 'boston': 937,\n",
       " 'sera': 938,\n",
       " 'sombre': 939,\n",
       " 'etiezvous': 940,\n",
       " 'derange': 941,\n",
       " 'armes': 942,\n",
       " 'clair': 943,\n",
       " 'ment': 944,\n",
       " 'marre': 945,\n",
       " 'desormais': 946,\n",
       " 'calmezvous': 947,\n",
       " 'dernier': 948,\n",
       " 'disle': 949,\n",
       " 'attaque': 950,\n",
       " 'garcons': 951,\n",
       " 'eveille': 952,\n",
       " 'celuici': 953,\n",
       " 'sien': 954,\n",
       " 'rencontres': 955,\n",
       " 'mignonne': 956,\n",
       " 'mercis': 957,\n",
       " 'meilleur': 958,\n",
       " 'souci': 959,\n",
       " 'revenu': 960,\n",
       " 'masseoir': 961,\n",
       " 'poussez': 962,\n",
       " 'reviendrai': 963,\n",
       " 'soupe': 964,\n",
       " 'serremoi': 965,\n",
       " 'montez': 966,\n",
       " 'suivez': 967,\n",
       " 'detendezvous': 968,\n",
       " 'etaitce': 969,\n",
       " 'change': 970,\n",
       " 'nayez': 971,\n",
       " 'effrayant': 972,\n",
       " 'volontaire': 973,\n",
       " 'quon': 974,\n",
       " 'seche': 975,\n",
       " 'mordu': 976,\n",
       " 'erreur': 977,\n",
       " 'relaxe': 978,\n",
       " 'partis': 979,\n",
       " 'chaussures': 980,\n",
       " 'papa': 981,\n",
       " 'suivi': 982,\n",
       " 'plaisantez': 983,\n",
       " 'avancez': 984,\n",
       " 'grossier': 985,\n",
       " 'sures': 986,\n",
       " 'voyait': 987,\n",
       " 'loi': 988,\n",
       " 'nabandonne': 989,\n",
       " 'pieds': 990,\n",
       " 'pari': 991,\n",
       " 'gateau': 992,\n",
       " 'loin': 993,\n",
       " 'sourire': 994,\n",
       " 'recommence': 995,\n",
       " 'conduit': 996,\n",
       " 'apeure': 997,\n",
       " 'jaloux': 998,\n",
       " 'important': 999,\n",
       " 'paierai': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word index of French tokenizer\n",
    "french_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the training dataset\n",
    "x_train = encode_sequences(french_tokenizer, french_maximum_length, train[:, 1])\n",
    "y_train = encode_sequences(eng_tokenizer, english_maximum_length, train[:, 0])\n",
    "y_train = encode_output(y_train, english_vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the validation dataset\n",
    "x_valid = encode_sequences(french_tokenizer, french_maximum_length, valid[:, 1])\n",
    "y_valid = encode_sequences(eng_tokenizer, english_maximum_length, valid[:, 0])\n",
    "y_valid = encode_output(y_valid, english_vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building testing dataset\n",
    "x_test = encode_sequences(french_tokenizer, french_maximum_length, test[:, 1])\n",
    "y_test = encode_sequences(eng_tokenizer, english_maximum_length, test[:, 0])\n",
    "y_test = encode_output(y_test, english_vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Both Encoder GRU and Decoder GRU cell\n",
    "def both_gru(french_vocab, english_vocab, french_timesteps, english_timesteps, num_units):\n",
    "\tlearning_rate = 1e-2\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Embedding(french_vocab, num_units, input_length = french_timesteps, mask_zero = True))\n",
    "\tmodel.add(GRU(num_units))\n",
    "\tmodel.add(RepeatVector(english_timesteps))\n",
    "\tmodel.add(GRU(num_units,return_sequences=True))\n",
    "\tmodel.add(TimeDistributed(Dense(english_vocab, activation='softmax')))\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Model-1\n",
    "\n",
    "learning_rate = 1e-2\n",
    "both_gru = both_gru(french_vocabulary_size, english_vocabulary_size, french_maximum_length, english_maximum_length, 256)\n",
    "both_gru.compile(optimizer = Adam(learning_rate), loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 10, 256)           1472768   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 256)               394752    \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 5, 256)            394752    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 5, 2776)           713432    \n",
      "=================================================================\n",
      "Total params: 2,975,704\n",
      "Trainable params: 2,975,704\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "# Summary of the Model-1\n",
    "\n",
    "print(both_gru.summary())\n",
    "plot_model(both_gru, to_file='gru_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "165/165 - 27s - loss: 3.6164 - categorical_accuracy: 0.4724 - val_loss: 3.1550 - val_categorical_accuracy: 0.5169\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.15496, saving model to both_gru.h5\n",
      "Epoch 2/30\n",
      "165/165 - 11s - loss: 2.7978 - categorical_accuracy: 0.5495 - val_loss: 2.7806 - val_categorical_accuracy: 0.5655\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.15496 to 2.78064, saving model to both_gru.h5\n",
      "Epoch 3/30\n",
      "165/165 - 10s - loss: 2.3569 - categorical_accuracy: 0.5946 - val_loss: 2.5868 - val_categorical_accuracy: 0.5869\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.78064 to 2.58683, saving model to both_gru.h5\n",
      "Epoch 4/30\n",
      "165/165 - 10s - loss: 2.0226 - categorical_accuracy: 0.6243 - val_loss: 2.4399 - val_categorical_accuracy: 0.6047\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.58683 to 2.43994, saving model to both_gru.h5\n",
      "Epoch 5/30\n",
      "165/165 - 10s - loss: 1.7371 - categorical_accuracy: 0.6542 - val_loss: 2.3341 - val_categorical_accuracy: 0.6193\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.43994 to 2.33408, saving model to both_gru.h5\n",
      "Epoch 6/30\n",
      "165/165 - 10s - loss: 1.4986 - categorical_accuracy: 0.6809 - val_loss: 2.2673 - val_categorical_accuracy: 0.6279\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.33408 to 2.26729, saving model to both_gru.h5\n",
      "Epoch 7/30\n",
      "165/165 - 10s - loss: 1.3089 - categorical_accuracy: 0.7075 - val_loss: 2.2183 - val_categorical_accuracy: 0.6351\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.26729 to 2.21828, saving model to both_gru.h5\n",
      "Epoch 8/30\n",
      "165/165 - 12s - loss: 1.1369 - categorical_accuracy: 0.7312 - val_loss: 2.1846 - val_categorical_accuracy: 0.6408\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.21828 to 2.18458, saving model to both_gru.h5\n",
      "Epoch 9/30\n",
      "165/165 - 11s - loss: 0.9818 - categorical_accuracy: 0.7584 - val_loss: 2.1482 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.18458 to 2.14819, saving model to both_gru.h5\n",
      "Epoch 10/30\n",
      "165/165 - 10s - loss: 0.8683 - categorical_accuracy: 0.7804 - val_loss: 2.1835 - val_categorical_accuracy: 0.6513\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.14819\n",
      "Epoch 11/30\n",
      "165/165 - 10s - loss: 0.7903 - categorical_accuracy: 0.7970 - val_loss: 2.2201 - val_categorical_accuracy: 0.6548\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.14819\n",
      "Epoch 12/30\n",
      "165/165 - 10s - loss: 0.7312 - categorical_accuracy: 0.8092 - val_loss: 2.2226 - val_categorical_accuracy: 0.6501\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.14819\n",
      "Epoch 13/30\n",
      "165/165 - 10s - loss: 0.6769 - categorical_accuracy: 0.8209 - val_loss: 2.2085 - val_categorical_accuracy: 0.6525\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.14819\n",
      "Epoch 14/30\n",
      "165/165 - 11s - loss: 0.6147 - categorical_accuracy: 0.8331 - val_loss: 2.2157 - val_categorical_accuracy: 0.6585\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.14819\n",
      "Epoch 15/30\n",
      "165/165 - 11s - loss: 0.5611 - categorical_accuracy: 0.8471 - val_loss: 2.2435 - val_categorical_accuracy: 0.6608\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 2.14819\n",
      "Epoch 16/30\n",
      "165/165 - 10s - loss: 0.5382 - categorical_accuracy: 0.8521 - val_loss: 2.2489 - val_categorical_accuracy: 0.6557\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 2.14819\n",
      "Epoch 17/30\n",
      "165/165 - 10s - loss: 0.5073 - categorical_accuracy: 0.8564 - val_loss: 2.3035 - val_categorical_accuracy: 0.6656\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 2.14819\n",
      "Epoch 18/30\n",
      "165/165 - 10s - loss: 0.5016 - categorical_accuracy: 0.8574 - val_loss: 2.3066 - val_categorical_accuracy: 0.6635\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 2.14819\n",
      "Epoch 19/30\n",
      "165/165 - 10s - loss: 0.4651 - categorical_accuracy: 0.8680 - val_loss: 2.3003 - val_categorical_accuracy: 0.6592\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 2.14819\n",
      "Epoch 20/30\n",
      "165/165 - 10s - loss: 0.4351 - categorical_accuracy: 0.8740 - val_loss: 2.3357 - val_categorical_accuracy: 0.6681\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 2.14819\n",
      "Epoch 21/30\n",
      "165/165 - 10s - loss: 0.4186 - categorical_accuracy: 0.8798 - val_loss: 2.3422 - val_categorical_accuracy: 0.6627\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 2.14819\n",
      "Epoch 22/30\n",
      "165/165 - 12s - loss: 0.4119 - categorical_accuracy: 0.8807 - val_loss: 2.3798 - val_categorical_accuracy: 0.6620\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 2.14819\n",
      "Epoch 23/30\n",
      "165/165 - 11s - loss: 0.4306 - categorical_accuracy: 0.8730 - val_loss: 2.3927 - val_categorical_accuracy: 0.6616\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 2.14819\n",
      "Epoch 24/30\n",
      "165/165 - 10s - loss: 0.4119 - categorical_accuracy: 0.8786 - val_loss: 2.3970 - val_categorical_accuracy: 0.6577\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 2.14819\n",
      "Epoch 25/30\n",
      "165/165 - 11s - loss: 0.4211 - categorical_accuracy: 0.8762 - val_loss: 2.4337 - val_categorical_accuracy: 0.6488\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 2.14819\n",
      "Epoch 26/30\n",
      "165/165 - 11s - loss: 0.4331 - categorical_accuracy: 0.8717 - val_loss: 2.4595 - val_categorical_accuracy: 0.6509\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 2.14819\n",
      "Epoch 27/30\n",
      "165/165 - 11s - loss: 0.4332 - categorical_accuracy: 0.8717 - val_loss: 2.4886 - val_categorical_accuracy: 0.6551\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 2.14819\n",
      "Epoch 28/30\n",
      "165/165 - 11s - loss: 0.4240 - categorical_accuracy: 0.8745 - val_loss: 2.4928 - val_categorical_accuracy: 0.6581\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 2.14819\n",
      "Epoch 29/30\n",
      "165/165 - 11s - loss: 0.4083 - categorical_accuracy: 0.8779 - val_loss: 2.5170 - val_categorical_accuracy: 0.6540\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 2.14819\n",
      "Epoch 30/30\n",
      "165/165 - 11s - loss: 0.4183 - categorical_accuracy: 0.8744 - val_loss: 2.5102 - val_categorical_accuracy: 0.6532\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 2.14819\n"
     ]
    }
   ],
   "source": [
    "# Fitting the Model-1\n",
    "filename = 'both_gru.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "history_both_gru = both_gru.fit(x_train, y_train, epochs = 30, batch_size = 64, validation_data=(x_valid, y_valid), callbacks = [checkpoint], verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        loss  categorical_accuracy  val_loss  val_categorical_accuracy\n",
      "0   3.616443              0.472419  3.154955                  0.516933\n",
      "1   2.797794              0.549467  2.780640                  0.565467\n",
      "2   2.356876              0.594590  2.586829                  0.586933\n",
      "3   2.022557              0.624267  2.439938                  0.604667\n",
      "4   1.737110              0.654209  2.334076                  0.619333\n",
      "5   1.498589              0.680933  2.267290                  0.627867\n",
      "6   1.308935              0.707505  2.218283                  0.635067\n",
      "7   1.136943              0.731181  2.184579                  0.640800\n",
      "8   0.981828              0.758438  2.148189                  0.650000\n",
      "9   0.868323              0.780362  2.183507                  0.651333\n",
      "10  0.790319              0.797029  2.220069                  0.654800\n",
      "11  0.731204              0.809181  2.222612                  0.650133\n",
      "12  0.676890              0.820933  2.208533                  0.652533\n",
      "13  0.614675              0.833067  2.215735                  0.658533\n",
      "14  0.561076              0.847124  2.243456                  0.660800\n",
      "15  0.538227              0.852057  2.248871                  0.655733\n",
      "16  0.507331              0.856381  2.303508                  0.665600\n",
      "17  0.501554              0.857352  2.306629                  0.663467\n",
      "18  0.465090              0.868038  2.300340                  0.659200\n",
      "19  0.435098              0.873962  2.335677                  0.668133\n",
      "20  0.418633              0.879752  2.342199                  0.662667\n",
      "21  0.411857              0.880686  2.379785                  0.662000\n",
      "22  0.430558              0.873048  2.392745                  0.661600\n",
      "23  0.411913              0.878552  2.397018                  0.657733\n",
      "24  0.421070              0.876209  2.433667                  0.648800\n",
      "25  0.433062              0.871676  2.459460                  0.650933\n",
      "26  0.433158              0.871733  2.488617                  0.655067\n",
      "27  0.424042              0.874476  2.492827                  0.658133\n",
      "28  0.408274              0.877905  2.516997                  0.654000\n",
      "29  0.418251              0.874362  2.510224                  0.653200\n"
     ]
    }
   ],
   "source": [
    "# Saving the history of the Model-1\n",
    "\n",
    "df_history_both_gru = pd.DataFrame(history_both_gru.history)\n",
    "df_history_both_gru.to_csv('df_history_both_gru.csv')\n",
    "print(df_history_both_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Model-1\n",
    "both_gru = load_model('both_gru.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French(Source) : je me suis marre\n",
      "Target : i had some fun\n",
      "Predicted : i had i fun \n",
      "\n",
      "French(Source) : laissezmoi sortir\n",
      "Target : let me out\n",
      "Predicted : let me out \n",
      "\n",
      "French(Source) : on peut me faire confiance\n",
      "Target : im trustworthy\n",
      "Predicted : we must win \n",
      "\n",
      "French(Source) : quelquun estil la\n",
      "Target : anybody home\n",
      "Predicted : anybody anybody \n",
      "\n",
      "French(Source) : mettezle la\n",
      "Target : put it there\n",
      "Predicted : put it there \n",
      "\n",
      "French(Source) : vous etes sournois\n",
      "Target : youre sneaky\n",
      "Predicted : youre sneaky \n",
      "\n",
      "French(Source) : ne vous precipitez pas\n",
      "Target : dont rush\n",
      "Predicted : dont back \n",
      "\n",
      "French(Source) : degage\n",
      "Target : go away\n",
      "Predicted : get away \n",
      "\n",
      "French(Source) : elle laide\n",
      "Target : she helps him\n",
      "Predicted : she helps him \n",
      "\n",
      "French(Source) : je ny parviens pas\n",
      "Target : i cant do it\n",
      "Predicted : i cant it it \n",
      "\n",
      "French(Source) : jai la priorite sur vous\n",
      "Target : i outrank you\n",
      "Predicted : i outrank you \n",
      "\n",
      "French(Source) : je men suis doute\n",
      "Target : i thought so\n",
      "Predicted : i thought so \n",
      "\n",
      "French(Source) : je me suis amendee\n",
      "Target : im reformed\n",
      "Predicted : i froze \n",
      "\n",
      "French(Source) : y atil qui que ce soit ici\n",
      "Target : is anyone here\n",
      "Predicted : here anybody here here \n",
      "\n",
      "French(Source) : de the sil vous plait\n",
      "Target : tea please\n",
      "Predicted : please please \n",
      "\n",
      "French(Source) : tu sais qui je suis\n",
      "Target : do you know me\n",
      "Predicted : you you you \n",
      "\n",
      "French(Source) : tom sest exerce\n",
      "Target : tom exercised\n",
      "Predicted : tom exercised \n",
      "\n",
      "French(Source) : vous netes pas grosse\n",
      "Target : youre not fat\n",
      "Predicted : youre not fat \n",
      "\n",
      "French(Source) : tom est plus malin\n",
      "Target : tom is smarter\n",
      "Predicted : tom is smarter \n",
      "\n",
      "French(Source) : elles sont mauvaises\n",
      "Target : theyre bad\n",
      "Predicted : theyre boring \n",
      "\n",
      "French(Source) : je suis respectueux\n",
      "Target : im observant\n",
      "Predicted : im observant \n",
      "\n",
      "French(Source) : je te remercie\n",
      "Target : thank you\n",
      "Predicted : i thank you \n",
      "\n",
      "French(Source) : jecoute\n",
      "Target : im listening\n",
      "Predicted : im listening \n",
      "\n",
      "French(Source) : nous sommes amoureux\n",
      "Target : were in love\n",
      "Predicted : were in love \n",
      "\n",
      "French(Source) : pour sur\n",
      "Target : of course\n",
      "Predicted : of course \n",
      "\n",
      "French(Source) : pouvezvous faire cela\n",
      "Target : can you do that\n",
      "Predicted : can you see it \n",
      "\n",
      "French(Source) : je suis en train de conduire\n",
      "Target : im driving\n",
      "Predicted : im driving \n",
      "\n",
      "French(Source) : je me sens perdue\n",
      "Target : i feel lost\n",
      "Predicted : i feel faint \n",
      "\n",
      "French(Source) : desormais je me sens mal\n",
      "Target : now i feel bad\n",
      "Predicted : say i feel \n",
      "\n",
      "French(Source) : prenez le commandement\n",
      "Target : take command\n",
      "Predicted : take command \n",
      "\n",
      "French(Source) : ils disposent de vin\n",
      "Target : they have wine\n",
      "Predicted : they have wine \n",
      "\n",
      "French(Source) : tom est special\n",
      "Target : tom is special\n",
      "Predicted : tom is \n",
      "\n",
      "French(Source) : cest vrai\n",
      "Target : its true\n",
      "Predicted : thats is \n",
      "\n",
      "French(Source) : cest notre devoir\n",
      "Target : its our duty\n",
      "Predicted : its obvious duty \n",
      "\n",
      "French(Source) : il court\n",
      "Target : he runs\n",
      "Predicted : he runs it \n",
      "\n",
      "French(Source) : donnemoi la moitie\n",
      "Target : give me half\n",
      "Predicted : give me explain \n",
      "\n",
      "French(Source) : je ne malarme pas\n",
      "Target : im not alarmed\n",
      "Predicted : im not care \n",
      "\n",
      "French(Source) : jai eu besoin dargent\n",
      "Target : i needed money\n",
      "Predicted : i need money \n",
      "\n",
      "French(Source) : etesvous au travail\n",
      "Target : are you at work\n",
      "Predicted : are you at work \n",
      "\n",
      "French(Source) : tu as lair de tennuyer\n",
      "Target : you look bored\n",
      "Predicted : you look you \n",
      "\n",
      "French(Source) : jai ete offense\n",
      "Target : i was offended\n",
      "Predicted : i was offended \n",
      "\n",
      "French(Source) : estelle medecin\n",
      "Target : is she a doctor\n",
      "Predicted : is she a doctor \n",
      "\n",
      "French(Source) : je suis choque\n",
      "Target : im shocked\n",
      "Predicted : im stupid \n",
      "\n",
      "French(Source) : aije besoin de continuer\n",
      "Target : need i go on\n",
      "Predicted : did need need \n",
      "\n",
      "French(Source) : aidezmoi tom\n",
      "Target : help me tom\n",
      "Predicted : let tom tom \n",
      "\n",
      "French(Source) : soyez prets\n",
      "Target : be prepared\n",
      "Predicted : be prepared \n",
      "\n",
      "French(Source) : jai promis a tom\n",
      "Target : i promised tom\n",
      "Predicted : i punished tom \n",
      "\n",
      "French(Source) : je me suis debattue\n",
      "Target : i struggled\n",
      "Predicted : i stood \n",
      "\n",
      "French(Source) : tu dois y aller\n",
      "Target : you have to go\n",
      "Predicted : you must leave \n",
      "\n",
      "French(Source) : fiezvous juste a moi\n",
      "Target : just trust me\n",
      "Predicted : nobody one me \n",
      "\n",
      "BLEU score: 0.227204\n"
     ]
    }
   ],
   "source": [
    "#  Model-1 on the training sequences\n",
    "\n",
    "bleu_train_both_gru = model_evaluation(both_gru, eng_tokenizer, x_train, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French(Source) : faites comme il vous plaira\n",
      "Target : do as you like\n",
      "Predicted : do as you want \n",
      "\n",
      "French(Source) : nous partons a la retraite\n",
      "Target : were retiring\n",
      "Predicted : we got \n",
      "\n",
      "French(Source) : tes givree\n",
      "Target : youre nuts\n",
      "Predicted : youre stink \n",
      "\n",
      "French(Source) : apportez un appareil photo\n",
      "Target : bring a camera\n",
      "Predicted : bring your camera \n",
      "\n",
      "French(Source) : tu sembles contrariee\n",
      "Target : you seem upset\n",
      "Predicted : you seem upset \n",
      "\n",
      "French(Source) : peutetre viendratelle\n",
      "Target : she may come\n",
      "Predicted : how could us \n",
      "\n",
      "French(Source) : je dispose dun plan\n",
      "Target : i have a plan\n",
      "Predicted : i had a plan \n",
      "\n",
      "French(Source) : je dormis tout le jour\n",
      "Target : i slept all day\n",
      "Predicted : i from from \n",
      "\n",
      "French(Source) : jai vu cela\n",
      "Target : i saw that\n",
      "Predicted : i said that \n",
      "\n",
      "French(Source) : tom sauta\n",
      "Target : tom jumped\n",
      "Predicted : tom was \n",
      "\n",
      "French(Source) : pouvonsnous taider\n",
      "Target : can we help you\n",
      "Predicted : can you help \n",
      "\n",
      "French(Source) : voila cinq dollars\n",
      "Target : heres\n",
      "Predicted : the the the \n",
      "\n",
      "French(Source) : jai besoin dun couteau\n",
      "Target : i need a knife\n",
      "Predicted : i need a ride \n",
      "\n",
      "French(Source) : bonjour tout le monde\n",
      "Target : hello everyone\n",
      "Predicted : everyone everyone \n",
      "\n",
      "French(Source) : la recree est terminee\n",
      "Target : recess ended\n",
      "Predicted : recess ended \n",
      "\n",
      "French(Source) : viens dehors\n",
      "Target : come outside\n",
      "Predicted : come it \n",
      "\n",
      "French(Source) : quand cela atil eu lieu\n",
      "Target : when was that\n",
      "Predicted : it was it it \n",
      "\n",
      "French(Source) : cest le sien\n",
      "Target : its hers\n",
      "Predicted : its hers \n",
      "\n",
      "French(Source) : je nourris le chien\n",
      "Target : i fed the dog\n",
      "Predicted : i fed the dog \n",
      "\n",
      "French(Source) : jai ete contente\n",
      "Target : i was pleased\n",
      "Predicted : i was good \n",
      "\n",
      "French(Source) : tom sest senti triste\n",
      "Target : tom felt sad\n",
      "Predicted : tom felt sad \n",
      "\n",
      "French(Source) : je ne savais pas\n",
      "Target : i didnt know\n",
      "Predicted : i didnt knew clue \n",
      "\n",
      "French(Source) : tout est la\n",
      "Target : its all there\n",
      "Predicted : its is \n",
      "\n",
      "French(Source) : me faitesvous confiance\n",
      "Target : do you trust me\n",
      "Predicted : i trusted it \n",
      "\n",
      "French(Source) : vise plus haut\n",
      "Target : aim higher\n",
      "Predicted : its is \n",
      "\n",
      "French(Source) : jai beaucoup rate\n",
      "Target : i missed a lot\n",
      "Predicted : i a a fever \n",
      "\n",
      "French(Source) : cest toi\n",
      "Target : is it you\n",
      "Predicted : you yours \n",
      "\n",
      "French(Source) : ce nest pas facile\n",
      "Target : its not easy\n",
      "Predicted : its isnt \n",
      "\n",
      "French(Source) : personne ne la su\n",
      "Target : no one knew it\n",
      "Predicted : nobody one \n",
      "\n",
      "French(Source) : regarde\n",
      "Target : have a look\n",
      "Predicted : get away away \n",
      "\n",
      "French(Source) : montremoi\n",
      "Target : show me\n",
      "Predicted : make it \n",
      "\n",
      "French(Source) : ce nest pas un menteur\n",
      "Target : hes not a liar\n",
      "Predicted : its no good \n",
      "\n",
      "French(Source) : dites sil vous plait\n",
      "Target : say please\n",
      "Predicted : please please \n",
      "\n",
      "French(Source) : prendsle\n",
      "Target : pick him up\n",
      "Predicted : take it \n",
      "\n",
      "French(Source) : on est rentres maintenant\n",
      "Target : were home now\n",
      "Predicted : now now now \n",
      "\n",
      "French(Source) : jaime les maths\n",
      "Target : i like math\n",
      "Predicted : i like grapes \n",
      "\n",
      "French(Source) : je men occuperai\n",
      "Target : ill handle it\n",
      "Predicted : i guide it \n",
      "\n",
      "French(Source) : arrete de bailler\n",
      "Target : stop yawning\n",
      "Predicted : stop pouting \n",
      "\n",
      "French(Source) : je suis bilingue\n",
      "Target : im bilingual\n",
      "Predicted : im watching adult \n",
      "\n",
      "French(Source) : soyez le bienvenu\n",
      "Target : welcome\n",
      "Predicted : welcome be \n",
      "\n",
      "French(Source) : je lai vu en premier\n",
      "Target : i saw it first\n",
      "Predicted : i always him \n",
      "\n",
      "French(Source) : oubliezle\n",
      "Target : forget him\n",
      "Predicted : forget forget \n",
      "\n",
      "French(Source) : je suis dans un endroit\n",
      "Target : i am in a spot\n",
      "Predicted : im a a robot \n",
      "\n",
      "French(Source) : je suis quelquun de bien\n",
      "Target : im a nice guy\n",
      "Predicted : im very good \n",
      "\n",
      "French(Source) : restez au lit\n",
      "Target : stay in bed\n",
      "Predicted : stay bed bed \n",
      "\n",
      "French(Source) : tu en as fini\n",
      "Target : youre through\n",
      "Predicted : youre through \n",
      "\n",
      "French(Source) : reste alite\n",
      "Target : stay in bed\n",
      "Predicted : stay go bed \n",
      "\n",
      "French(Source) : tu seras en securite\n",
      "Target : youll be safe\n",
      "Predicted : you work \n",
      "\n",
      "French(Source) : comme elle est belle\n",
      "Target : how nice\n",
      "Predicted : how beautiful \n",
      "\n",
      "French(Source) : cest malhonnete\n",
      "Target : its dishonest\n",
      "Predicted : this is \n",
      "\n",
      "BLEU score: 0.083297\n"
     ]
    }
   ],
   "source": [
    "# Testing Model-1 \n",
    "\n",
    "bleu_test_both_gru = model_evaluation(both_gru, eng_tokenizer, x_test, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Both Encoder LSTM and Decoder LSTM cell\n",
    "def both_lstm(french_vocab, english_vocab, french_timesteps, english_timesteps, num_units):\n",
    "\tlearning_rate = 1e-2\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Embedding(french_vocab, num_units, input_length= french_timesteps, mask_zero=True))\n",
    "\tmodel.add(LSTM(num_units))\n",
    "\tmodel.add(RepeatVector(english_timesteps))\n",
    "\tmodel.add(LSTM(num_units,return_sequences=True))\n",
    "\tmodel.add(TimeDistributed(Dense(english_vocab, activation='softmax')))\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Model-2\n",
    "\n",
    "learning_rate = 1e-2\n",
    "both_lstm = both_lstm(french_vocabulary_size, english_vocabulary_size, french_maximum_length, english_maximum_length, 256)\n",
    "both_lstm.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 10, 256)           1472768   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 5, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 5, 2776)           713432    \n",
      "=================================================================\n",
      "Total params: 3,236,824\n",
      "Trainable params: 3,236,824\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "# Summarising Model-2\n",
    "\n",
    "print(both_lstm.summary())\n",
    "plot_model(both_lstm, to_file='both_lstm.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "165/165 - 22s - loss: 3.6130 - categorical_accuracy: 0.4722 - val_loss: 3.1232 - val_categorical_accuracy: 0.5061\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.12317, saving model to both_lstm.h5\n",
      "Epoch 2/30\n",
      "165/165 - 14s - loss: 2.7115 - categorical_accuracy: 0.5495 - val_loss: 2.6402 - val_categorical_accuracy: 0.5712\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.12317 to 2.64020, saving model to both_lstm.h5\n",
      "Epoch 3/30\n",
      "165/165 - 14s - loss: 2.1778 - categorical_accuracy: 0.6073 - val_loss: 2.4160 - val_categorical_accuracy: 0.5972\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.64020 to 2.41602, saving model to both_lstm.h5\n",
      "Epoch 4/30\n",
      "165/165 - 15s - loss: 1.8106 - categorical_accuracy: 0.6439 - val_loss: 2.2561 - val_categorical_accuracy: 0.6215\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.41602 to 2.25609, saving model to both_lstm.h5\n",
      "Epoch 5/30\n",
      "165/165 - 16s - loss: 1.5008 - categorical_accuracy: 0.6817 - val_loss: 2.1887 - val_categorical_accuracy: 0.6320\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.25609 to 2.18869, saving model to both_lstm.h5\n",
      "Epoch 6/30\n",
      "165/165 - 15s - loss: 1.2311 - categorical_accuracy: 0.7184 - val_loss: 2.1437 - val_categorical_accuracy: 0.6411\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.18869 to 2.14372, saving model to both_lstm.h5\n",
      "Epoch 7/30\n",
      "165/165 - 16s - loss: 1.0180 - categorical_accuracy: 0.7528 - val_loss: 2.1598 - val_categorical_accuracy: 0.6521\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.14372\n",
      "Epoch 8/30\n",
      "165/165 - 12s - loss: 0.8295 - categorical_accuracy: 0.7881 - val_loss: 2.1420 - val_categorical_accuracy: 0.6620\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.14372 to 2.14197, saving model to both_lstm.h5\n",
      "Epoch 9/30\n",
      "165/165 - 12s - loss: 0.6756 - categorical_accuracy: 0.8199 - val_loss: 2.1671 - val_categorical_accuracy: 0.6605\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.14197\n",
      "Epoch 10/30\n",
      "165/165 - 12s - loss: 0.5785 - categorical_accuracy: 0.8434 - val_loss: 2.2046 - val_categorical_accuracy: 0.6691\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.14197\n",
      "Epoch 11/30\n",
      "165/165 - 12s - loss: 0.4915 - categorical_accuracy: 0.8635 - val_loss: 2.2387 - val_categorical_accuracy: 0.6677\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.14197\n",
      "Epoch 12/30\n",
      "165/165 - 12s - loss: 0.4414 - categorical_accuracy: 0.8767 - val_loss: 2.2843 - val_categorical_accuracy: 0.6721\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.14197\n",
      "Epoch 13/30\n",
      "165/165 - 14s - loss: 0.3879 - categorical_accuracy: 0.8901 - val_loss: 2.2739 - val_categorical_accuracy: 0.6703\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.14197\n",
      "Epoch 14/30\n",
      "165/165 - 14s - loss: 0.3565 - categorical_accuracy: 0.8974 - val_loss: 2.3262 - val_categorical_accuracy: 0.6697\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.14197\n",
      "Epoch 15/30\n",
      "165/165 - 15s - loss: 0.3299 - categorical_accuracy: 0.9044 - val_loss: 2.3534 - val_categorical_accuracy: 0.6685\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 2.14197\n",
      "Epoch 16/30\n",
      "165/165 - 12s - loss: 0.3044 - categorical_accuracy: 0.9112 - val_loss: 2.3594 - val_categorical_accuracy: 0.6797\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 2.14197\n",
      "Epoch 17/30\n",
      "165/165 - 12s - loss: 0.2894 - categorical_accuracy: 0.9150 - val_loss: 2.4147 - val_categorical_accuracy: 0.6705\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 2.14197\n",
      "Epoch 18/30\n",
      "165/165 - 12s - loss: 0.2701 - categorical_accuracy: 0.9207 - val_loss: 2.4031 - val_categorical_accuracy: 0.6729\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 2.14197\n",
      "Epoch 19/30\n",
      "165/165 - 12s - loss: 0.2686 - categorical_accuracy: 0.9190 - val_loss: 2.4324 - val_categorical_accuracy: 0.6715\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 2.14197\n",
      "Epoch 20/30\n",
      "165/165 - 15s - loss: 0.2561 - categorical_accuracy: 0.9235 - val_loss: 2.4329 - val_categorical_accuracy: 0.6681\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 2.14197\n",
      "Epoch 21/30\n",
      "165/165 - 16s - loss: 0.2660 - categorical_accuracy: 0.9192 - val_loss: 2.4856 - val_categorical_accuracy: 0.6676\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 2.14197\n",
      "Epoch 22/30\n",
      "165/165 - 13s - loss: 0.2583 - categorical_accuracy: 0.9197 - val_loss: 2.4731 - val_categorical_accuracy: 0.6713\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 2.14197\n",
      "Epoch 23/30\n",
      "165/165 - 12s - loss: 0.2522 - categorical_accuracy: 0.9229 - val_loss: 2.5096 - val_categorical_accuracy: 0.6673\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 2.14197\n",
      "Epoch 24/30\n",
      "165/165 - 12s - loss: 0.2439 - categorical_accuracy: 0.9257 - val_loss: 2.5521 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 2.14197\n",
      "Epoch 25/30\n",
      "165/165 - 12s - loss: 0.2578 - categorical_accuracy: 0.9212 - val_loss: 2.5311 - val_categorical_accuracy: 0.6672\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 2.14197\n",
      "Epoch 26/30\n",
      "165/165 - 16s - loss: 0.2559 - categorical_accuracy: 0.9217 - val_loss: 2.5664 - val_categorical_accuracy: 0.6688\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 2.14197\n",
      "Epoch 27/30\n",
      "165/165 - 13s - loss: 0.2573 - categorical_accuracy: 0.9198 - val_loss: 2.5945 - val_categorical_accuracy: 0.6727\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 2.14197\n",
      "Epoch 28/30\n",
      "165/165 - 13s - loss: 0.2606 - categorical_accuracy: 0.9189 - val_loss: 2.5981 - val_categorical_accuracy: 0.6657\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 2.14197\n",
      "Epoch 29/30\n",
      "165/165 - 12s - loss: 0.2858 - categorical_accuracy: 0.9118 - val_loss: 2.6081 - val_categorical_accuracy: 0.6657\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 2.14197\n",
      "Epoch 30/30\n",
      "165/165 - 13s - loss: 0.3138 - categorical_accuracy: 0.9028 - val_loss: 2.6390 - val_categorical_accuracy: 0.6597\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 2.14197\n"
     ]
    }
   ],
   "source": [
    "# Fitting the Model-2\n",
    "\n",
    "filename = 'both_lstm.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "history_both_lstm = both_lstm.fit(x_train, y_train, epochs=30, batch_size=64, validation_data=(x_valid, y_valid), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the history of the Model-2\n",
    "\n",
    "df_history_both_lstm = pd.DataFrame(history_both_lstm.history)\n",
    "df_history_both_lstm\n",
    "df_history_both_lstm.to_csv('df_history_both_lstm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Model-2\n",
    "both_lstm = load_model('both_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French(Source) : je me suis marre\n",
      "Target : i had some fun\n",
      "Predicted : i had doubts fun \n",
      "\n",
      "French(Source) : laissezmoi sortir\n",
      "Target : let me out\n",
      "Predicted : let me out \n",
      "\n",
      "French(Source) : on peut me faire confiance\n",
      "Target : im trustworthy\n",
      "Predicted : i trustworthy me \n",
      "\n",
      "French(Source) : quelquun estil la\n",
      "Target : anybody home\n",
      "Predicted : anybody tom home \n",
      "\n",
      "French(Source) : mettezle la\n",
      "Target : put it there\n",
      "Predicted : put it there \n",
      "\n",
      "French(Source) : vous etes sournois\n",
      "Target : youre sneaky\n",
      "Predicted : youre sneaky \n",
      "\n",
      "French(Source) : ne vous precipitez pas\n",
      "Target : dont rush\n",
      "Predicted : dont rush \n",
      "\n",
      "French(Source) : degage\n",
      "Target : go away\n",
      "Predicted : get away \n",
      "\n",
      "French(Source) : elle laide\n",
      "Target : she helps him\n",
      "Predicted : she helps him \n",
      "\n",
      "French(Source) : je ny parviens pas\n",
      "Target : i cant do it\n",
      "Predicted : i cant do it \n",
      "\n",
      "French(Source) : jai la priorite sur vous\n",
      "Target : i outrank you\n",
      "Predicted : i outrank you \n",
      "\n",
      "French(Source) : je men suis doute\n",
      "Target : i thought so\n",
      "Predicted : i thought it doubts \n",
      "\n",
      "French(Source) : je me suis amendee\n",
      "Target : im reformed\n",
      "Predicted : i retired \n",
      "\n",
      "French(Source) : y atil qui que ce soit ici\n",
      "Target : is anyone here\n",
      "Predicted : anybody inside here \n",
      "\n",
      "French(Source) : de the sil vous plait\n",
      "Target : tea please\n",
      "Predicted : water please \n",
      "\n",
      "French(Source) : tu sais qui je suis\n",
      "Target : do you know me\n",
      "Predicted : do you go us \n",
      "\n",
      "French(Source) : tom sest exerce\n",
      "Target : tom exercised\n",
      "Predicted : tom retired \n",
      "\n",
      "French(Source) : vous netes pas grosse\n",
      "Target : youre not fat\n",
      "Predicted : youre not fat \n",
      "\n",
      "French(Source) : tom est plus malin\n",
      "Target : tom is smarter\n",
      "Predicted : is is smarter \n",
      "\n",
      "French(Source) : elles sont mauvaises\n",
      "Target : theyre bad\n",
      "Predicted : theyre bad \n",
      "\n",
      "French(Source) : je suis respectueux\n",
      "Target : im observant\n",
      "Predicted : im observant \n",
      "\n",
      "French(Source) : je te remercie\n",
      "Target : thank you\n",
      "Predicted : thank thank you \n",
      "\n",
      "French(Source) : jecoute\n",
      "Target : im listening\n",
      "Predicted : im morning \n",
      "\n",
      "French(Source) : nous sommes amoureux\n",
      "Target : were in love\n",
      "Predicted : were in love \n",
      "\n",
      "French(Source) : pour sur\n",
      "Target : of course\n",
      "Predicted : of course \n",
      "\n",
      "French(Source) : pouvezvous faire cela\n",
      "Target : can you do that\n",
      "Predicted : can you do that \n",
      "\n",
      "French(Source) : je suis en train de conduire\n",
      "Target : im driving\n",
      "Predicted : im grounded \n",
      "\n",
      "French(Source) : je me sens perdue\n",
      "Target : i feel lost\n",
      "Predicted : i feel lost \n",
      "\n",
      "French(Source) : desormais je me sens mal\n",
      "Target : now i feel bad\n",
      "Predicted : i i feel bad \n",
      "\n",
      "French(Source) : prenez le commandement\n",
      "Target : take command\n",
      "Predicted : take command \n",
      "\n",
      "French(Source) : ils disposent de vin\n",
      "Target : they have wine\n",
      "Predicted : they have wine \n",
      "\n",
      "French(Source) : tom est special\n",
      "Target : tom is special\n",
      "Predicted : tom is \n",
      "\n",
      "French(Source) : cest vrai\n",
      "Target : its true\n",
      "Predicted : its right \n",
      "\n",
      "French(Source) : cest notre devoir\n",
      "Target : its our duty\n",
      "Predicted : its our duty \n",
      "\n",
      "French(Source) : il court\n",
      "Target : he runs\n",
      "Predicted : he runs running \n",
      "\n",
      "French(Source) : donnemoi la moitie\n",
      "Target : give me half\n",
      "Predicted : give me half \n",
      "\n",
      "French(Source) : je ne malarme pas\n",
      "Target : im not alarmed\n",
      "Predicted : im not alarmed \n",
      "\n",
      "French(Source) : jai eu besoin dargent\n",
      "Target : i needed money\n",
      "Predicted : i need money \n",
      "\n",
      "French(Source) : etesvous au travail\n",
      "Target : are you at work\n",
      "Predicted : are you at work \n",
      "\n",
      "French(Source) : tu as lair de tennuyer\n",
      "Target : you look bored\n",
      "Predicted : you look bored \n",
      "\n",
      "French(Source) : jai ete offense\n",
      "Target : i was offended\n",
      "Predicted : i was offended \n",
      "\n",
      "French(Source) : estelle medecin\n",
      "Target : is she a doctor\n",
      "Predicted : is she a doctor \n",
      "\n",
      "French(Source) : je suis choque\n",
      "Target : im shocked\n",
      "Predicted : im shocked \n",
      "\n",
      "French(Source) : aije besoin de continuer\n",
      "Target : need i go on\n",
      "Predicted : must i go \n",
      "\n",
      "French(Source) : aidezmoi tom\n",
      "Target : help me tom\n",
      "Predicted : help me out \n",
      "\n",
      "French(Source) : soyez prets\n",
      "Target : be prepared\n",
      "Predicted : be prepared \n",
      "\n",
      "French(Source) : jai promis a tom\n",
      "Target : i promised tom\n",
      "Predicted : i saw tom \n",
      "\n",
      "French(Source) : je me suis debattue\n",
      "Target : i struggled\n",
      "Predicted : i recovered \n",
      "\n",
      "French(Source) : tu dois y aller\n",
      "Target : you have to go\n",
      "Predicted : you must to go \n",
      "\n",
      "French(Source) : fiezvous juste a moi\n",
      "Target : just trust me\n",
      "Predicted : just want me me \n",
      "\n",
      "BLEU score: 0.304897\n"
     ]
    }
   ],
   "source": [
    "# Model-2 training sequences\n",
    "bleu_train_both_lstm = model_evaluation(both_lstm, eng_tokenizer, x_train, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French(Source) : faites comme il vous plaira\n",
      "Target : do as you like\n",
      "Predicted : do as you you \n",
      "\n",
      "French(Source) : nous partons a la retraite\n",
      "Target : were retiring\n",
      "Predicted : were starved \n",
      "\n",
      "French(Source) : tes givree\n",
      "Target : youre nuts\n",
      "Predicted : youre good \n",
      "\n",
      "French(Source) : apportez un appareil photo\n",
      "Target : bring a camera\n",
      "Predicted : bring a lawyer \n",
      "\n",
      "French(Source) : tu sembles contrariee\n",
      "Target : you seem upset\n",
      "Predicted : you look upset \n",
      "\n",
      "French(Source) : peutetre viendratelle\n",
      "Target : she may come\n",
      "Predicted : call the \n",
      "\n",
      "French(Source) : je dispose dun plan\n",
      "Target : i have a plan\n",
      "Predicted : i bought plan plan \n",
      "\n",
      "French(Source) : je dormis tout le jour\n",
      "Target : i slept all day\n",
      "Predicted : i all off \n",
      "\n",
      "French(Source) : jai vu cela\n",
      "Target : i saw that\n",
      "Predicted : i seen that \n",
      "\n",
      "French(Source) : tom sauta\n",
      "Target : tom jumped\n",
      "Predicted : tom yelled \n",
      "\n",
      "French(Source) : pouvonsnous taider\n",
      "Target : can we help you\n",
      "Predicted : can we help \n",
      "\n",
      "French(Source) : voila cinq dollars\n",
      "Target : heres\n",
      "Predicted : its does \n",
      "\n",
      "French(Source) : jai besoin dun couteau\n",
      "Target : i need a knife\n",
      "Predicted : i need a pencil \n",
      "\n",
      "French(Source) : bonjour tout le monde\n",
      "Target : hello everyone\n",
      "Predicted : everyone upstairs \n",
      "\n",
      "French(Source) : la recree est terminee\n",
      "Target : recess ended\n",
      "Predicted : recess ended \n",
      "\n",
      "French(Source) : viens dehors\n",
      "Target : come outside\n",
      "Predicted : come over \n",
      "\n",
      "French(Source) : quand cela atil eu lieu\n",
      "Target : when was that\n",
      "Predicted : is tom is \n",
      "\n",
      "French(Source) : cest le sien\n",
      "Target : its hers\n",
      "Predicted : its the \n",
      "\n",
      "French(Source) : je nourris le chien\n",
      "Target : i fed the dog\n",
      "Predicted : i fed the dog \n",
      "\n",
      "French(Source) : jai ete contente\n",
      "Target : i was pleased\n",
      "Predicted : i got all \n",
      "\n",
      "French(Source) : tom sest senti triste\n",
      "Target : tom felt sad\n",
      "Predicted : tom felt sad \n",
      "\n",
      "French(Source) : je ne savais pas\n",
      "Target : i didnt know\n",
      "Predicted : i wont cry much \n",
      "\n",
      "French(Source) : tout est la\n",
      "Target : its all there\n",
      "Predicted : its all \n",
      "\n",
      "French(Source) : me faitesvous confiance\n",
      "Target : do you trust me\n",
      "Predicted : i you me \n",
      "\n",
      "French(Source) : vise plus haut\n",
      "Target : aim higher\n",
      "Predicted : its upstairs \n",
      "\n",
      "French(Source) : jai beaucoup rate\n",
      "Target : i missed a lot\n",
      "Predicted : i have a room \n",
      "\n",
      "French(Source) : cest toi\n",
      "Target : is it you\n",
      "Predicted : love done \n",
      "\n",
      "French(Source) : ce nest pas facile\n",
      "Target : its not easy\n",
      "Predicted : its not easy \n",
      "\n",
      "French(Source) : personne ne la su\n",
      "Target : no one knew it\n",
      "Predicted : no one escaped \n",
      "\n",
      "French(Source) : regarde\n",
      "Target : have a look\n",
      "Predicted : look out \n",
      "\n",
      "French(Source) : montremoi\n",
      "Target : show me\n",
      "Predicted : here it \n",
      "\n",
      "French(Source) : ce nest pas un menteur\n",
      "Target : hes not a liar\n",
      "Predicted : its no liar \n",
      "\n",
      "French(Source) : dites sil vous plait\n",
      "Target : say please\n",
      "Predicted : please please please \n",
      "\n",
      "French(Source) : prendsle\n",
      "Target : pick him up\n",
      "Predicted : take it \n",
      "\n",
      "French(Source) : on est rentres maintenant\n",
      "Target : were home now\n",
      "Predicted : its agrees now \n",
      "\n",
      "French(Source) : jaime les maths\n",
      "Target : i like math\n",
      "Predicted : i like stories \n",
      "\n",
      "French(Source) : je men occuperai\n",
      "Target : ill handle it\n",
      "Predicted : i managing it \n",
      "\n",
      "French(Source) : arrete de bailler\n",
      "Target : stop yawning\n",
      "Predicted : stop pouting \n",
      "\n",
      "French(Source) : je suis bilingue\n",
      "Target : im bilingual\n",
      "Predicted : im exhausted \n",
      "\n",
      "French(Source) : soyez le bienvenu\n",
      "Target : welcome\n",
      "Predicted : be still \n",
      "\n",
      "French(Source) : je lai vu en premier\n",
      "Target : i saw it first\n",
      "Predicted : i let him \n",
      "\n",
      "French(Source) : oubliezle\n",
      "Target : forget him\n",
      "Predicted : save it \n",
      "\n",
      "French(Source) : je suis dans un endroit\n",
      "Target : i am in a spot\n",
      "Predicted : im such a kid \n",
      "\n",
      "French(Source) : je suis quelquun de bien\n",
      "Target : im a nice guy\n",
      "Predicted : im exhausted \n",
      "\n",
      "French(Source) : restez au lit\n",
      "Target : stay in bed\n",
      "Predicted : stay in bed \n",
      "\n",
      "French(Source) : tu en as fini\n",
      "Target : youre through\n",
      "Predicted : youre through \n",
      "\n",
      "French(Source) : reste alite\n",
      "Target : stay in bed\n",
      "Predicted : stay in bed \n",
      "\n",
      "French(Source) : tu seras en securite\n",
      "Target : youll be safe\n",
      "Predicted : you look safe \n",
      "\n",
      "French(Source) : comme elle est belle\n",
      "Target : how nice\n",
      "Predicted : how beautiful \n",
      "\n",
      "French(Source) : cest malhonnete\n",
      "Target : its dishonest\n",
      "Predicted : its indecent \n",
      "\n",
      "BLEU score: 0.124619\n"
     ]
    }
   ],
   "source": [
    "# Testing the Model-2 test sequences\n",
    "bleu_test_both_lstm = model_evaluation(both_lstm, eng_tokenizer, x_test, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3:  Encoder GRU and Decoder LSTM cell\n",
    "\n",
    "def gru_lstm(french_vocab, english_vocab, french_timesteps, english_timesteps, num_units):\n",
    "\tlearning_rate = 1e-2\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Embedding(french_vocab, num_units, input_length= french_timesteps, mask_zero=True))\n",
    "\tmodel.add(GRU(num_units))\n",
    "\tmodel.add(RepeatVector(english_timesteps))\n",
    "\tmodel.add(LSTM(num_units,return_sequences=True))\n",
    "\tmodel.add(TimeDistributed(Dense(english_vocab, activation='softmax')))\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Model-3\n",
    "\n",
    "learning_rate = 1e-2\n",
    "gru_lstm = gru_lstm(french_vocabulary_size, english_vocabulary_size, french_maximum_length, english_maximum_length, 256)\n",
    "gru_lstm.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 10, 256)           1472768   \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 256)               394752    \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 5, 2776)           713432    \n",
      "=================================================================\n",
      "Total params: 3,106,264\n",
      "Trainable params: 3,106,264\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "# Summary of Model-3\n",
    "\n",
    "print(gru_lstm.summary())\n",
    "plot_model(gru_lstm, to_file='gru_lstm.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "165/165 - 17s - loss: 3.5322 - categorical_accuracy: 0.4839 - val_loss: 3.0153 - val_categorical_accuracy: 0.5311\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.01526, saving model to gru_lstm.h5\n",
      "Epoch 2/30\n",
      "165/165 - 11s - loss: 2.6284 - categorical_accuracy: 0.5694 - val_loss: 2.6154 - val_categorical_accuracy: 0.5872\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.01526 to 2.61542, saving model to gru_lstm.h5\n",
      "Epoch 3/30\n",
      "165/165 - 11s - loss: 2.1314 - categorical_accuracy: 0.6198 - val_loss: 2.4003 - val_categorical_accuracy: 0.6101\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.61542 to 2.40034, saving model to gru_lstm.h5\n",
      "Epoch 4/30\n",
      "165/165 - 11s - loss: 1.7506 - categorical_accuracy: 0.6601 - val_loss: 2.2451 - val_categorical_accuracy: 0.6261\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.40034 to 2.24515, saving model to gru_lstm.h5\n",
      "Epoch 5/30\n",
      "165/165 - 11s - loss: 1.4514 - categorical_accuracy: 0.6932 - val_loss: 2.1575 - val_categorical_accuracy: 0.6403\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.24515 to 2.15752, saving model to gru_lstm.h5\n",
      "Epoch 6/30\n",
      "165/165 - 12s - loss: 1.1944 - categorical_accuracy: 0.7271 - val_loss: 2.1278 - val_categorical_accuracy: 0.6489\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.15752 to 2.12781, saving model to gru_lstm.h5\n",
      "Epoch 7/30\n",
      "165/165 - 11s - loss: 1.0026 - categorical_accuracy: 0.7583 - val_loss: 2.1049 - val_categorical_accuracy: 0.6539\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.12781 to 2.10486, saving model to gru_lstm.h5\n",
      "Epoch 8/30\n",
      "165/165 - 12s - loss: 0.8407 - categorical_accuracy: 0.7863 - val_loss: 2.0995 - val_categorical_accuracy: 0.6657\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.10486 to 2.09948, saving model to gru_lstm.h5\n",
      "Epoch 9/30\n",
      "165/165 - 11s - loss: 0.7267 - categorical_accuracy: 0.8093 - val_loss: 2.1081 - val_categorical_accuracy: 0.6685\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.09948\n",
      "Epoch 10/30\n",
      "165/165 - 11s - loss: 0.6119 - categorical_accuracy: 0.8345 - val_loss: 2.1192 - val_categorical_accuracy: 0.6664\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.09948\n",
      "Epoch 11/30\n",
      "165/165 - 12s - loss: 0.5359 - categorical_accuracy: 0.8534 - val_loss: 2.1562 - val_categorical_accuracy: 0.6752\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.09948\n",
      "Epoch 12/30\n",
      "165/165 - 13s - loss: 0.4572 - categorical_accuracy: 0.8747 - val_loss: 2.1561 - val_categorical_accuracy: 0.6717\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.09948\n",
      "Epoch 13/30\n",
      "165/165 - 12s - loss: 0.4093 - categorical_accuracy: 0.8850 - val_loss: 2.1654 - val_categorical_accuracy: 0.6745\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.09948\n",
      "Epoch 14/30\n",
      "165/165 - 11s - loss: 0.3846 - categorical_accuracy: 0.8894 - val_loss: 2.2101 - val_categorical_accuracy: 0.6759\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.09948\n",
      "Epoch 15/30\n",
      "165/165 - 11s - loss: 0.3561 - categorical_accuracy: 0.8976 - val_loss: 2.2324 - val_categorical_accuracy: 0.6783\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 2.09948\n",
      "Epoch 16/30\n",
      "165/165 - 11s - loss: 0.3284 - categorical_accuracy: 0.9034 - val_loss: 2.2399 - val_categorical_accuracy: 0.6773\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 2.09948\n",
      "Epoch 17/30\n",
      "165/165 - 12s - loss: 0.3214 - categorical_accuracy: 0.9044 - val_loss: 2.2519 - val_categorical_accuracy: 0.6803\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 2.09948\n",
      "Epoch 18/30\n",
      "165/165 - 11s - loss: 0.3035 - categorical_accuracy: 0.9102 - val_loss: 2.2970 - val_categorical_accuracy: 0.6760\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 2.09948\n",
      "Epoch 19/30\n",
      "165/165 - 11s - loss: 0.3103 - categorical_accuracy: 0.9071 - val_loss: 2.3287 - val_categorical_accuracy: 0.6675\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 2.09948\n",
      "Epoch 20/30\n",
      "165/165 - 11s - loss: 0.2971 - categorical_accuracy: 0.9104 - val_loss: 2.3271 - val_categorical_accuracy: 0.6744\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 2.09948\n",
      "Epoch 21/30\n",
      "165/165 - 11s - loss: 0.3162 - categorical_accuracy: 0.9051 - val_loss: 2.3428 - val_categorical_accuracy: 0.6768\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 2.09948\n",
      "Epoch 22/30\n",
      "165/165 - 11s - loss: 0.3164 - categorical_accuracy: 0.9044 - val_loss: 2.3530 - val_categorical_accuracy: 0.6703\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 2.09948\n",
      "Epoch 23/30\n",
      "165/165 - 11s - loss: 0.3244 - categorical_accuracy: 0.9010 - val_loss: 2.4002 - val_categorical_accuracy: 0.6787\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 2.09948\n",
      "Epoch 24/30\n",
      "165/165 - 11s - loss: 0.3199 - categorical_accuracy: 0.9027 - val_loss: 2.3935 - val_categorical_accuracy: 0.6719\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 2.09948\n",
      "Epoch 25/30\n",
      "165/165 - 11s - loss: 0.3138 - categorical_accuracy: 0.9038 - val_loss: 2.4322 - val_categorical_accuracy: 0.6773\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 2.09948\n",
      "Epoch 26/30\n",
      "165/165 - 11s - loss: 0.3203 - categorical_accuracy: 0.9004 - val_loss: 2.4492 - val_categorical_accuracy: 0.6748\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 2.09948\n",
      "Epoch 27/30\n",
      "165/165 - 11s - loss: 0.3185 - categorical_accuracy: 0.9029 - val_loss: 2.4582 - val_categorical_accuracy: 0.6759\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 2.09948\n",
      "Epoch 28/30\n",
      "165/165 - 12s - loss: 0.3087 - categorical_accuracy: 0.9043 - val_loss: 2.4919 - val_categorical_accuracy: 0.6712\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 2.09948\n",
      "Epoch 29/30\n",
      "165/165 - 12s - loss: 0.3159 - categorical_accuracy: 0.9008 - val_loss: 2.4763 - val_categorical_accuracy: 0.6708\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 2.09948\n",
      "Epoch 30/30\n",
      "165/165 - 12s - loss: 0.3136 - categorical_accuracy: 0.9036 - val_loss: 2.4975 - val_categorical_accuracy: 0.6720\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 2.09948\n"
     ]
    }
   ],
   "source": [
    "# Fitting the Model-3\n",
    "\n",
    "filename = 'gru_lstm.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "history_gru_lstm = gru_lstm.fit(x_train, y_train, epochs=30, batch_size=64, validation_data=(x_valid, y_valid), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        loss  categorical_accuracy  val_loss  val_categorical_accuracy\n",
      "0   3.612989              0.472171  3.123174                  0.506133\n",
      "1   2.711535              0.549467  2.640196                  0.571200\n",
      "2   2.177814              0.607276  2.416023                  0.597200\n",
      "3   1.810572              0.643924  2.256093                  0.621467\n",
      "4   1.500825              0.681695  2.188692                  0.632000\n",
      "5   1.231123              0.718381  2.143722                  0.641067\n",
      "6   1.018034              0.752762  2.159810                  0.652133\n",
      "7   0.829502              0.788076  2.141968                  0.662000\n",
      "8   0.675615              0.819867  2.167118                  0.660533\n",
      "9   0.578506              0.843371  2.204639                  0.669067\n",
      "10  0.491479              0.863543  2.238677                  0.667733\n",
      "11  0.441413              0.876667  2.284345                  0.672133\n",
      "12  0.387935              0.890076  2.273862                  0.670267\n",
      "13  0.356470              0.897390  2.326172                  0.669733\n",
      "14  0.329899              0.904400  2.353433                  0.668533\n",
      "15  0.304421              0.911181  2.359426                  0.679733\n",
      "16  0.289416              0.914971  2.414748                  0.670533\n",
      "17  0.270090              0.920743  2.403123                  0.672933\n",
      "18  0.268648              0.918990  2.432450                  0.671467\n",
      "19  0.256100              0.923467  2.432874                  0.668133\n",
      "20  0.265997              0.919238  2.485591                  0.667600\n",
      "21  0.258273              0.919733  2.473088                  0.671333\n",
      "22  0.252185              0.922914  2.509631                  0.667333\n",
      "23  0.243921              0.925695  2.552095                  0.670000\n",
      "24  0.257793              0.921181  2.531067                  0.667200\n",
      "25  0.255921              0.921657  2.566408                  0.668800\n",
      "26  0.257300              0.919848  2.594534                  0.672667\n",
      "27  0.260570              0.918895  2.598118                  0.665733\n",
      "28  0.285786              0.911771  2.608110                  0.665733\n",
      "29  0.313751              0.902781  2.638984                  0.659733\n"
     ]
    }
   ],
   "source": [
    "# Saving the history of the Model-3\n",
    "\n",
    "df_history_gru_lstm = pd.DataFrame(history_both_lstm.history)\n",
    "df_history_gru_lstm.to_csv('df_history_gru_lstm.csv')\n",
    "print(df_history_gru_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Model-3\n",
    "\n",
    "gru_lstm = load_model('gru_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French(Source) : je me suis marre\n",
      "Target : i had some fun\n",
      "Predicted : i had some fun \n",
      "\n",
      "French(Source) : laissezmoi sortir\n",
      "Target : let me out\n",
      "Predicted : let me out \n",
      "\n",
      "French(Source) : on peut me faire confiance\n",
      "Target : im trustworthy\n",
      "Predicted : we trustworthy win \n",
      "\n",
      "French(Source) : quelquun estil la\n",
      "Target : anybody home\n",
      "Predicted : anybody tom \n",
      "\n",
      "French(Source) : mettezle la\n",
      "Target : put it there\n",
      "Predicted : bring it there \n",
      "\n",
      "French(Source) : vous etes sournois\n",
      "Target : youre sneaky\n",
      "Predicted : youre sneaky \n",
      "\n",
      "French(Source) : ne vous precipitez pas\n",
      "Target : dont rush\n",
      "Predicted : dont fret \n",
      "\n",
      "French(Source) : degage\n",
      "Target : go away\n",
      "Predicted : beat away \n",
      "\n",
      "French(Source) : elle laide\n",
      "Target : she helps him\n",
      "Predicted : she helps him \n",
      "\n",
      "French(Source) : je ny parviens pas\n",
      "Target : i cant do it\n",
      "Predicted : i cant forget \n",
      "\n",
      "French(Source) : jai la priorite sur vous\n",
      "Target : i outrank you\n",
      "Predicted : i outrank you \n",
      "\n",
      "French(Source) : je men suis doute\n",
      "Target : i thought so\n",
      "Predicted : i thought my \n",
      "\n",
      "French(Source) : je me suis amendee\n",
      "Target : im reformed\n",
      "Predicted : im naked \n",
      "\n",
      "French(Source) : y atil qui que ce soit ici\n",
      "Target : is anyone here\n",
      "Predicted : is it here \n",
      "\n",
      "French(Source) : de the sil vous plait\n",
      "Target : tea please\n",
      "Predicted : please please \n",
      "\n",
      "French(Source) : tu sais qui je suis\n",
      "Target : do you know me\n",
      "Predicted : you can help \n",
      "\n",
      "French(Source) : tom sest exerce\n",
      "Target : tom exercised\n",
      "Predicted : tom exercised \n",
      "\n",
      "French(Source) : vous netes pas grosse\n",
      "Target : youre not fat\n",
      "Predicted : youre not fat \n",
      "\n",
      "French(Source) : tom est plus malin\n",
      "Target : tom is smarter\n",
      "Predicted : tom is grim \n",
      "\n",
      "French(Source) : elles sont mauvaises\n",
      "Target : theyre bad\n",
      "Predicted : theyre bad \n",
      "\n",
      "French(Source) : je suis respectueux\n",
      "Target : im observant\n",
      "Predicted : im committed \n",
      "\n",
      "French(Source) : je te remercie\n",
      "Target : thank you\n",
      "Predicted : thank thank it \n",
      "\n",
      "French(Source) : jecoute\n",
      "Target : im listening\n",
      "Predicted : im listening \n",
      "\n",
      "French(Source) : nous sommes amoureux\n",
      "Target : were in love\n",
      "Predicted : were in love \n",
      "\n",
      "French(Source) : pour sur\n",
      "Target : of course\n",
      "Predicted : of course \n",
      "\n",
      "French(Source) : pouvezvous faire cela\n",
      "Target : can you do that\n",
      "Predicted : can you do it \n",
      "\n",
      "French(Source) : je suis en train de conduire\n",
      "Target : im driving\n",
      "Predicted : im hurry \n",
      "\n",
      "French(Source) : je me sens perdue\n",
      "Target : i feel lost\n",
      "Predicted : i feel lost \n",
      "\n",
      "French(Source) : desormais je me sens mal\n",
      "Target : now i feel bad\n",
      "Predicted : i i feel \n",
      "\n",
      "French(Source) : prenez le commandement\n",
      "Target : take command\n",
      "Predicted : take command \n",
      "\n",
      "French(Source) : ils disposent de vin\n",
      "Target : they have wine\n",
      "Predicted : they have wine \n",
      "\n",
      "French(Source) : tom est special\n",
      "Target : tom is special\n",
      "Predicted : tom is \n",
      "\n",
      "French(Source) : cest vrai\n",
      "Target : its true\n",
      "Predicted : seriously it \n",
      "\n",
      "French(Source) : cest notre devoir\n",
      "Target : its our duty\n",
      "Predicted : its our fault \n",
      "\n",
      "French(Source) : il court\n",
      "Target : he runs\n",
      "Predicted : he runs \n",
      "\n",
      "French(Source) : donnemoi la moitie\n",
      "Target : give me half\n",
      "Predicted : give me half \n",
      "\n",
      "French(Source) : je ne malarme pas\n",
      "Target : im not alarmed\n",
      "Predicted : im not certain \n",
      "\n",
      "French(Source) : jai eu besoin dargent\n",
      "Target : i needed money\n",
      "Predicted : i need money \n",
      "\n",
      "French(Source) : etesvous au travail\n",
      "Target : are you at work\n",
      "Predicted : are you working \n",
      "\n",
      "French(Source) : tu as lair de tennuyer\n",
      "Target : you look bored\n",
      "Predicted : you look bored \n",
      "\n",
      "French(Source) : jai ete offense\n",
      "Target : i was offended\n",
      "Predicted : i was offended \n",
      "\n",
      "French(Source) : estelle medecin\n",
      "Target : is she a doctor\n",
      "Predicted : is me a doctor \n",
      "\n",
      "French(Source) : je suis choque\n",
      "Target : im shocked\n",
      "Predicted : im shocked \n",
      "\n",
      "French(Source) : aije besoin de continuer\n",
      "Target : need i go on\n",
      "Predicted : may i go \n",
      "\n",
      "French(Source) : aidezmoi tom\n",
      "Target : help me tom\n",
      "Predicted : call tom tom \n",
      "\n",
      "French(Source) : soyez prets\n",
      "Target : be prepared\n",
      "Predicted : be prepared \n",
      "\n",
      "French(Source) : jai promis a tom\n",
      "Target : i promised tom\n",
      "Predicted : i yelled tom \n",
      "\n",
      "French(Source) : je me suis debattue\n",
      "Target : i struggled\n",
      "Predicted : i recovered \n",
      "\n",
      "French(Source) : tu dois y aller\n",
      "Target : you have to go\n",
      "Predicted : you must go \n",
      "\n",
      "French(Source) : fiezvous juste a moi\n",
      "Target : just trust me\n",
      "Predicted : just trust me \n",
      "\n",
      "BLEU score: 0.279137\n"
     ]
    }
   ],
   "source": [
    "# Model-3 on the training sequences\n",
    "\n",
    "bleu_train_gru_lstm = model_evaluation(gru_lstm, eng_tokenizer, x_train, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French(Source) : faites comme il vous plaira\n",
      "Target : do as you like\n",
      "Predicted : do as do do \n",
      "\n",
      "French(Source) : nous partons a la retraite\n",
      "Target : were retiring\n",
      "Predicted : were retired \n",
      "\n",
      "French(Source) : tes givree\n",
      "Target : youre nuts\n",
      "Predicted : youre you \n",
      "\n",
      "French(Source) : apportez un appareil photo\n",
      "Target : bring a camera\n",
      "Predicted : bring a camera \n",
      "\n",
      "French(Source) : tu sembles contrariee\n",
      "Target : you seem upset\n",
      "Predicted : you seem upset \n",
      "\n",
      "French(Source) : peutetre viendratelle\n",
      "Target : she may come\n",
      "Predicted : maybe is is \n",
      "\n",
      "French(Source) : je dispose dun plan\n",
      "Target : i have a plan\n",
      "Predicted : i have a \n",
      "\n",
      "French(Source) : je dormis tout le jour\n",
      "Target : i slept all day\n",
      "Predicted : i the the \n",
      "\n",
      "French(Source) : jai vu cela\n",
      "Target : i saw that\n",
      "Predicted : i saw that \n",
      "\n",
      "French(Source) : tom sauta\n",
      "Target : tom jumped\n",
      "Predicted : tom joined \n",
      "\n",
      "French(Source) : pouvonsnous taider\n",
      "Target : can we help you\n",
      "Predicted : can we help \n",
      "\n",
      "French(Source) : voila cinq dollars\n",
      "Target : heres\n",
      "Predicted : heres the shut \n",
      "\n",
      "French(Source) : jai besoin dun couteau\n",
      "Target : i need a knife\n",
      "Predicted : i need a donut \n",
      "\n",
      "French(Source) : bonjour tout le monde\n",
      "Target : hello everyone\n",
      "Predicted : everyone farts \n",
      "\n",
      "French(Source) : la recree est terminee\n",
      "Target : recess ended\n",
      "Predicted : recess ended \n",
      "\n",
      "French(Source) : viens dehors\n",
      "Target : come outside\n",
      "Predicted : come over \n",
      "\n",
      "French(Source) : quand cela atil eu lieu\n",
      "Target : when was that\n",
      "Predicted : is it hurt \n",
      "\n",
      "French(Source) : cest le sien\n",
      "Target : its hers\n",
      "Predicted : thats is \n",
      "\n",
      "French(Source) : je nourris le chien\n",
      "Target : i fed the dog\n",
      "Predicted : i fed the dog \n",
      "\n",
      "French(Source) : jai ete contente\n",
      "Target : i was pleased\n",
      "Predicted : i was unlucky \n",
      "\n",
      "French(Source) : tom sest senti triste\n",
      "Target : tom felt sad\n",
      "Predicted : tom felt sad \n",
      "\n",
      "French(Source) : je ne savais pas\n",
      "Target : i didnt know\n",
      "Predicted : i not not \n",
      "\n",
      "French(Source) : tout est la\n",
      "Target : its all there\n",
      "Predicted : everyones is \n",
      "\n",
      "French(Source) : me faitesvous confiance\n",
      "Target : do you trust me\n",
      "Predicted : i lost lost \n",
      "\n",
      "French(Source) : vise plus haut\n",
      "Target : aim higher\n",
      "Predicted : schools is \n",
      "\n",
      "French(Source) : jai beaucoup rate\n",
      "Target : i missed a lot\n",
      "Predicted : i have a lot \n",
      "\n",
      "French(Source) : cest toi\n",
      "Target : is it you\n",
      "Predicted : thats it \n",
      "\n",
      "French(Source) : ce nest pas facile\n",
      "Target : its not easy\n",
      "Predicted : its not mine \n",
      "\n",
      "French(Source) : personne ne la su\n",
      "Target : no one knew it\n",
      "Predicted : no one likes \n",
      "\n",
      "French(Source) : regarde\n",
      "Target : have a look\n",
      "Predicted : look it \n",
      "\n",
      "French(Source) : montremoi\n",
      "Target : show me\n",
      "Predicted : bring it \n",
      "\n",
      "French(Source) : ce nest pas un menteur\n",
      "Target : hes not a liar\n",
      "Predicted : hes not a \n",
      "\n",
      "French(Source) : dites sil vous plait\n",
      "Target : say please\n",
      "Predicted : please please \n",
      "\n",
      "French(Source) : prendsle\n",
      "Target : pick him up\n",
      "Predicted : take it \n",
      "\n",
      "French(Source) : on est rentres maintenant\n",
      "Target : were home now\n",
      "Predicted : everyones all now \n",
      "\n",
      "French(Source) : jaime les maths\n",
      "Target : i like math\n",
      "Predicted : i like puzzles \n",
      "\n",
      "French(Source) : je men occuperai\n",
      "Target : ill handle it\n",
      "Predicted : i messed it \n",
      "\n",
      "French(Source) : arrete de bailler\n",
      "Target : stop yawning\n",
      "Predicted : stop yawning \n",
      "\n",
      "French(Source) : je suis bilingue\n",
      "Target : im bilingual\n",
      "Predicted : im exhausted \n",
      "\n",
      "French(Source) : soyez le bienvenu\n",
      "Target : welcome\n",
      "Predicted : welcome back \n",
      "\n",
      "French(Source) : je lai vu en premier\n",
      "Target : i saw it first\n",
      "Predicted : i saw him \n",
      "\n",
      "French(Source) : oubliezle\n",
      "Target : forget him\n",
      "Predicted : forget it \n",
      "\n",
      "French(Source) : je suis dans un endroit\n",
      "Target : i am in a spot\n",
      "Predicted : im a man man \n",
      "\n",
      "French(Source) : je suis quelquun de bien\n",
      "Target : im a nice guy\n",
      "Predicted : im married \n",
      "\n",
      "French(Source) : restez au lit\n",
      "Target : stay in bed\n",
      "Predicted : stay working bed \n",
      "\n",
      "French(Source) : tu en as fini\n",
      "Target : youre through\n",
      "Predicted : youre through \n",
      "\n",
      "French(Source) : reste alite\n",
      "Target : stay in bed\n",
      "Predicted : keep on smiling \n",
      "\n",
      "French(Source) : tu seras en securite\n",
      "Target : youll be safe\n",
      "Predicted : you safe \n",
      "\n",
      "French(Source) : comme elle est belle\n",
      "Target : how nice\n",
      "Predicted : how beautiful \n",
      "\n",
      "French(Source) : cest malhonnete\n",
      "Target : its dishonest\n",
      "Predicted : this is \n",
      "\n",
      "BLEU score: 0.105344\n"
     ]
    }
   ],
   "source": [
    "# Testing the Model-3 on the test sequences\n",
    "\n",
    "bleu_test_gru_lstm = model_evaluation(gru_lstm, eng_tokenizer, x_test, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4:  Encoder LSTM and Decoder GRU cell\n",
    "\n",
    "def lstm_gru(french_vocab, english_vocab, french_timesteps, english_timesteps, num_units):\n",
    "\tlearning_rate = 1e-2\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Embedding(french_vocab, num_units, input_length= french_timesteps, mask_zero=True))\n",
    "\tmodel.add(LSTM(num_units))\n",
    "\tmodel.add(RepeatVector(english_timesteps))\n",
    "\tmodel.add(GRU(num_units,return_sequences=True))\n",
    "\tmodel.add(TimeDistributed(Dense(english_vocab, activation='softmax')))\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Model-4\n",
    "\n",
    "learning_rate = 1e-2\n",
    "lstm_gru = lstm_gru(french_vocabulary_size, english_vocabulary_size, french_maximum_length, english_maximum_length, 256)\n",
    "lstm_gru.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 10, 256)           1472768   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_3 (RepeatVecto (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 5, 256)            394752    \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 5, 2776)           713432    \n",
      "=================================================================\n",
      "Total params: 3,106,264\n",
      "Trainable params: 3,106,264\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "# Summary of Model-4\n",
    "\n",
    "print(lstm_gru.summary())\n",
    "plot_model(lstm_gru, to_file='lstm_gru.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "165/165 - 16s - loss: 3.6424 - categorical_accuracy: 0.4711 - val_loss: 3.2285 - val_categorical_accuracy: 0.5009\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.22854, saving model to lstm_gru.h5\n",
      "Epoch 2/30\n",
      "165/165 - 11s - loss: 2.8447 - categorical_accuracy: 0.5371 - val_loss: 2.7890 - val_categorical_accuracy: 0.5588\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.22854 to 2.78902, saving model to lstm_gru.h5\n",
      "Epoch 3/30\n",
      "165/165 - 12s - loss: 2.3612 - categorical_accuracy: 0.5900 - val_loss: 2.5039 - val_categorical_accuracy: 0.5945\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.78902 to 2.50387, saving model to lstm_gru.h5\n",
      "Epoch 4/30\n",
      "165/165 - 12s - loss: 1.9692 - categorical_accuracy: 0.6299 - val_loss: 2.3614 - val_categorical_accuracy: 0.6141\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.50387 to 2.36140, saving model to lstm_gru.h5\n",
      "Epoch 5/30\n",
      "165/165 - 12s - loss: 1.6437 - categorical_accuracy: 0.6668 - val_loss: 2.2189 - val_categorical_accuracy: 0.6277\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.36140 to 2.21893, saving model to lstm_gru.h5\n",
      "Epoch 6/30\n",
      "165/165 - 12s - loss: 1.3611 - categorical_accuracy: 0.7020 - val_loss: 2.1424 - val_categorical_accuracy: 0.6415\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.21893 to 2.14236, saving model to lstm_gru.h5\n",
      "Epoch 7/30\n",
      "165/165 - 11s - loss: 1.1212 - categorical_accuracy: 0.7385 - val_loss: 2.1138 - val_categorical_accuracy: 0.6555\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.14236 to 2.11378, saving model to lstm_gru.h5\n",
      "Epoch 8/30\n",
      "165/165 - 11s - loss: 0.9287 - categorical_accuracy: 0.7703 - val_loss: 2.0870 - val_categorical_accuracy: 0.6651\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.11378 to 2.08704, saving model to lstm_gru.h5\n",
      "Epoch 9/30\n",
      "165/165 - 12s - loss: 0.7688 - categorical_accuracy: 0.8018 - val_loss: 2.1064 - val_categorical_accuracy: 0.6725\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.08704\n",
      "Epoch 10/30\n",
      "165/165 - 12s - loss: 0.6420 - categorical_accuracy: 0.8282 - val_loss: 2.1348 - val_categorical_accuracy: 0.6771\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.08704\n",
      "Epoch 11/30\n",
      "165/165 - 12s - loss: 0.5464 - categorical_accuracy: 0.8516 - val_loss: 2.1031 - val_categorical_accuracy: 0.6831\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.08704\n",
      "Epoch 12/30\n",
      "165/165 - 12s - loss: 0.4780 - categorical_accuracy: 0.8686 - val_loss: 2.1417 - val_categorical_accuracy: 0.6771\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.08704\n",
      "Epoch 13/30\n",
      "165/165 - 12s - loss: 0.4289 - categorical_accuracy: 0.8807 - val_loss: 2.1639 - val_categorical_accuracy: 0.6829\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.08704\n",
      "Epoch 14/30\n",
      "165/165 - 12s - loss: 0.3858 - categorical_accuracy: 0.8916 - val_loss: 2.2169 - val_categorical_accuracy: 0.6772\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.08704\n",
      "Epoch 15/30\n",
      "165/165 - 12s - loss: 0.3638 - categorical_accuracy: 0.8964 - val_loss: 2.2333 - val_categorical_accuracy: 0.6777\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 2.08704\n",
      "Epoch 16/30\n",
      "165/165 - 13s - loss: 0.3435 - categorical_accuracy: 0.9011 - val_loss: 2.2612 - val_categorical_accuracy: 0.6765\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 2.08704\n",
      "Epoch 17/30\n",
      "165/165 - 14s - loss: 0.3384 - categorical_accuracy: 0.9034 - val_loss: 2.2553 - val_categorical_accuracy: 0.6749\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 2.08704\n",
      "Epoch 18/30\n",
      "165/165 - 12s - loss: 0.3266 - categorical_accuracy: 0.9046 - val_loss: 2.2861 - val_categorical_accuracy: 0.6835\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 2.08704\n",
      "Epoch 19/30\n",
      "165/165 - 12s - loss: 0.3183 - categorical_accuracy: 0.9073 - val_loss: 2.3161 - val_categorical_accuracy: 0.6824\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 2.08704\n",
      "Epoch 20/30\n",
      "165/165 - 13s - loss: 0.3093 - categorical_accuracy: 0.9097 - val_loss: 2.3373 - val_categorical_accuracy: 0.6743\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 2.08704\n",
      "Epoch 21/30\n",
      "165/165 - 12s - loss: 0.3134 - categorical_accuracy: 0.9060 - val_loss: 2.3491 - val_categorical_accuracy: 0.6771\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 2.08704\n",
      "Epoch 22/30\n",
      "165/165 - 14s - loss: 0.3160 - categorical_accuracy: 0.9060 - val_loss: 2.3842 - val_categorical_accuracy: 0.6739\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 2.08704\n",
      "Epoch 23/30\n",
      "165/165 - 13s - loss: 0.3192 - categorical_accuracy: 0.9050 - val_loss: 2.4275 - val_categorical_accuracy: 0.6736\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 2.08704\n",
      "Epoch 24/30\n",
      "165/165 - 13s - loss: 0.3275 - categorical_accuracy: 0.9019 - val_loss: 2.4586 - val_categorical_accuracy: 0.6731\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 2.08704\n",
      "Epoch 25/30\n",
      "165/165 - 13s - loss: 0.3587 - categorical_accuracy: 0.8947 - val_loss: 2.4677 - val_categorical_accuracy: 0.6725\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 2.08704\n",
      "Epoch 26/30\n",
      "165/165 - 12s - loss: 0.3646 - categorical_accuracy: 0.8920 - val_loss: 2.4323 - val_categorical_accuracy: 0.6744\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 2.08704\n",
      "Epoch 27/30\n",
      "165/165 - 12s - loss: 0.3415 - categorical_accuracy: 0.8975 - val_loss: 2.4777 - val_categorical_accuracy: 0.6756\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 2.08704\n",
      "Epoch 28/30\n",
      "165/165 - 12s - loss: 0.3246 - categorical_accuracy: 0.9018 - val_loss: 2.4944 - val_categorical_accuracy: 0.6729\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 2.08704\n",
      "Epoch 29/30\n",
      "165/165 - 12s - loss: 0.2926 - categorical_accuracy: 0.9105 - val_loss: 2.5266 - val_categorical_accuracy: 0.6692\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 2.08704\n",
      "Epoch 30/30\n",
      "165/165 - 12s - loss: 0.2857 - categorical_accuracy: 0.9128 - val_loss: 2.5332 - val_categorical_accuracy: 0.6681\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 2.08704\n"
     ]
    }
   ],
   "source": [
    "# Fitting the Model-4\n",
    "\n",
    "filename = 'lstm_gru.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "history_lstm_gru = lstm_gru.fit(x_train, y_train, epochs=30, batch_size=64, validation_data=(x_valid, y_valid), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        loss  categorical_accuracy  val_loss  val_categorical_accuracy\n",
      "0   3.612989              0.472171  3.123174                  0.506133\n",
      "1   2.711535              0.549467  2.640196                  0.571200\n",
      "2   2.177814              0.607276  2.416023                  0.597200\n",
      "3   1.810572              0.643924  2.256093                  0.621467\n",
      "4   1.500825              0.681695  2.188692                  0.632000\n",
      "5   1.231123              0.718381  2.143722                  0.641067\n",
      "6   1.018034              0.752762  2.159810                  0.652133\n",
      "7   0.829502              0.788076  2.141968                  0.662000\n",
      "8   0.675615              0.819867  2.167118                  0.660533\n",
      "9   0.578506              0.843371  2.204639                  0.669067\n",
      "10  0.491479              0.863543  2.238677                  0.667733\n",
      "11  0.441413              0.876667  2.284345                  0.672133\n",
      "12  0.387935              0.890076  2.273862                  0.670267\n",
      "13  0.356470              0.897390  2.326172                  0.669733\n",
      "14  0.329899              0.904400  2.353433                  0.668533\n",
      "15  0.304421              0.911181  2.359426                  0.679733\n",
      "16  0.289416              0.914971  2.414748                  0.670533\n",
      "17  0.270090              0.920743  2.403123                  0.672933\n",
      "18  0.268648              0.918990  2.432450                  0.671467\n",
      "19  0.256100              0.923467  2.432874                  0.668133\n",
      "20  0.265997              0.919238  2.485591                  0.667600\n",
      "21  0.258273              0.919733  2.473088                  0.671333\n",
      "22  0.252185              0.922914  2.509631                  0.667333\n",
      "23  0.243921              0.925695  2.552095                  0.670000\n",
      "24  0.257793              0.921181  2.531067                  0.667200\n",
      "25  0.255921              0.921657  2.566408                  0.668800\n",
      "26  0.257300              0.919848  2.594534                  0.672667\n",
      "27  0.260570              0.918895  2.598118                  0.665733\n",
      "28  0.285786              0.911771  2.608110                  0.665733\n",
      "29  0.313751              0.902781  2.638984                  0.659733\n"
     ]
    }
   ],
   "source": [
    "# Saving the history of the Model-4\n",
    "\n",
    "df_history_lstm_gru = pd.DataFrame(history_both_lstm.history)\n",
    "df_history_lstm_gru.to_csv('df_history_lstm_gru.csv')\n",
    "print(df_history_lstm_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Model-4\n",
    "lstm_gru = load_model('lstm_gru.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French(Source) : je me suis marre\n",
      "Target : i had some fun\n",
      "Predicted : i had for fun \n",
      "\n",
      "French(Source) : laissezmoi sortir\n",
      "Target : let me out\n",
      "Predicted : let me out \n",
      "\n",
      "French(Source) : on peut me faire confiance\n",
      "Target : im trustworthy\n",
      "Predicted : im trusted sorry \n",
      "\n",
      "French(Source) : quelquun estil la\n",
      "Target : anybody home\n",
      "Predicted : is anybody home \n",
      "\n",
      "French(Source) : mettezle la\n",
      "Target : put it there\n",
      "Predicted : put it there \n",
      "\n",
      "French(Source) : vous etes sournois\n",
      "Target : youre sneaky\n",
      "Predicted : youre sneaky \n",
      "\n",
      "French(Source) : ne vous precipitez pas\n",
      "Target : dont rush\n",
      "Predicted : dont rush \n",
      "\n",
      "French(Source) : degage\n",
      "Target : go away\n",
      "Predicted : go fly a \n",
      "\n",
      "French(Source) : elle laide\n",
      "Target : she helps him\n",
      "Predicted : she helps him \n",
      "\n",
      "French(Source) : je ny parviens pas\n",
      "Target : i cant do it\n",
      "Predicted : i cant buy it \n",
      "\n",
      "French(Source) : jai la priorite sur vous\n",
      "Target : i outrank you\n",
      "Predicted : i outrank you \n",
      "\n",
      "French(Source) : je men suis doute\n",
      "Target : i thought so\n",
      "Predicted : i went it \n",
      "\n",
      "French(Source) : je me suis amendee\n",
      "Target : im reformed\n",
      "Predicted : im reformed \n",
      "\n",
      "French(Source) : y atil qui que ce soit ici\n",
      "Target : is anyone here\n",
      "Predicted : anybody anybody here \n",
      "\n",
      "French(Source) : de the sil vous plait\n",
      "Target : tea please\n",
      "Predicted : please tea \n",
      "\n",
      "French(Source) : tu sais qui je suis\n",
      "Target : do you know me\n",
      "Predicted : you you you \n",
      "\n",
      "French(Source) : tom sest exerce\n",
      "Target : tom exercised\n",
      "Predicted : tom rested \n",
      "\n",
      "French(Source) : vous netes pas grosse\n",
      "Target : youre not fat\n",
      "Predicted : youre not fat \n",
      "\n",
      "French(Source) : tom est plus malin\n",
      "Target : tom is smarter\n",
      "Predicted : tom is smarter \n",
      "\n",
      "French(Source) : elles sont mauvaises\n",
      "Target : theyre bad\n",
      "Predicted : theyre bad \n",
      "\n",
      "French(Source) : je suis respectueux\n",
      "Target : im observant\n",
      "Predicted : im observant \n",
      "\n",
      "French(Source) : je te remercie\n",
      "Target : thank you\n",
      "Predicted : thank thank you \n",
      "\n",
      "French(Source) : jecoute\n",
      "Target : im listening\n",
      "Predicted : im lying \n",
      "\n",
      "French(Source) : nous sommes amoureux\n",
      "Target : were in love\n",
      "Predicted : were in love \n",
      "\n",
      "French(Source) : pour sur\n",
      "Target : of course\n",
      "Predicted : of course \n",
      "\n",
      "French(Source) : pouvezvous faire cela\n",
      "Target : can you do that\n",
      "Predicted : can you do that \n",
      "\n",
      "French(Source) : je suis en train de conduire\n",
      "Target : im driving\n",
      "Predicted : im driving \n",
      "\n",
      "French(Source) : je me sens perdue\n",
      "Target : i feel lost\n",
      "Predicted : i feeling low \n",
      "\n",
      "French(Source) : desormais je me sens mal\n",
      "Target : now i feel bad\n",
      "Predicted : i me bad \n",
      "\n",
      "French(Source) : prenez le commandement\n",
      "Target : take command\n",
      "Predicted : take mine \n",
      "\n",
      "French(Source) : ils disposent de vin\n",
      "Target : they have wine\n",
      "Predicted : they have wine \n",
      "\n",
      "French(Source) : tom est special\n",
      "Target : tom is special\n",
      "Predicted : tom is \n",
      "\n",
      "French(Source) : cest vrai\n",
      "Target : its true\n",
      "Predicted : its is \n",
      "\n",
      "French(Source) : cest notre devoir\n",
      "Target : its our duty\n",
      "Predicted : its our duty \n",
      "\n",
      "French(Source) : il court\n",
      "Target : he runs\n",
      "Predicted : he is running \n",
      "\n",
      "French(Source) : donnemoi la moitie\n",
      "Target : give me half\n",
      "Predicted : give me half \n",
      "\n",
      "French(Source) : je ne malarme pas\n",
      "Target : im not alarmed\n",
      "Predicted : im not not \n",
      "\n",
      "French(Source) : jai eu besoin dargent\n",
      "Target : i needed money\n",
      "Predicted : i needed money \n",
      "\n",
      "French(Source) : etesvous au travail\n",
      "Target : are you at work\n",
      "Predicted : are you work work \n",
      "\n",
      "French(Source) : tu as lair de tennuyer\n",
      "Target : you look bored\n",
      "Predicted : you look bored \n",
      "\n",
      "French(Source) : jai ete offense\n",
      "Target : i was offended\n",
      "Predicted : i was offended \n",
      "\n",
      "French(Source) : estelle medecin\n",
      "Target : is she a doctor\n",
      "Predicted : is she a doctor \n",
      "\n",
      "French(Source) : je suis choque\n",
      "Target : im shocked\n",
      "Predicted : im shocked \n",
      "\n",
      "French(Source) : aije besoin de continuer\n",
      "Target : need i go on\n",
      "Predicted : let i go \n",
      "\n",
      "French(Source) : aidezmoi tom\n",
      "Target : help me tom\n",
      "Predicted : help me back \n",
      "\n",
      "French(Source) : soyez prets\n",
      "Target : be prepared\n",
      "Predicted : be prepared \n",
      "\n",
      "French(Source) : jai promis a tom\n",
      "Target : i promised tom\n",
      "Predicted : i warned tom \n",
      "\n",
      "French(Source) : je me suis debattue\n",
      "Target : i struggled\n",
      "Predicted : i cringed \n",
      "\n",
      "French(Source) : tu dois y aller\n",
      "Target : you have to go\n",
      "Predicted : you must go go \n",
      "\n",
      "French(Source) : fiezvous juste a moi\n",
      "Target : just trust me\n",
      "Predicted : just me me me \n",
      "\n",
      "BLEU score: 0.261796\n"
     ]
    }
   ],
   "source": [
    "# Model-4 on training sequences\n",
    "\n",
    "bleu_train_lstm_gru = model_evaluation(lstm_gru, eng_tokenizer, x_train, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French(Source) : faites comme il vous plaira\n",
      "Target : do as you like\n",
      "Predicted : do as you can \n",
      "\n",
      "French(Source) : nous partons a la retraite\n",
      "Target : were retiring\n",
      "Predicted : stay starving \n",
      "\n",
      "French(Source) : tes givree\n",
      "Target : youre nuts\n",
      "Predicted : youre you \n",
      "\n",
      "French(Source) : apportez un appareil photo\n",
      "Target : bring a camera\n",
      "Predicted : call a book \n",
      "\n",
      "French(Source) : tu sembles contrariee\n",
      "Target : you seem upset\n",
      "Predicted : you seem upset \n",
      "\n",
      "French(Source) : peutetre viendratelle\n",
      "Target : she may come\n",
      "Predicted : what liked \n",
      "\n",
      "French(Source) : je dispose dun plan\n",
      "Target : i have a plan\n",
      "Predicted : i have a plan \n",
      "\n",
      "French(Source) : je dormis tout le jour\n",
      "Target : i slept all day\n",
      "Predicted : i all every town \n",
      "\n",
      "French(Source) : jai vu cela\n",
      "Target : i saw that\n",
      "Predicted : i had that \n",
      "\n",
      "French(Source) : tom sauta\n",
      "Target : tom jumped\n",
      "Predicted : tom is \n",
      "\n",
      "French(Source) : pouvonsnous taider\n",
      "Target : can we help you\n",
      "Predicted : can we help \n",
      "\n",
      "French(Source) : voila cinq dollars\n",
      "Target : heres\n",
      "Predicted : hows the those \n",
      "\n",
      "French(Source) : jai besoin dun couteau\n",
      "Target : i need a knife\n",
      "Predicted : i need a map \n",
      "\n",
      "French(Source) : bonjour tout le monde\n",
      "Target : hello everyone\n",
      "Predicted : everyone everyone \n",
      "\n",
      "French(Source) : la recree est terminee\n",
      "Target : recess ended\n",
      "Predicted : recess ended \n",
      "\n",
      "French(Source) : viens dehors\n",
      "Target : come outside\n",
      "Predicted : come outside on \n",
      "\n",
      "French(Source) : quand cela atil eu lieu\n",
      "Target : when was that\n",
      "Predicted : is my night \n",
      "\n",
      "French(Source) : cest le sien\n",
      "Target : its hers\n",
      "Predicted : thats hers \n",
      "\n",
      "French(Source) : je nourris le chien\n",
      "Target : i fed the dog\n",
      "Predicted : i have your dog \n",
      "\n",
      "French(Source) : jai ete contente\n",
      "Target : i was pleased\n",
      "Predicted : i was got \n",
      "\n",
      "French(Source) : tom sest senti triste\n",
      "Target : tom felt sad\n",
      "Predicted : tom felt sad \n",
      "\n",
      "French(Source) : je ne savais pas\n",
      "Target : i didnt know\n",
      "Predicted : i cant say it \n",
      "\n",
      "French(Source) : tout est la\n",
      "Target : its all there\n",
      "Predicted : all all here \n",
      "\n",
      "French(Source) : me faitesvous confiance\n",
      "Target : do you trust me\n",
      "Predicted : yes trust me \n",
      "\n",
      "French(Source) : vise plus haut\n",
      "Target : aim higher\n",
      "Predicted : aim up \n",
      "\n",
      "French(Source) : jai beaucoup rate\n",
      "Target : i missed a lot\n",
      "Predicted : i made a \n",
      "\n",
      "French(Source) : cest toi\n",
      "Target : is it you\n",
      "Predicted : its you \n",
      "\n",
      "French(Source) : ce nest pas facile\n",
      "Target : its not easy\n",
      "Predicted : its not mine \n",
      "\n",
      "French(Source) : personne ne la su\n",
      "Target : no one knew it\n",
      "Predicted : no one it \n",
      "\n",
      "French(Source) : regarde\n",
      "Target : have a look\n",
      "Predicted : look look \n",
      "\n",
      "French(Source) : montremoi\n",
      "Target : show me\n",
      "Predicted : take it \n",
      "\n",
      "French(Source) : ce nest pas un menteur\n",
      "Target : hes not a liar\n",
      "Predicted : its not a \n",
      "\n",
      "French(Source) : dites sil vous plait\n",
      "Target : say please\n",
      "Predicted : listen please \n",
      "\n",
      "French(Source) : prendsle\n",
      "Target : pick him up\n",
      "Predicted : take it \n",
      "\n",
      "French(Source) : on est rentres maintenant\n",
      "Target : were home now\n",
      "Predicted : its all mad \n",
      "\n",
      "French(Source) : jaime les maths\n",
      "Target : i like math\n",
      "Predicted : i like blue \n",
      "\n",
      "French(Source) : je men occuperai\n",
      "Target : ill handle it\n",
      "Predicted : i play \n",
      "\n",
      "French(Source) : arrete de bailler\n",
      "Target : stop yawning\n",
      "Predicted : stop stop \n",
      "\n",
      "French(Source) : je suis bilingue\n",
      "Target : im bilingual\n",
      "Predicted : im sure \n",
      "\n",
      "French(Source) : soyez le bienvenu\n",
      "Target : welcome\n",
      "Predicted : be on \n",
      "\n",
      "French(Source) : je lai vu en premier\n",
      "Target : i saw it first\n",
      "Predicted : i saw them go \n",
      "\n",
      "French(Source) : oubliezle\n",
      "Target : forget him\n",
      "Predicted : forget it \n",
      "\n",
      "French(Source) : je suis dans un endroit\n",
      "Target : i am in a spot\n",
      "Predicted : im a a guy \n",
      "\n",
      "French(Source) : je suis quelquun de bien\n",
      "Target : im a nice guy\n",
      "Predicted : im all good \n",
      "\n",
      "French(Source) : restez au lit\n",
      "Target : stay in bed\n",
      "Predicted : go in bed \n",
      "\n",
      "French(Source) : tu en as fini\n",
      "Target : youre through\n",
      "Predicted : youre through \n",
      "\n",
      "French(Source) : reste alite\n",
      "Target : stay in bed\n",
      "Predicted : stay in bed \n",
      "\n",
      "French(Source) : tu seras en securite\n",
      "Target : youll be safe\n",
      "Predicted : youre safe \n",
      "\n",
      "French(Source) : comme elle est belle\n",
      "Target : how nice\n",
      "Predicted : how a \n",
      "\n",
      "French(Source) : cest malhonnete\n",
      "Target : its dishonest\n",
      "Predicted : its is \n",
      "\n",
      "BLEU score: 0.116645\n"
     ]
    }
   ],
   "source": [
    "# Testing Model-4 on test sequences\n",
    "\n",
    "bleu_test_lstm_gru = model_evaluation(lstm_gru, eng_tokenizer, x_test, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
