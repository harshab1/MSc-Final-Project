{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import re\n",
    "import pickle \n",
    "from unicodedata import normalize\n",
    "import numpy\n",
    "from numpy import array\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "from numpy import argmax\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN,GRU,LSTM\n",
    "from keras.layers import Dense, Embedding, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Archive:  fra-eng.zip',\n",
       " 'replace _about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL',\n",
       " '(EOF or read error, treating as \"[N]one\" ...)']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetching and decompressing the dataset \n",
    "!!curl -O http://www.manythings.org/anki/fra-eng.zip\n",
    "!!unzip fra-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function performs two tasks:\n",
    "\n",
    "# 1. Loading the text data preserving the Unicode french characters and then\n",
    "#Â 2. Each line of the text file contain English sentence and its French translation seperated by tab character. \n",
    "\n",
    "def loading_and_pairs(file):\n",
    "    \n",
    "\t# opening the text file in read only mode with unicode encoding\n",
    "\tf = open(file = file, mode = 'rt', encoding = 'utf-8')\n",
    "    \n",
    "\t# reading the text from the opened file\n",
    "\ttext = f.read()\n",
    "    \n",
    "\t# finally closing the file\n",
    "\tf.close()\n",
    "    \n",
    "    # Obtaining each line in the file\n",
    "\tl = text.strip().split('\\n')\n",
    "    \n",
    "    # Obtaining the pairs of English sentence and its french translation\n",
    "\tpairs = [l_each.split('\\t') for l_each in  l]\n",
    "    \n",
    "\treturn pairs   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the lines by removing all the non-printable characters, punctuation characters\n",
    "# Given a list of lines cleaning them\n",
    "\n",
    "def pairs_clean(lines_from_text):\n",
    "    \n",
    "\tnew_cleaned = list()\n",
    "    \n",
    "\t# using regular expression for removing non-printable characters\n",
    "\tregular_non_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    \n",
    "\t# using regular expression for removing punctuation characters and obtaining translation table\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "\tfor pair in lines_from_text:\n",
    "        \n",
    "\t\tclean_pair = list()\n",
    "\t\tfor new_line in pair:\n",
    "            \n",
    "\t\t\t# normalization to remove canonical and compatibility related issues\n",
    "\t\t\tnew_line = normalize('NFD', new_line).encode('ascii', 'ignore')\n",
    "\t\t\tnew_line = new_line.decode('UTF-8')\n",
    "            \n",
    "\t\t\t# tokenizing the white space\n",
    "\t\t\tnew_line = new_line.split()\n",
    "            \n",
    "\t\t\t# normalizing the text to lowercase\n",
    "\t\t\tnew_line = [word.lower() for word in new_line]\n",
    "            \n",
    "\t\t\t# removing punctuation from each token using regular expression table \n",
    "\t\t\tnew_line = [word.translate(table) for word in new_line]\n",
    "            \n",
    "\t\t\t# removing non-printable characters using the above regular expression\n",
    "\t\t\tnew_line = [regular_non_print.sub('', w) for w in new_line]\n",
    "            \n",
    "\t\t\t# removing the non-alphabetic tokens such as numbers\n",
    "\t\t\tnew_line = [word for word in new_line if word.isalpha()]\n",
    "            \n",
    "\t\t\t# store as string\n",
    "\t\t\tclean_pair.append(' '.join(new_line))\n",
    "\t\tnew_cleaned.append(clean_pair)\n",
    "\treturn array(new_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the list of clean sentences to file\n",
    "def saving_data(sentences, file):\n",
    "\tpickle.dump( sentences, open(file = file, mode = 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the text dataset\n",
    "text_file = 'fra.txt'\n",
    "text_pairs = loading_and_pairs(text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the sentences\n",
    "cleaned_pairs = pairs_clean(text_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the cleaned pairs to file\n",
    "saving_data(cleaned_pairs, 'eng-fre.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There a total of 190206 pairs of tranlations in the dataset\n"
     ]
    }
   ],
   "source": [
    "# Total number of translation sentences\n",
    "print(\"There a total of {} pairs of tranlations in the dataset\".format(cleaned_pairs.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English : i wish that i couldve eaten at that restaurant with you\n",
      "French : jaurais aime pouvoir manger dans ce restaurant avec toi \n",
      "\n",
      "English : this is really hard\n",
      "French : cest tres dur \n",
      "\n",
      "English : he has two cats\n",
      "French : il a deux chats \n",
      "\n",
      "English : i didnt break it\n",
      "French : je ne lai pas rompu \n",
      "\n",
      "English : i never thought id see your face again\n",
      "French : je nai jamais pense que je reverrais votre visage \n",
      "\n",
      "English : apples are cheap today\n",
      "French : les pommes sont bon marche aujourdhui \n",
      "\n",
      "English : my brother is now in australia\n",
      "French : mon frere est maintenant en australie \n",
      "\n",
      "English : my cat loves toys\n",
      "French : mon chat adore les jouets \n",
      "\n",
      "English : tom is fighting cancer\n",
      "French : tom combat le cancer \n",
      "\n",
      "English : im pretty sure that it wont snow today\n",
      "French : je suis assez certaine quil ne neigera pas aujourdhui \n",
      "\n",
      "English : he confessed his crime\n",
      "French : il a avoue son crime \n",
      "\n",
      "English : i stole the idea\n",
      "French : jai vole lidee \n",
      "\n",
      "English : i was the last to know\n",
      "French : jetais le dernier a savoir \n",
      "\n",
      "English : call your daughter\n",
      "French : appelle ta fille \n",
      "\n",
      "English : the only thing we have to fear is fear itself\n",
      "French : la seule chose a craindre cest la crainte ellememe \n",
      "\n",
      "English : what you have just said reminds me of an old saying\n",
      "French : ce que tu as dit me rappelle un vieux proverbe \n",
      "\n",
      "English : ill lend you the tools that you need to do that\n",
      "French : je te preterai les outils dont tu as besoin pour faire cela \n",
      "\n",
      "English : why are you wearing a tux\n",
      "French : pourquoi portestu un smoking \n",
      "\n",
      "English : tom paid cash\n",
      "French : tom a paye en liquide \n",
      "\n",
      "English : i can deliver that to tom\n",
      "French : je peux livrer cela a tom \n",
      "\n",
      "English : get tom to help you\n",
      "French : convaincs tom de taider \n",
      "\n",
      "English : some of them are teachers\n",
      "French : certains dentre eux sont enseignants \n",
      "\n",
      "English : tom said he didnt like chocolate ice cream\n",
      "French : tom a dit quil naimait pas la glace au chocolat \n",
      "\n",
      "English : tom doesnt know where mary wants to spend her summer vacation\n",
      "French : tom ne sait pas ou marie a envie de passer ses vacances cet ete \n",
      "\n",
      "English : could we make this a priority\n",
      "French : pourrionsnous en faire une priorite \n",
      "\n",
      "English : school begins at half past eight\n",
      "French : lecole commence a huit heures et demie \n",
      "\n",
      "English : where shall we begin\n",
      "French : ou devonsnous commencer \n",
      "\n",
      "English : his father left him the house in his will\n",
      "French : son pere lui legua la maison dans son testament \n",
      "\n",
      "English : im studying french\n",
      "French : jetudie le francais \n",
      "\n",
      "English : tom didnt think his bosss plan was a viable one\n",
      "French : tom ne pensait pas que le plan de son patron etait viable \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Looking at few sentences\n",
    "\n",
    "random_sample_list = random.sample(range(0, cleaned_pairs.shape[0]), 30)\n",
    "for i in random_sample_list:\n",
    "\tprint('English : %s\\nFrench : %s \\n' % (cleaned_pairs[i,0], cleaned_pairs[i,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from pickle import dump\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the previously cleaned data \n",
    "def loading_sentences(filename):\n",
    "\treturn pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the raw dataset\n",
    "text_dataset = loading_sentences('eng-fre.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are over 190000 pairs of sentences\n",
    "# It will take long time for training and testing the model\n",
    "# Hence dataset size is reduced\n",
    "num_sentences = 25000 #clean_pairs.shape[0]\n",
    "reduced_dataset_25000 = text_dataset[:num_sentences, :]\n",
    "train_size = numpy.rint(0.7 * num_sentences)\n",
    "validation_size = numpy.rint(0.1 * num_sentences)\n",
    "test_size = numpy.rint(0.2 * num_sentences)\n",
    "\n",
    "# randomly shuffling the dataset\n",
    "shuffle(reduced_dataset_25000)\n",
    "\n",
    "# spliting the reduced dataset into train, validation and test\n",
    "split_1 = int(train_size)\n",
    "split_2 = int(train_size+validation_size)\n",
    "split_3 = int(train_size+validation_size+test_size)\n",
    "train, validation, test = reduced_dataset_25000[:split_1], reduced_dataset_25000[split_1:split_2], reduced_dataset_25000[split_2:split_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17500, 3), (2500, 3), (5000, 3))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, validation.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the reduced dataset to training, validation and testing data\n",
    "\n",
    "saving_data(text_dataset, 'eng-fre-total.pkl')\n",
    "saving_data(train, 'eng-fre-train.pkl')\n",
    "saving_data(validation, 'eng-fre-validation.pkl')\n",
    "saving_data(test, 'eng-fre-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using keras tokenize class, for mapping the words to integers needed for modeling\n",
    "\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the maximum sentence length from the list of phrases \n",
    "\n",
    "def max_length(input_lines):\n",
    "\treturn max(len(each_line.split()) for each_line in input_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding and padding the input and output sequences\n",
    "\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "\t# Each input and output sequence are  encoded to integers\n",
    "\ttext_encoded_integers = tokenizer.texts_to_sequences(lines)\n",
    "\t# Obtained sequences are padded with 0 values at the end to make their lenght as maxmim phrase length\n",
    "\tpadding_sequences = pad_sequences(text_encoded_integers, maxlen=length, padding='post')\n",
    "\treturn padding_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output target sequences (English sentences) has to be one hot encoded as the model will \n",
    "# predicts probability of each word in the vocabulary as output. \n",
    "\n",
    "def encode_output(output_sequences, vocabulary_size):\n",
    "\toutput_list = list()\n",
    "\tfor output_sequence in output_sequences:\n",
    "\t\tcat = to_categorical(output_sequence, num_classes=vocabulary_size)\n",
    "\t\toutput_list.append(cat)\n",
    "\tencoded_output = array(output_list)\n",
    "\tencoded_output = encoded_output.reshape(output_sequences.shape[0], output_sequences.shape[1], vocabulary_size)\n",
    "\treturn encoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse mapping an predicted sequence of integers to a words by looking up tokenizer\n",
    "\n",
    "def reverse_mapping(output_integer, tokenizer):\n",
    "\tfor w, i in tokenizer.word_index.items():\n",
    "\t\tif i == output_integer:\n",
    "\t\t\treturn w\n",
    "\treturn None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping the sequence of integers for generating string of words\n",
    "\n",
    "def predict_sequence(rnn_model, tokenizer, original_data):\n",
    "\tprediction = rnn_model.predict(original_data, verbose=0)[0]\n",
    "\tintegers = [argmax(vector) for vector in prediction]\n",
    "\ttarget_list = list()\n",
    "\tfor i in integers:\n",
    "\t\tword = reverse_mapping(i, tokenizer)\n",
    "\t\tif word is None:\n",
    "\t\t\tbreak\n",
    "\t\ttarget_list.append(word)\n",
    "\treturn ' '.join(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the performance of each model using BLEU score by comparing predicted result to original/expected sequences \n",
    "\n",
    "\n",
    "def model_evaluation(model_name, tokenizer_used, sources_text, raw_text_dataset):\n",
    "\tactual_value, predicted_value = list(), list()\n",
    "\tbleu_scores = []\n",
    "\tfor i, j in enumerate(sources_text):\n",
    "\t\t# translating the encoded input text sequence\n",
    "\t\tj = j.reshape((1, j.shape[0]))\n",
    "\t\ttranslation = predict_sequence(model_name, eng_tokenizer, j)\n",
    "\t\traw_target, raw_src = raw_text_dataset[i][0], raw_text_dataset[i][1]\n",
    "        \n",
    "        # Printing 50 French to English translations by the model\n",
    "        \n",
    "\t\tif i < 50:\n",
    "\t\t\tprint('French(Source) : %s\\nTarget : %s\\nPredicted : %s \\n' % (raw_src, raw_target, translation))\n",
    "        \n",
    "        \n",
    "\t\tactual_value.append([raw_target.split()])\n",
    "\t\tpredicted_value.append(translation.split())\n",
    "        \n",
    "    # Calculating BLEU score\n",
    "\tbleu_score = corpus_bleu(actual_value, predicted_value, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    \n",
    "\t# Print BLEU score\n",
    "\tprint('BLEU score: %f' % bleu_score)\n",
    "    \n",
    "\treturn bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the total, train, validation and test datasets \n",
    "dataset = loading_sentences('eng-fre-total.pkl')\n",
    "train = loading_sentences('eng-fre-train.pkl')\n",
    "valid = loading_sentences('eng-fre-validation.pkl')\n",
    "test = loading_sentences('eng-fre-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(190206, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_dataset_25000.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the French tokenizer\n",
    "french_tokenizer = create_tokenizer(reduced_dataset_25000[:, 1])\n",
    "french_maximum_length = max_length(reduced_dataset_25000[:, 1])\n",
    "french_vocabulary_size = len(french_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French Vocabulary Size: 8021\n",
      "French Maximum Sentence Length: 12\n"
     ]
    }
   ],
   "source": [
    "print('French Vocabulary Size:', french_vocabulary_size)\n",
    "print('French Maximum Sentence Length:', french_maximum_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the English tokenizer\n",
    "eng_tokenizer = create_tokenizer(reduced_dataset_25000[:, 0])\n",
    "english_maximum_length = max_length(reduced_dataset_25000[:, 0])\n",
    "english_vocabulary_size = len(eng_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 3949\n",
      "English Maximum Sentence Length: 5\n"
     ]
    }
   ],
   "source": [
    "print('English Vocabulary Size:',english_vocabulary_size)\n",
    "print('English Maximum Sentence Length:', english_maximum_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 1,\n",
       " 'you': 2,\n",
       " 'tom': 3,\n",
       " 'it': 4,\n",
       " 'a': 5,\n",
       " 'is': 6,\n",
       " 'im': 7,\n",
       " 'he': 8,\n",
       " 'youre': 9,\n",
       " 'me': 10,\n",
       " 'was': 11,\n",
       " 'the': 12,\n",
       " 'are': 13,\n",
       " 'we': 14,\n",
       " 'this': 15,\n",
       " 'its': 16,\n",
       " 'to': 17,\n",
       " 'that': 18,\n",
       " 'go': 19,\n",
       " 'do': 20,\n",
       " 'were': 21,\n",
       " 'have': 22,\n",
       " 'your': 23,\n",
       " 'dont': 24,\n",
       " 'not': 25,\n",
       " 'my': 26,\n",
       " 'be': 27,\n",
       " 'no': 28,\n",
       " 'can': 29,\n",
       " 'she': 30,\n",
       " 'they': 31,\n",
       " 'did': 32,\n",
       " 'all': 33,\n",
       " 'like': 34,\n",
       " 'get': 35,\n",
       " 'ill': 36,\n",
       " 'here': 37,\n",
       " 'up': 38,\n",
       " 'in': 39,\n",
       " 'on': 40,\n",
       " 'need': 41,\n",
       " 'love': 42,\n",
       " 'what': 43,\n",
       " 'thats': 44,\n",
       " 'him': 45,\n",
       " 'how': 46,\n",
       " 'theyre': 47,\n",
       " 'want': 48,\n",
       " 'very': 49,\n",
       " 'one': 50,\n",
       " 'come': 51,\n",
       " 'please': 52,\n",
       " 'out': 53,\n",
       " 'hes': 54,\n",
       " 'got': 55,\n",
       " 'let': 56,\n",
       " 'us': 57,\n",
       " 'just': 58,\n",
       " 'look': 59,\n",
       " 'lets': 60,\n",
       " 'cant': 61,\n",
       " 'so': 62,\n",
       " 'am': 63,\n",
       " 'help': 64,\n",
       " 'now': 65,\n",
       " 'stop': 66,\n",
       " 'take': 67,\n",
       " 'know': 68,\n",
       " 'see': 69,\n",
       " 'too': 70,\n",
       " 'who': 71,\n",
       " 'keep': 72,\n",
       " 'will': 73,\n",
       " 'well': 74,\n",
       " 'there': 75,\n",
       " 'good': 76,\n",
       " 'toms': 77,\n",
       " 'for': 78,\n",
       " 'try': 79,\n",
       " 'has': 80,\n",
       " 'stay': 81,\n",
       " 'at': 82,\n",
       " 'feel': 83,\n",
       " 'had': 84,\n",
       " 'of': 85,\n",
       " 'back': 86,\n",
       " 'home': 87,\n",
       " 'saw': 88,\n",
       " 'must': 89,\n",
       " 'happy': 90,\n",
       " 'give': 91,\n",
       " 'hate': 92,\n",
       " 'busy': 93,\n",
       " 'lost': 94,\n",
       " 'leave': 95,\n",
       " 'alone': 96,\n",
       " 'car': 97,\n",
       " 'again': 98,\n",
       " 'work': 99,\n",
       " 'felt': 100,\n",
       " 'her': 101,\n",
       " 'them': 102,\n",
       " 'didnt': 103,\n",
       " 'may': 104,\n",
       " 'down': 105,\n",
       " 'made': 106,\n",
       " 'eat': 107,\n",
       " 'away': 108,\n",
       " 'right': 109,\n",
       " 'dog': 110,\n",
       " 'wait': 111,\n",
       " 'went': 112,\n",
       " 'an': 113,\n",
       " 'time': 114,\n",
       " 'never': 115,\n",
       " 'ive': 116,\n",
       " 'nice': 117,\n",
       " 'job': 118,\n",
       " 'everyone': 119,\n",
       " 'still': 120,\n",
       " 'drink': 121,\n",
       " 'mary': 122,\n",
       " 'whats': 123,\n",
       " 'isnt': 124,\n",
       " 'say': 125,\n",
       " 'ready': 126,\n",
       " 'with': 127,\n",
       " 'tired': 128,\n",
       " 'his': 129,\n",
       " 'make': 130,\n",
       " 'hurt': 131,\n",
       " 'left': 132,\n",
       " 'safe': 133,\n",
       " 'why': 134,\n",
       " 'mine': 135,\n",
       " 'going': 136,\n",
       " 'big': 137,\n",
       " 'done': 138,\n",
       " 'bad': 139,\n",
       " 'careful': 140,\n",
       " 'tell': 141,\n",
       " 'book': 142,\n",
       " 'looks': 143,\n",
       " 'some': 144,\n",
       " 'call': 145,\n",
       " 'over': 146,\n",
       " 'old': 147,\n",
       " 'kept': 148,\n",
       " 'fun': 149,\n",
       " 'about': 150,\n",
       " 'off': 151,\n",
       " 'ok': 152,\n",
       " 'late': 153,\n",
       " 'shes': 154,\n",
       " 'talk': 155,\n",
       " 'easy': 156,\n",
       " 'run': 157,\n",
       " 'does': 158,\n",
       " 'cold': 159,\n",
       " 'hurry': 160,\n",
       " 'really': 161,\n",
       " 'new': 162,\n",
       " 'wasnt': 163,\n",
       " 'died': 164,\n",
       " 'lot': 165,\n",
       " 'ask': 166,\n",
       " 'could': 167,\n",
       " 'think': 168,\n",
       " 'hard': 169,\n",
       " 'open': 170,\n",
       " 'wrong': 171,\n",
       " 'should': 172,\n",
       " 'read': 173,\n",
       " 'french': 174,\n",
       " 'came': 175,\n",
       " 'live': 176,\n",
       " 'money': 177,\n",
       " 'whos': 178,\n",
       " 'more': 179,\n",
       " 'day': 180,\n",
       " 'likes': 181,\n",
       " 'bed': 182,\n",
       " 'sit': 183,\n",
       " 'watch': 184,\n",
       " 'found': 185,\n",
       " 'trust': 186,\n",
       " 'wont': 187,\n",
       " 'sure': 188,\n",
       " 'way': 189,\n",
       " 'broke': 190,\n",
       " 'enough': 191,\n",
       " 'bring': 192,\n",
       " 'yours': 193,\n",
       " 'win': 194,\n",
       " 'hear': 195,\n",
       " 'said': 196,\n",
       " 'sick': 197,\n",
       " 'today': 198,\n",
       " 'wheres': 199,\n",
       " 'crazy': 200,\n",
       " 'our': 201,\n",
       " 'swim': 202,\n",
       " 'room': 203,\n",
       " 'find': 204,\n",
       " 'door': 205,\n",
       " 'pay': 206,\n",
       " 'hope': 207,\n",
       " 'sad': 208,\n",
       " 'angry': 209,\n",
       " 'cat': 210,\n",
       " 'miss': 211,\n",
       " 'upset': 212,\n",
       " 'early': 213,\n",
       " 'lucky': 214,\n",
       " 'die': 215,\n",
       " 'where': 216,\n",
       " 'nothing': 217,\n",
       " 'quit': 218,\n",
       " 'id': 219,\n",
       " 'nobody': 220,\n",
       " 'called': 221,\n",
       " 'won': 222,\n",
       " 'buy': 223,\n",
       " 'tall': 224,\n",
       " 'life': 225,\n",
       " 'sleep': 226,\n",
       " 'fine': 227,\n",
       " 'dead': 228,\n",
       " 'forget': 229,\n",
       " 'funny': 230,\n",
       " 'loves': 231,\n",
       " 'hungry': 232,\n",
       " 'better': 233,\n",
       " 'walk': 234,\n",
       " 'man': 235,\n",
       " 'close': 236,\n",
       " 'wine': 237,\n",
       " 'gave': 238,\n",
       " 'cool': 239,\n",
       " 'play': 240,\n",
       " 'naive': 241,\n",
       " 'eyes': 242,\n",
       " 'idea': 243,\n",
       " 'quiet': 244,\n",
       " 'mad': 245,\n",
       " 'water': 246,\n",
       " 'working': 247,\n",
       " 'mean': 248,\n",
       " 'kidding': 249,\n",
       " 'care': 250,\n",
       " 'hows': 251,\n",
       " 'yourself': 252,\n",
       " 'sorry': 253,\n",
       " 'sing': 254,\n",
       " 'knows': 255,\n",
       " 'much': 256,\n",
       " 'free': 257,\n",
       " 'took': 258,\n",
       " 'drunk': 259,\n",
       " 'bit': 260,\n",
       " 'fat': 261,\n",
       " 'serious': 262,\n",
       " 'turn': 263,\n",
       " 'looked': 264,\n",
       " 'theres': 265,\n",
       " 'show': 266,\n",
       " 'stupid': 267,\n",
       " 'fast': 268,\n",
       " 'by': 269,\n",
       " 'remember': 270,\n",
       " 'check': 271,\n",
       " 'ahead': 272,\n",
       " 'perfect': 273,\n",
       " 'true': 274,\n",
       " 'anyone': 275,\n",
       " 'knew': 276,\n",
       " 'beer': 277,\n",
       " 'both': 278,\n",
       " 'hold': 279,\n",
       " 'finished': 280,\n",
       " 'start': 281,\n",
       " 'move': 282,\n",
       " 'everybody': 283,\n",
       " 'shot': 284,\n",
       " 'first': 285,\n",
       " 'cry': 286,\n",
       " 'outside': 287,\n",
       " 'from': 288,\n",
       " 'liked': 289,\n",
       " 'handle': 290,\n",
       " 'put': 291,\n",
       " 'house': 292,\n",
       " 'calm': 293,\n",
       " 'fired': 294,\n",
       " 'hair': 295,\n",
       " 'hat': 296,\n",
       " 'needed': 297,\n",
       " 'touch': 298,\n",
       " 'coming': 299,\n",
       " 'loved': 300,\n",
       " 'stand': 301,\n",
       " 'gone': 302,\n",
       " 'once': 303,\n",
       " 'break': 304,\n",
       " 'works': 305,\n",
       " 'problem': 306,\n",
       " 'these': 307,\n",
       " 'follow': 308,\n",
       " 'running': 309,\n",
       " 'sweet': 310,\n",
       " 'boy': 311,\n",
       " 'friends': 312,\n",
       " 'ate': 313,\n",
       " 'listen': 314,\n",
       " 'lazy': 315,\n",
       " 'sign': 316,\n",
       " 'inside': 317,\n",
       " 'doctor': 318,\n",
       " 'hot': 319,\n",
       " 'plan': 320,\n",
       " 'drive': 321,\n",
       " 'best': 322,\n",
       " 'lie': 323,\n",
       " 'heard': 324,\n",
       " 'yet': 325,\n",
       " 'told': 326,\n",
       " 'drop': 327,\n",
       " 'fix': 328,\n",
       " 'seem': 329,\n",
       " 'hand': 330,\n",
       " 'seems': 331,\n",
       " 'everything': 332,\n",
       " 'fish': 333,\n",
       " 'ran': 334,\n",
       " 'name': 335,\n",
       " 'boss': 336,\n",
       " 'wake': 337,\n",
       " 'beat': 338,\n",
       " 'wanted': 339,\n",
       " 'as': 340,\n",
       " 'word': 341,\n",
       " 'prepared': 342,\n",
       " 'school': 343,\n",
       " 'luck': 344,\n",
       " 'food': 345,\n",
       " 'bag': 346,\n",
       " 'son': 347,\n",
       " 'met': 348,\n",
       " 'wonderful': 349,\n",
       " 'shut': 350,\n",
       " 'caught': 351,\n",
       " 'kids': 352,\n",
       " 'write': 353,\n",
       " 'and': 354,\n",
       " 'arent': 355,\n",
       " 'red': 356,\n",
       " 'thanks': 357,\n",
       " 'needs': 358,\n",
       " 'young': 359,\n",
       " 'invited': 360,\n",
       " 'lying': 361,\n",
       " 'weird': 362,\n",
       " 'pretty': 363,\n",
       " 'hates': 364,\n",
       " 'sat': 365,\n",
       " 'rude': 366,\n",
       " 'music': 367,\n",
       " 'almost': 368,\n",
       " 'only': 369,\n",
       " 'rich': 370,\n",
       " 'strong': 371,\n",
       " 'speak': 372,\n",
       " 'cut': 373,\n",
       " 'scared': 374,\n",
       " 'anybody': 375,\n",
       " 'key': 376,\n",
       " 'around': 377,\n",
       " 'use': 378,\n",
       " 'lonely': 379,\n",
       " 'awesome': 380,\n",
       " 'talking': 381,\n",
       " 'doing': 382,\n",
       " 'shy': 383,\n",
       " 'fell': 384,\n",
       " 'grab': 385,\n",
       " 'long': 386,\n",
       " 'friend': 387,\n",
       " 'enjoy': 388,\n",
       " 'cute': 389,\n",
       " 'itll': 390,\n",
       " 'nervous': 391,\n",
       " 'someone': 392,\n",
       " 'bus': 393,\n",
       " 'married': 394,\n",
       " 'girls': 395,\n",
       " 'kind': 396,\n",
       " 'great': 397,\n",
       " 'coffee': 398,\n",
       " 'tried': 399,\n",
       " 'dogs': 400,\n",
       " 'quickly': 401,\n",
       " 'confused': 402,\n",
       " 'owe': 403,\n",
       " 'fair': 404,\n",
       " 'trusted': 405,\n",
       " 'myself': 406,\n",
       " 'excited': 407,\n",
       " 'wants': 408,\n",
       " 'crying': 409,\n",
       " 'laughed': 410,\n",
       " 'paid': 411,\n",
       " 'rest': 412,\n",
       " 'gun': 413,\n",
       " 'lock': 414,\n",
       " 'answer': 415,\n",
       " 'lunch': 416,\n",
       " 'clean': 417,\n",
       " 'nuts': 418,\n",
       " 'theyll': 419,\n",
       " 'joking': 420,\n",
       " 'normal': 421,\n",
       " 'game': 422,\n",
       " 'change': 423,\n",
       " 'deal': 424,\n",
       " 'lied': 425,\n",
       " 'hang': 426,\n",
       " 'missed': 427,\n",
       " 'saved': 428,\n",
       " 'two': 429,\n",
       " 'cheated': 430,\n",
       " 'moving': 431,\n",
       " 'wise': 432,\n",
       " 'tie': 433,\n",
       " 'winning': 434,\n",
       " 'changed': 435,\n",
       " 'worked': 436,\n",
       " 'ignore': 437,\n",
       " 'amazing': 438,\n",
       " 'agree': 439,\n",
       " 'alive': 440,\n",
       " 'relaxed': 441,\n",
       " 'wife': 442,\n",
       " 'send': 443,\n",
       " 'soon': 444,\n",
       " 'doesnt': 445,\n",
       " 'fight': 446,\n",
       " 'sleepy': 447,\n",
       " 'vote': 448,\n",
       " 'betrayed': 449,\n",
       " 'smart': 450,\n",
       " 'weak': 451,\n",
       " 'tv': 452,\n",
       " 'slow': 453,\n",
       " 'heres': 454,\n",
       " 'those': 455,\n",
       " 'dream': 456,\n",
       " 'terrific': 457,\n",
       " 'began': 458,\n",
       " 'study': 459,\n",
       " 'helped': 460,\n",
       " 'minute': 461,\n",
       " 'drank': 462,\n",
       " 'somebody': 463,\n",
       " 'seen': 464,\n",
       " 'something': 465,\n",
       " 'after': 466,\n",
       " 'kill': 467,\n",
       " 'reading': 468,\n",
       " 'trying': 469,\n",
       " 'hired': 470,\n",
       " 'ones': 471,\n",
       " 'spoke': 472,\n",
       " 'catch': 473,\n",
       " 'started': 474,\n",
       " 'awful': 475,\n",
       " 'if': 476,\n",
       " 'already': 477,\n",
       " 'upstairs': 478,\n",
       " 'boston': 479,\n",
       " 'woke': 480,\n",
       " 'fit': 481,\n",
       " 'hurts': 482,\n",
       " 'relax': 483,\n",
       " 'light': 484,\n",
       " 'killed': 485,\n",
       " 'tea': 486,\n",
       " 'youve': 487,\n",
       " 'slowly': 488,\n",
       " 'hands': 489,\n",
       " 'last': 490,\n",
       " 'seat': 491,\n",
       " 'important': 492,\n",
       " 'afraid': 493,\n",
       " 'correct': 494,\n",
       " 'smell': 495,\n",
       " 'weve': 496,\n",
       " 'honest': 497,\n",
       " 'count': 498,\n",
       " 'tough': 499,\n",
       " 'trapped': 500,\n",
       " 'place': 501,\n",
       " 'guess': 502,\n",
       " 'eating': 503,\n",
       " 'full': 504,\n",
       " 'cried': 505,\n",
       " 'teacher': 506,\n",
       " 'father': 507,\n",
       " 'push': 508,\n",
       " 'leaving': 509,\n",
       " 'god': 510,\n",
       " 'bored': 511,\n",
       " 'smiled': 512,\n",
       " 'milk': 513,\n",
       " 'horse': 514,\n",
       " 'mind': 515,\n",
       " 'unlucky': 516,\n",
       " 'laugh': 517,\n",
       " 'been': 518,\n",
       " 'trip': 519,\n",
       " 'phone': 520,\n",
       " 'joke': 521,\n",
       " 'pen': 522,\n",
       " 'cruel': 523,\n",
       " 'asked': 524,\n",
       " 'pardon': 525,\n",
       " 'which': 526,\n",
       " 'song': 527,\n",
       " 'forgot': 528,\n",
       " 'famous': 529,\n",
       " 'stopped': 530,\n",
       " 'innocent': 531,\n",
       " 'tomorrow': 532,\n",
       " 'liar': 533,\n",
       " 'arm': 534,\n",
       " 'creative': 535,\n",
       " 'exhausted': 536,\n",
       " 'bought': 537,\n",
       " 'dying': 538,\n",
       " 'asleep': 539,\n",
       " 'smiling': 540,\n",
       " 'turned': 541,\n",
       " 'walked': 542,\n",
       " 'continue': 543,\n",
       " 'blame': 544,\n",
       " 'involved': 545,\n",
       " 'finish': 546,\n",
       " 'town': 547,\n",
       " 'bill': 548,\n",
       " 'lose': 549,\n",
       " 'getting': 550,\n",
       " 'being': 551,\n",
       " 'short': 552,\n",
       " 'tomll': 553,\n",
       " 'set': 554,\n",
       " 'age': 555,\n",
       " 'mom': 556,\n",
       " 'often': 557,\n",
       " 'cook': 558,\n",
       " 'hug': 559,\n",
       " 'own': 560,\n",
       " 'terrible': 561,\n",
       " 'silent': 562,\n",
       " 'student': 563,\n",
       " 'cats': 564,\n",
       " 'shoes': 565,\n",
       " 'smoke': 566,\n",
       " 'retired': 567,\n",
       " 'sounds': 568,\n",
       " 'became': 569,\n",
       " 'slept': 570,\n",
       " 'join': 571,\n",
       " 'arrived': 572,\n",
       " 'dark': 573,\n",
       " 'quite': 574,\n",
       " 'might': 575,\n",
       " 'failed': 576,\n",
       " 'patient': 577,\n",
       " 'pain': 578,\n",
       " 'kiss': 579,\n",
       " 'merciful': 580,\n",
       " 'awake': 581,\n",
       " 'believe': 582,\n",
       " 'books': 583,\n",
       " 'every': 584,\n",
       " 'wet': 585,\n",
       " 'shopping': 586,\n",
       " 'naked': 587,\n",
       " 'fake': 588,\n",
       " 'raise': 589,\n",
       " 'child': 590,\n",
       " 'waited': 591,\n",
       " 'youll': 592,\n",
       " 'jump': 593,\n",
       " 'tight': 594,\n",
       " 'side': 595,\n",
       " 'real': 596,\n",
       " 'when': 597,\n",
       " 'step': 598,\n",
       " 'doors': 599,\n",
       " 'feet': 600,\n",
       " 'yes': 601,\n",
       " 'foolish': 602,\n",
       " 'far': 603,\n",
       " 'useless': 604,\n",
       " 'driving': 605,\n",
       " 'into': 606,\n",
       " 'duty': 607,\n",
       " 'worry': 608,\n",
       " 'small': 609,\n",
       " 'carefully': 610,\n",
       " 'family': 611,\n",
       " 'smile': 612,\n",
       " 'map': 613,\n",
       " 'bite': 614,\n",
       " 'yelling': 615,\n",
       " 'obey': 616,\n",
       " 'girl': 617,\n",
       " 'idiot': 618,\n",
       " 'sound': 619,\n",
       " 'bet': 620,\n",
       " 'kissed': 621,\n",
       " 'face': 622,\n",
       " 'wish': 623,\n",
       " 'always': 624,\n",
       " 'worried': 625,\n",
       " 'news': 626,\n",
       " 'point': 627,\n",
       " 'silly': 628,\n",
       " 'resilient': 629,\n",
       " 'dancing': 630,\n",
       " 'motivated': 631,\n",
       " 'blind': 632,\n",
       " 'ours': 633,\n",
       " 'boring': 634,\n",
       " 'seated': 635,\n",
       " 'save': 636,\n",
       " 'ugly': 637,\n",
       " 'hit': 638,\n",
       " 'choice': 639,\n",
       " 'thirsty': 640,\n",
       " 'many': 641,\n",
       " 'welcome': 642,\n",
       " 'curious': 643,\n",
       " 'warned': 644,\n",
       " 'shall': 645,\n",
       " 'pick': 646,\n",
       " 'positive': 647,\n",
       " 'fly': 648,\n",
       " 'control': 649,\n",
       " 'japanese': 650,\n",
       " 'excuse': 651,\n",
       " 'content': 652,\n",
       " 'beautiful': 653,\n",
       " 'trusts': 654,\n",
       " 'losing': 655,\n",
       " 'blue': 656,\n",
       " 'guy': 657,\n",
       " 'ambitious': 658,\n",
       " 'escaped': 659,\n",
       " 'warm': 660,\n",
       " 'punctual': 661,\n",
       " 'wrote': 662,\n",
       " 'bike': 663,\n",
       " 'shoot': 664,\n",
       " 'seemed': 665,\n",
       " 'camera': 666,\n",
       " 'head': 667,\n",
       " 'forgive': 668,\n",
       " 'hi': 669,\n",
       " 'fire': 670,\n",
       " 'fearless': 671,\n",
       " 'couldnt': 672,\n",
       " 'dance': 673,\n",
       " 'meant': 674,\n",
       " 'story': 675,\n",
       " 'nearby': 676,\n",
       " 'mother': 677,\n",
       " 'refused': 678,\n",
       " 'clear': 679,\n",
       " 'helps': 680,\n",
       " 'end': 681,\n",
       " 'dumped': 682,\n",
       " 'meet': 683,\n",
       " 'boat': 684,\n",
       " 'singing': 685,\n",
       " 'surprised': 686,\n",
       " 'hero': 687,\n",
       " 'choose': 688,\n",
       " 'date': 689,\n",
       " 'dangerous': 690,\n",
       " 'unbelievable': 691,\n",
       " 'tricked': 692,\n",
       " 'brave': 693,\n",
       " 'beg': 694,\n",
       " 'unhappy': 695,\n",
       " 'behind': 696,\n",
       " 'decided': 697,\n",
       " 'everyones': 698,\n",
       " 'watching': 699,\n",
       " 'law': 700,\n",
       " 'stuck': 701,\n",
       " 'refuse': 702,\n",
       " 'depressed': 703,\n",
       " 'would': 704,\n",
       " 'looking': 705,\n",
       " 'talked': 706,\n",
       " 'order': 707,\n",
       " 'studying': 708,\n",
       " 'smells': 709,\n",
       " 'meat': 710,\n",
       " 'dinner': 711,\n",
       " 'agreed': 712,\n",
       " 'powerful': 713,\n",
       " 'direct': 714,\n",
       " 'war': 715,\n",
       " 'cake': 716,\n",
       " 'opened': 717,\n",
       " 'cover': 718,\n",
       " 'laughing': 719,\n",
       " 'discreet': 720,\n",
       " 'guys': 721,\n",
       " 'maybe': 722,\n",
       " 'cops': 723,\n",
       " 'focused': 724,\n",
       " 'sent': 725,\n",
       " 'armed': 726,\n",
       " 'loosen': 727,\n",
       " 'broken': 728,\n",
       " 'followed': 729,\n",
       " 'satisfied': 730,\n",
       " 'closer': 731,\n",
       " 'certain': 732,\n",
       " 'second': 733,\n",
       " 'understand': 734,\n",
       " 'survive': 735,\n",
       " 'rush': 736,\n",
       " 'forgetful': 737,\n",
       " 'sense': 738,\n",
       " 'stubborn': 739,\n",
       " 'fear': 740,\n",
       " 'mess': 741,\n",
       " 'men': 742,\n",
       " 'ball': 743,\n",
       " 'movie': 744,\n",
       " 'clever': 745,\n",
       " 'jobs': 746,\n",
       " 'black': 747,\n",
       " 'lives': 748,\n",
       " 'rescued': 749,\n",
       " 'trouble': 750,\n",
       " 'horrible': 751,\n",
       " 'loud': 752,\n",
       " 'happened': 753,\n",
       " 'dry': 754,\n",
       " 'listening': 755,\n",
       " 'guilty': 756,\n",
       " 'also': 757,\n",
       " 'kid': 758,\n",
       " 'skiing': 759,\n",
       " 'coat': 760,\n",
       " 'wash': 761,\n",
       " 'prefer': 762,\n",
       " 'fighting': 763,\n",
       " 'hello': 764,\n",
       " 'pizza': 765,\n",
       " 'keys': 766,\n",
       " 'bald': 767,\n",
       " 'party': 768,\n",
       " 'together': 769,\n",
       " 'attack': 770,\n",
       " 'bell': 771,\n",
       " 'friendly': 772,\n",
       " 'skinny': 773,\n",
       " 'confident': 774,\n",
       " 'used': 775,\n",
       " 'deaf': 776,\n",
       " 'special': 777,\n",
       " 'grow': 778,\n",
       " 'writing': 779,\n",
       " 'fined': 780,\n",
       " 'screaming': 781,\n",
       " 'happen': 782,\n",
       " 'speaks': 783,\n",
       " 'ski': 784,\n",
       " 'saint': 785,\n",
       " 'yourselves': 786,\n",
       " 'crafty': 787,\n",
       " 'objective': 788,\n",
       " 'fault': 789,\n",
       " 'dumb': 790,\n",
       " 'fool': 791,\n",
       " 'cash': 792,\n",
       " 'ashamed': 793,\n",
       " 'students': 794,\n",
       " 'tonight': 795,\n",
       " 'brought': 796,\n",
       " 'sincere': 797,\n",
       " 'eggs': 798,\n",
       " 'hated': 799,\n",
       " 'faster': 800,\n",
       " 'singer': 801,\n",
       " 'lit': 802,\n",
       " 'lawyer': 803,\n",
       " 'stalling': 804,\n",
       " 'memorize': 805,\n",
       " 'grew': 806,\n",
       " 'powerless': 807,\n",
       " 'cooking': 808,\n",
       " 'secret': 809,\n",
       " 'promised': 810,\n",
       " 'fed': 811,\n",
       " 'chair': 812,\n",
       " 'list': 813,\n",
       " 'simple': 814,\n",
       " 'starved': 815,\n",
       " 'twins': 816,\n",
       " 'along': 817,\n",
       " 'rid': 818,\n",
       " 'truly': 819,\n",
       " 'soccer': 820,\n",
       " 'any': 821,\n",
       " 'decide': 822,\n",
       " 'begin': 823,\n",
       " 'stunned': 824,\n",
       " 'stayed': 825,\n",
       " 'drinks': 826,\n",
       " 'rather': 827,\n",
       " 'sneaky': 828,\n",
       " 'nearly': 829,\n",
       " 'night': 830,\n",
       " 'jealous': 831,\n",
       " 'reply': 832,\n",
       " 'shoe': 833,\n",
       " 'store': 834,\n",
       " 'team': 835,\n",
       " 'explain': 836,\n",
       " 'proof': 837,\n",
       " 'teach': 838,\n",
       " 'dizzy': 839,\n",
       " 'hugged': 840,\n",
       " 'bath': 841,\n",
       " 'pull': 842,\n",
       " 'warn': 843,\n",
       " 'throw': 844,\n",
       " 'gear': 845,\n",
       " 'comes': 846,\n",
       " 'envious': 847,\n",
       " 'watched': 848,\n",
       " 'injured': 849,\n",
       " 'poor': 850,\n",
       " 'fortunate': 851,\n",
       " 'walking': 852,\n",
       " 'staying': 853,\n",
       " 'hurried': 854,\n",
       " 'brief': 855,\n",
       " 'while': 856,\n",
       " 'phoned': 857,\n",
       " 'believed': 858,\n",
       " 'afford': 859,\n",
       " 'sister': 860,\n",
       " 'makes': 861,\n",
       " 'moment': 862,\n",
       " 'charming': 863,\n",
       " 'finally': 864,\n",
       " 'plays': 865,\n",
       " 'ruthless': 866,\n",
       " 'arguing': 867,\n",
       " 'hers': 868,\n",
       " 'risk': 869,\n",
       " 'dreaming': 870,\n",
       " 'jokes': 871,\n",
       " 'promise': 872,\n",
       " 'demented': 873,\n",
       " 'fussy': 874,\n",
       " 'taste': 875,\n",
       " 'horses': 876,\n",
       " 'week': 877,\n",
       " 'artist': 878,\n",
       " 'deserve': 879,\n",
       " 'cheerful': 880,\n",
       " 'resist': 881,\n",
       " 'downstairs': 882,\n",
       " 'ordered': 883,\n",
       " 'manage': 884,\n",
       " 'window': 885,\n",
       " 'single': 886,\n",
       " 'thank': 887,\n",
       " 'burned': 888,\n",
       " 'polite': 889,\n",
       " 'whose': 890,\n",
       " 'shocked': 891,\n",
       " 'smoking': 892,\n",
       " 'tense': 893,\n",
       " 'oh': 894,\n",
       " 'volunteered': 895,\n",
       " 'screamed': 896,\n",
       " 'stoned': 897,\n",
       " 'psychic': 898,\n",
       " 'stinks': 899,\n",
       " 'matter': 900,\n",
       " 'dirty': 901,\n",
       " 'children': 902,\n",
       " 'another': 903,\n",
       " 'closely': 904,\n",
       " 'everybodys': 905,\n",
       " 'cheered': 906,\n",
       " 'hopeless': 907,\n",
       " 'sharp': 908,\n",
       " 'drives': 909,\n",
       " 'box': 910,\n",
       " 'train': 911,\n",
       " 'wore': 912,\n",
       " 'shame': 913,\n",
       " 'baby': 914,\n",
       " 'seriously': 915,\n",
       " 'little': 916,\n",
       " 'chose': 917,\n",
       " 'amused': 918,\n",
       " 'touched': 919,\n",
       " 'ruined': 920,\n",
       " 'bless': 921,\n",
       " 'thin': 922,\n",
       " 'shaken': 923,\n",
       " 'generous': 924,\n",
       " 'person': 925,\n",
       " 'attacked': 926,\n",
       " 'smaller': 927,\n",
       " 'nonsense': 928,\n",
       " 'mistaken': 929,\n",
       " 'outrank': 930,\n",
       " 'sore': 931,\n",
       " 'impressed': 932,\n",
       " 'enemy': 933,\n",
       " 'plans': 934,\n",
       " 'cows': 935,\n",
       " 'sleeping': 936,\n",
       " 'thorough': 937,\n",
       " 'flinched': 938,\n",
       " 'same': 939,\n",
       " 'lovely': 940,\n",
       " 'knees': 941,\n",
       " 'digging': 942,\n",
       " 'apologize': 943,\n",
       " 'deserved': 944,\n",
       " 'born': 945,\n",
       " 'high': 946,\n",
       " 'odd': 947,\n",
       " 'maid': 948,\n",
       " 'nose': 949,\n",
       " 'congratulations': 950,\n",
       " 'feels': 951,\n",
       " 'worth': 952,\n",
       " 'escaping': 953,\n",
       " 'talks': 954,\n",
       " 'english': 955,\n",
       " 'annoying': 956,\n",
       " 'sang': 957,\n",
       " 'built': 958,\n",
       " 'quick': 959,\n",
       " 'foot': 960,\n",
       " 'people': 961,\n",
       " 'ended': 962,\n",
       " 'feeling': 963,\n",
       " 'forgiven': 964,\n",
       " 'noticed': 965,\n",
       " 'strict': 966,\n",
       " 'finicky': 967,\n",
       " 'prudent': 968,\n",
       " 'bird': 969,\n",
       " 'doubts': 970,\n",
       " 'anything': 971,\n",
       " 'rain': 972,\n",
       " 'six': 973,\n",
       " 'messed': 974,\n",
       " 'insane': 975,\n",
       " 'ears': 976,\n",
       " 'intrigued': 977,\n",
       " 'leak': 978,\n",
       " 'different': 979,\n",
       " 'birds': 980,\n",
       " 'travel': 981,\n",
       " 'divorced': 982,\n",
       " 'arrogant': 983,\n",
       " 'canceled': 984,\n",
       " 'attend': 985,\n",
       " 'scolded': 986,\n",
       " 'monday': 987,\n",
       " 'snow': 988,\n",
       " 'note': 989,\n",
       " 'scream': 990,\n",
       " 'canadian': 991,\n",
       " 'aside': 992,\n",
       " 'stood': 993,\n",
       " 'sue': 994,\n",
       " 'favor': 995,\n",
       " 'speaking': 996,\n",
       " 'moved': 997,\n",
       " 'fishing': 998,\n",
       " 'runs': 999,\n",
       " 'danger': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word index of English tokenizer\n",
    "eng_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'je': 1,\n",
       " 'tom': 2,\n",
       " 'a': 3,\n",
       " 'pas': 4,\n",
       " 'suis': 5,\n",
       " 'est': 6,\n",
       " 'vous': 7,\n",
       " 'il': 8,\n",
       " 'nous': 9,\n",
       " 'de': 10,\n",
       " 'ne': 11,\n",
       " 'le': 12,\n",
       " 'la': 13,\n",
       " 'jai': 14,\n",
       " 'cest': 15,\n",
       " 'un': 16,\n",
       " 'tu': 17,\n",
       " 'me': 18,\n",
       " 'en': 19,\n",
       " 'ca': 20,\n",
       " 'les': 21,\n",
       " 'une': 22,\n",
       " 'que': 23,\n",
       " 'etes': 24,\n",
       " 'elle': 25,\n",
       " 'sommes': 26,\n",
       " 'ce': 27,\n",
       " 'estce': 28,\n",
       " 'es': 29,\n",
       " 'sont': 30,\n",
       " 'fait': 31,\n",
       " 'ils': 32,\n",
       " 'qui': 33,\n",
       " 'tout': 34,\n",
       " 'des': 35,\n",
       " 'ma': 36,\n",
       " 'te': 37,\n",
       " 'mon': 38,\n",
       " 'bien': 39,\n",
       " 'elles': 40,\n",
       " 'du': 41,\n",
       " 'nest': 42,\n",
       " 'ici': 43,\n",
       " 'ete': 44,\n",
       " 'besoin': 45,\n",
       " 'y': 46,\n",
       " 'se': 47,\n",
       " 'va': 48,\n",
       " 'jaime': 49,\n",
       " 'faire': 50,\n",
       " 'etesvous': 51,\n",
       " 'cela': 52,\n",
       " 'personne': 53,\n",
       " 'moi': 54,\n",
       " 'tres': 55,\n",
       " 'faut': 56,\n",
       " 'votre': 57,\n",
       " 'etait': 58,\n",
       " 'lair': 59,\n",
       " 'veux': 60,\n",
       " 'peux': 61,\n",
       " 'ai': 62,\n",
       " 'fais': 63,\n",
       " 'soyez': 64,\n",
       " 'au': 65,\n",
       " 'jetais': 66,\n",
       " 'aller': 67,\n",
       " 'avons': 68,\n",
       " 'tous': 69,\n",
       " 'ton': 70,\n",
       " 'train': 71,\n",
       " 'on': 72,\n",
       " 'lai': 73,\n",
       " 'cetait': 74,\n",
       " 'sest': 75,\n",
       " 'comment': 76,\n",
       " 'sois': 77,\n",
       " 'monde': 78,\n",
       " 'maintenant': 79,\n",
       " 'estu': 80,\n",
       " 'trop': 81,\n",
       " 'plait': 82,\n",
       " 'jadore': 83,\n",
       " 'ceci': 84,\n",
       " 'bon': 85,\n",
       " 'deteste': 86,\n",
       " 'toi': 87,\n",
       " 'mal': 88,\n",
       " 'sens': 89,\n",
       " 'pour': 90,\n",
       " 'partir': 91,\n",
       " 'sil': 92,\n",
       " 'ont': 93,\n",
       " 'si': 94,\n",
       " 'sur': 95,\n",
       " 'as': 96,\n",
       " 'vu': 97,\n",
       " 'voiture': 98,\n",
       " 'comme': 99,\n",
       " 'lui': 100,\n",
       " 'toutes': 101,\n",
       " 'dois': 102,\n",
       " 'reste': 103,\n",
       " 'ta': 104,\n",
       " 'puisje': 105,\n",
       " 'maison': 106,\n",
       " 'plus': 107,\n",
       " 'dans': 108,\n",
       " 'ou': 109,\n",
       " 'nai': 110,\n",
       " 'vais': 111,\n",
       " 'avec': 112,\n",
       " 'estil': 113,\n",
       " 'dit': 114,\n",
       " 'jen': 115,\n",
       " 'quel': 116,\n",
       " 'arrete': 117,\n",
       " 'jamais': 118,\n",
       " 'faites': 119,\n",
       " 'cette': 120,\n",
       " 'chien': 121,\n",
       " 'viens': 122,\n",
       " 'voir': 123,\n",
       " 'men': 124,\n",
       " 'vraiment': 125,\n",
       " 'quelle': 126,\n",
       " 'restez': 127,\n",
       " 'na': 128,\n",
       " 'avez': 129,\n",
       " 'rien': 130,\n",
       " 'encore': 131,\n",
       " 'dun': 132,\n",
       " 'juste': 133,\n",
       " 'aime': 134,\n",
       " 'avezvous': 135,\n",
       " 'seul': 136,\n",
       " 'prie': 137,\n",
       " 'astu': 138,\n",
       " 'perdu': 139,\n",
       " 'temps': 140,\n",
       " 'nouveau': 141,\n",
       " 'chez': 142,\n",
       " 'beaucoup': 143,\n",
       " 'tes': 144,\n",
       " 'questce': 145,\n",
       " 'mes': 146,\n",
       " 'peut': 147,\n",
       " 'pourquoi': 148,\n",
       " 'porte': 149,\n",
       " 'regarde': 150,\n",
       " 'sais': 151,\n",
       " 'bonne': 152,\n",
       " 'quelquun': 153,\n",
       " 'trouve': 154,\n",
       " 'fort': 155,\n",
       " 'confiance': 156,\n",
       " 'deux': 157,\n",
       " 'tellement': 158,\n",
       " 'mange': 159,\n",
       " 'prends': 160,\n",
       " 'livre': 161,\n",
       " 'laisse': 162,\n",
       " 'ny': 163,\n",
       " 'parle': 164,\n",
       " 'heureux': 165,\n",
       " 'travail': 166,\n",
       " 'avait': 167,\n",
       " 'etre': 168,\n",
       " 'marie': 169,\n",
       " 'allez': 170,\n",
       " 'dune': 171,\n",
       " 'pris': 172,\n",
       " 'eu': 173,\n",
       " 'continue': 174,\n",
       " 'atil': 175,\n",
       " 'peu': 176,\n",
       " 'grand': 177,\n",
       " 'non': 178,\n",
       " 'prenez': 179,\n",
       " 'termine': 180,\n",
       " 'occupe': 181,\n",
       " 'arretez': 182,\n",
       " 'quoi': 183,\n",
       " 'laissezmoi': 184,\n",
       " 'aider': 185,\n",
       " 'laissemoi': 186,\n",
       " 'tai': 187,\n",
       " 'vie': 188,\n",
       " 'francais': 189,\n",
       " 'attention': 190,\n",
       " 'veuillez': 191,\n",
       " 'notre': 192,\n",
       " 'son': 193,\n",
       " 'jy': 194,\n",
       " 'vite': 195,\n",
       " 'assez': 196,\n",
       " 'nen': 197,\n",
       " 'malade': 198,\n",
       " 'gagne': 199,\n",
       " 'yeux': 200,\n",
       " 'tranquille': 201,\n",
       " 'coup': 202,\n",
       " 'fini': 203,\n",
       " 'chercher': 204,\n",
       " 'calme': 205,\n",
       " 'fatigue': 206,\n",
       " 'aujourdhui': 207,\n",
       " 'tomber': 208,\n",
       " 'securite': 209,\n",
       " 'marche': 210,\n",
       " 'mort': 211,\n",
       " 'daccord': 212,\n",
       " 'chose': 213,\n",
       " 'pret': 214,\n",
       " 'senti': 215,\n",
       " 'simplement': 216,\n",
       " 'manger': 217,\n",
       " 'lit': 218,\n",
       " 'sen': 219,\n",
       " 'faim': 220,\n",
       " 'sans': 221,\n",
       " 'mieux': 222,\n",
       " 'blesse': 223,\n",
       " 'boulot': 224,\n",
       " 'nager': 225,\n",
       " 'parler': 226,\n",
       " 'voici': 227,\n",
       " 'connais': 228,\n",
       " 'idee': 229,\n",
       " 'demande': 230,\n",
       " 'vrai': 231,\n",
       " 'fut': 232,\n",
       " 'adore': 233,\n",
       " 'vin': 234,\n",
       " 'venez': 235,\n",
       " 'travaille': 236,\n",
       " 'colere': 237,\n",
       " 'sait': 238,\n",
       " 'essaie': 239,\n",
       " 'froid': 240,\n",
       " 'regardez': 241,\n",
       " 'chance': 242,\n",
       " 'triste': 243,\n",
       " 'venir': 244,\n",
       " 'parti': 245,\n",
       " 'gros': 246,\n",
       " 'dire': 247,\n",
       " 'semble': 248,\n",
       " 'dispose': 249,\n",
       " 'retard': 250,\n",
       " 'debout': 251,\n",
       " 'aije': 252,\n",
       " 'essaye': 253,\n",
       " 'main': 254,\n",
       " 'quelque': 255,\n",
       " 'raison': 256,\n",
       " 'netait': 257,\n",
       " 'pouvons': 258,\n",
       " 'tard': 259,\n",
       " 'seule': 260,\n",
       " 'fou': 261,\n",
       " 'courir': 262,\n",
       " 'entendu': 263,\n",
       " 'vois': 264,\n",
       " 'manque': 265,\n",
       " 'sagit': 266,\n",
       " 'pense': 267,\n",
       " 'dis': 268,\n",
       " 'biere': 269,\n",
       " 'peur': 270,\n",
       " 'allons': 271,\n",
       " 'vis': 272,\n",
       " 'suisje': 273,\n",
       " 'passe': 274,\n",
       " 'facile': 275,\n",
       " 'homme': 276,\n",
       " 'serieux': 277,\n",
       " 'aussi': 278,\n",
       " 'continuez': 279,\n",
       " 'mauvais': 280,\n",
       " 'chat': 281,\n",
       " 'lire': 282,\n",
       " 'arme': 283,\n",
       " 'travailler': 284,\n",
       " 'peuxtu': 285,\n",
       " 'vient': 286,\n",
       " 'serai': 287,\n",
       " 'vos': 288,\n",
       " 'tort': 289,\n",
       " 'prendre': 290,\n",
       " 'probleme': 291,\n",
       " 'journee': 292,\n",
       " 'mary': 293,\n",
       " 'heureuse': 294,\n",
       " 'occupee': 295,\n",
       " 'pleurer': 296,\n",
       " 'tete': 297,\n",
       " 'bizarre': 298,\n",
       " 'marcher': 299,\n",
       " 'idiot': 300,\n",
       " 'vue': 301,\n",
       " 'fils': 302,\n",
       " 'toujours': 303,\n",
       " 'gentil': 304,\n",
       " 'ten': 305,\n",
       " 'cheveux': 306,\n",
       " 'chanter': 307,\n",
       " 'verre': 308,\n",
       " 'lecole': 309,\n",
       " 'merci': 310,\n",
       " 'par': 311,\n",
       " 'jirai': 312,\n",
       " 'joue': 313,\n",
       " 'presque': 314,\n",
       " 'pouvez': 315,\n",
       " 'ferme': 316,\n",
       " 'tot': 317,\n",
       " 'laissez': 318,\n",
       " 'essayer': 319,\n",
       " 'mourir': 320,\n",
       " 'dehors': 321,\n",
       " 'dormir': 322,\n",
       " 'prudent': 323,\n",
       " 'appele': 324,\n",
       " 'aux': 325,\n",
       " 'donnemoi': 326,\n",
       " 'venu': 327,\n",
       " 'prete': 328,\n",
       " 'entrer': 329,\n",
       " 'alle': 330,\n",
       " 'chaud': 331,\n",
       " 'femme': 332,\n",
       " 'et': 333,\n",
       " 'tire': 334,\n",
       " 'vieux': 335,\n",
       " 'age': 336,\n",
       " 'arrive': 337,\n",
       " 'avoir': 338,\n",
       " 'chapeau': 339,\n",
       " 'mis': 340,\n",
       " 'sommesnous': 341,\n",
       " 'netes': 342,\n",
       " 'enfants': 343,\n",
       " 'rester': 344,\n",
       " 'aucun': 345,\n",
       " 'sa': 346,\n",
       " 'chambre': 347,\n",
       " 'grande': 348,\n",
       " 'donne': 349,\n",
       " 'aide': 350,\n",
       " 'prudente': 351,\n",
       " 'laide': 352,\n",
       " 'lheure': 353,\n",
       " 'pouvezvous': 354,\n",
       " 'dessus': 355,\n",
       " 'sontils': 356,\n",
       " 'commence': 357,\n",
       " 'fois': 358,\n",
       " 'payer': 359,\n",
       " 'sortir': 360,\n",
       " 'linterieur': 361,\n",
       " 'attrape': 362,\n",
       " 'davantage': 363,\n",
       " 'veut': 364,\n",
       " 'dur': 365,\n",
       " 'plan': 366,\n",
       " 'pourrait': 367,\n",
       " 'gagner': 368,\n",
       " 'etions': 369,\n",
       " 'aucune': 370,\n",
       " 'oublie': 371,\n",
       " 'largent': 372,\n",
       " 'bus': 373,\n",
       " 'filles': 374,\n",
       " 'attends': 375,\n",
       " 'medecin': 376,\n",
       " 'estelle': 377,\n",
       " 'deja': 378,\n",
       " 'pouvonsnous': 379,\n",
       " 'voulons': 380,\n",
       " 'assis': 381,\n",
       " 'garde': 382,\n",
       " 'doit': 383,\n",
       " 'attendez': 384,\n",
       " 'egalement': 385,\n",
       " 'bras': 386,\n",
       " 'essayez': 387,\n",
       " 'leau': 388,\n",
       " 'sac': 389,\n",
       " 'donnezmoi': 390,\n",
       " 'quil': 391,\n",
       " 'lamour': 392,\n",
       " 'sympa': 393,\n",
       " 'voila': 394,\n",
       " 'casse': 395,\n",
       " 'place': 396,\n",
       " 'blague': 397,\n",
       " 'jeune': 398,\n",
       " 'my': 399,\n",
       " 'avance': 400,\n",
       " 'parfait': 401,\n",
       " 'devenu': 402,\n",
       " 'musique': 403,\n",
       " 'cafe': 404,\n",
       " 'narrive': 405,\n",
       " 'pars': 406,\n",
       " 'drole': 407,\n",
       " 'chiens': 408,\n",
       " 'meme': 409,\n",
       " 'netais': 410,\n",
       " 'riche': 411,\n",
       " 'plein': 412,\n",
       " 'plutot': 413,\n",
       " 'ville': 414,\n",
       " 'quitte': 415,\n",
       " 'cote': 416,\n",
       " 'quiconque': 417,\n",
       " 'mien': 418,\n",
       " 'nestce': 419,\n",
       " 'timide': 420,\n",
       " 'difficile': 421,\n",
       " 'garcon': 422,\n",
       " 'signe': 423,\n",
       " 'super': 424,\n",
       " 'desole': 425,\n",
       " 'telephone': 426,\n",
       " 'menti': 427,\n",
       " 'bois': 428,\n",
       " 'voyage': 429,\n",
       " 'touche': 430,\n",
       " 'paye': 431,\n",
       " 'mere': 432,\n",
       " 'naime': 433,\n",
       " 'pleure': 434,\n",
       " 'premier': 435,\n",
       " 'petit': 436,\n",
       " 'sagitil': 437,\n",
       " 'dejeuner': 438,\n",
       " 'content': 439,\n",
       " 'pied': 440,\n",
       " 'devons': 441,\n",
       " 'sontelles': 442,\n",
       " 'ecrit': 443,\n",
       " 'partie': 444,\n",
       " 'rouge': 445,\n",
       " 'poisson': 446,\n",
       " 'mauvaise': 447,\n",
       " 'compte': 448,\n",
       " 'detre': 449,\n",
       " 'soit': 450,\n",
       " 'enfant': 451,\n",
       " 'faux': 452,\n",
       " 'reveille': 453,\n",
       " 'connait': 454,\n",
       " 'peutetre': 455,\n",
       " 'nes': 456,\n",
       " 'fonctionne': 457,\n",
       " 'dieu': 458,\n",
       " 'occupes': 459,\n",
       " 'lentement': 460,\n",
       " 'arreter': 461,\n",
       " 'parole': 462,\n",
       " 'devrais': 463,\n",
       " 'japprecie': 464,\n",
       " 'nourriture': 465,\n",
       " 'nom': 466,\n",
       " 'sentis': 467,\n",
       " 'achete': 468,\n",
       " 'danser': 469,\n",
       " 'boston': 470,\n",
       " 'ecoute': 471,\n",
       " 'choix': 472,\n",
       " 'leve': 473,\n",
       " 'seconde': 474,\n",
       " 'moment': 475,\n",
       " 'rendu': 476,\n",
       " 'apprecie': 477,\n",
       " 'fille': 478,\n",
       " 'jespere': 479,\n",
       " 'faisle': 480,\n",
       " 'rire': 481,\n",
       " 'normal': 482,\n",
       " 'apporte': 483,\n",
       " 'horrible': 484,\n",
       " 'horreur': 485,\n",
       " 'libre': 486,\n",
       " 'faisons': 487,\n",
       " 'court': 488,\n",
       " 'rendre': 489,\n",
       " 'attendre': 490,\n",
       " 'quand': 491,\n",
       " 'the': 492,\n",
       " 'sors': 493,\n",
       " 'chanson': 494,\n",
       " 'sentie': 495,\n",
       " 'ferai': 496,\n",
       " 'carte': 497,\n",
       " 'javais': 498,\n",
       " 'rapide': 499,\n",
       " 'marrant': 500,\n",
       " 'faible': 501,\n",
       " 'chats': 502,\n",
       " 'photo': 503,\n",
       " 'invite': 504,\n",
       " 'trouver': 505,\n",
       " 'fus': 506,\n",
       " 'refuse': 507,\n",
       " 'toute': 508,\n",
       " 'combien': 509,\n",
       " 'fermez': 510,\n",
       " 'lait': 511,\n",
       " 'etiezvous': 512,\n",
       " 'retraite': 513,\n",
       " 'patron': 514,\n",
       " 'feu': 515,\n",
       " 'nuit': 516,\n",
       " 'folle': 517,\n",
       " 'cheval': 518,\n",
       " 'famille': 519,\n",
       " 'vote': 520,\n",
       " 'dites': 521,\n",
       " 'tue': 522,\n",
       " 'lavezvous': 523,\n",
       " 'sommeil': 524,\n",
       " 'vas': 525,\n",
       " 'ami': 526,\n",
       " 'pere': 527,\n",
       " 'rentre': 528,\n",
       " 'paresseux': 529,\n",
       " 'devraisje': 530,\n",
       " 'demain': 531,\n",
       " 'faisait': 532,\n",
       " 'crois': 533,\n",
       " 'pretes': 534,\n",
       " 'veuxtu': 535,\n",
       " 'lexterieur': 536,\n",
       " 'lastu': 537,\n",
       " 'ouvre': 538,\n",
       " 'cet': 539,\n",
       " 'cuisine': 540,\n",
       " 'entre': 541,\n",
       " 'honnete': 542,\n",
       " 'dargent': 543,\n",
       " 'lont': 544,\n",
       " 'fatiguee': 545,\n",
       " 'saoul': 546,\n",
       " 'haut': 547,\n",
       " 'desormais': 548,\n",
       " 'reve': 549,\n",
       " 'point': 550,\n",
       " 'stylo': 551,\n",
       " 'bouge': 552,\n",
       " 'lis': 553,\n",
       " 'oui': 554,\n",
       " 'prets': 555,\n",
       " 'crie': 556,\n",
       " 'heures': 557,\n",
       " 'incroyable': 558,\n",
       " 'change': 559,\n",
       " 'souvent': 560,\n",
       " 'apres': 561,\n",
       " 'amusant': 562,\n",
       " 'derriere': 563,\n",
       " 'semblez': 564,\n",
       " 'beau': 565,\n",
       " 'prepare': 566,\n",
       " 'ans': 567,\n",
       " 'taime': 568,\n",
       " 'pres': 569,\n",
       " 'faitesle': 570,\n",
       " 'honte': 571,\n",
       " 'regarder': 572,\n",
       " 'mienne': 573,\n",
       " 'trompe': 574,\n",
       " 'tombe': 575,\n",
       " 'cle': 576,\n",
       " 'sortez': 577,\n",
       " 'cours': 578,\n",
       " 'perdre': 579,\n",
       " 'fis': 580,\n",
       " 'genial': 581,\n",
       " 'note': 582,\n",
       " 'voulais': 583,\n",
       " 'occupees': 584,\n",
       " 'commande': 585,\n",
       " 'camp': 586,\n",
       " 'stupide': 587,\n",
       " 'chante': 588,\n",
       " 'dy': 589,\n",
       " 'reparer': 590,\n",
       " 'cesse': 591,\n",
       " 'comprends': 592,\n",
       " 'allonsy': 593,\n",
       " 'lavons': 594,\n",
       " 'mains': 595,\n",
       " 'assiedstoi': 596,\n",
       " 'sy': 597,\n",
       " 'naif': 598,\n",
       " 'vieille': 599,\n",
       " 'souviens': 600,\n",
       " 'velo': 601,\n",
       " 'jouer': 602,\n",
       " 'mentir': 603,\n",
       " 'jaimerais': 604,\n",
       " 'degage': 605,\n",
       " 'belle': 606,\n",
       " 'dici': 607,\n",
       " 'plaisir': 608,\n",
       " 'sauve': 609,\n",
       " 'piege': 610,\n",
       " 'boire': 611,\n",
       " 'appelle': 612,\n",
       " 'forme': 613,\n",
       " 'savoir': 614,\n",
       " 'piece': 615,\n",
       " 'reviens': 616,\n",
       " 'tour': 617,\n",
       " 'battre': 618,\n",
       " 'bonjour': 619,\n",
       " 'ri': 620,\n",
       " 'minute': 621,\n",
       " 'gentille': 622,\n",
       " 'fous': 623,\n",
       " 'loin': 624,\n",
       " 'voulezvous': 625,\n",
       " 'suffit': 626,\n",
       " 'gardez': 627,\n",
       " 'labas': 628,\n",
       " 'mignon': 629,\n",
       " 'saistu': 630,\n",
       " 'paix': 631,\n",
       " 'intelligent': 632,\n",
       " 'seuls': 633,\n",
       " 'pourrais': 634,\n",
       " 'ouvert': 635,\n",
       " 'tourne': 636,\n",
       " 'ses': 637,\n",
       " 'soin': 638,\n",
       " 'heros': 639,\n",
       " 'aveugle': 640,\n",
       " 'las': 641,\n",
       " 'mesure': 642,\n",
       " 'asseyezvous': 643,\n",
       " 'diner': 644,\n",
       " 'portes': 645,\n",
       " 'sera': 646,\n",
       " 'espoir': 647,\n",
       " 'tele': 648,\n",
       " 'chemin': 649,\n",
       " 'serais': 650,\n",
       " 'reussi': 651,\n",
       " 'naive': 652,\n",
       " 'deau': 653,\n",
       " 'bateau': 654,\n",
       " 'neige': 655,\n",
       " 'taider': 656,\n",
       " 'service': 657,\n",
       " 'navons': 658,\n",
       " 'courant': 659,\n",
       " 'chanceux': 660,\n",
       " 'amis': 661,\n",
       " 'taxi': 662,\n",
       " 'hurler': 663,\n",
       " 'sembles': 664,\n",
       " 'vus': 665,\n",
       " 'nerveux': 666,\n",
       " 'droite': 667,\n",
       " 'contente': 668,\n",
       " 'verifie': 669,\n",
       " 'satisfait': 670,\n",
       " 'forte': 671,\n",
       " 'droit': 672,\n",
       " 'sucre': 673,\n",
       " 'sincere': 674,\n",
       " 'prenons': 675,\n",
       " 'vers': 676,\n",
       " 'aimons': 677,\n",
       " 'vit': 678,\n",
       " 'emploi': 679,\n",
       " 'contrarie': 680,\n",
       " 'daller': 681,\n",
       " 'meilleur': 682,\n",
       " 'chouette': 683,\n",
       " 'sentait': 684,\n",
       " 'souri': 685,\n",
       " 'dormi': 686,\n",
       " 'devez': 687,\n",
       " 'vont': 688,\n",
       " 'verrouille': 689,\n",
       " 'quastu': 690,\n",
       " 'pieds': 691,\n",
       " 'prefere': 692,\n",
       " 'maider': 693,\n",
       " 'histoire': 694,\n",
       " 'seules': 695,\n",
       " 'guerre': 696,\n",
       " 'nimporte': 697,\n",
       " 'amoureux': 698,\n",
       " 'soif': 699,\n",
       " 'continuait': 700,\n",
       " 'disparu': 701,\n",
       " 'morte': 702,\n",
       " 'cuisiner': 703,\n",
       " 'actuellement': 704,\n",
       " 'important': 705,\n",
       " 'etais': 706,\n",
       " 'heure': 707,\n",
       " 'curieux': 708,\n",
       " 'pue': 709,\n",
       " 'flics': 710,\n",
       " 'cloche': 711,\n",
       " 'etaient': 712,\n",
       " 'morts': 713,\n",
       " 'peuvent': 714,\n",
       " 'mit': 715,\n",
       " 'fit': 716,\n",
       " 'tiens': 717,\n",
       " 'malheureux': 718,\n",
       " 'sure': 719,\n",
       " 'monte': 720,\n",
       " 'allezvous': 721,\n",
       " 'sortie': 722,\n",
       " 'dingue': 723,\n",
       " 'promis': 724,\n",
       " 'voix': 725,\n",
       " 'compris': 726,\n",
       " 'pouvais': 727,\n",
       " 'trahi': 728,\n",
       " 'montre': 729,\n",
       " 'type': 730,\n",
       " 'parie': 731,\n",
       " 'etaitce': 732,\n",
       " 'grave': 733,\n",
       " 'conduire': 734,\n",
       " 'entendre': 735,\n",
       " 'chaussures': 736,\n",
       " 'petite': 737,\n",
       " 'contrariee': 738,\n",
       " 'salut': 739,\n",
       " 'bonnes': 740,\n",
       " 'sante': 741,\n",
       " 'erreur': 742,\n",
       " 'magnifique': 743,\n",
       " 'mechant': 744,\n",
       " 'retourne': 745,\n",
       " 'viande': 746,\n",
       " 'skier': 747,\n",
       " 'bientot': 748,\n",
       " 'cessez': 749,\n",
       " 'simple': 750,\n",
       " 'etaistu': 751,\n",
       " 'saute': 752,\n",
       " 'argent': 753,\n",
       " 'heureuses': 754,\n",
       " 'sombre': 755,\n",
       " 'reveillee': 756,\n",
       " 'serieuse': 757,\n",
       " 'savezvous': 758,\n",
       " 'boite': 759,\n",
       " 'avais': 760,\n",
       " 'bouger': 761,\n",
       " 'prudents': 762,\n",
       " 'vire': 763,\n",
       " 'rapidement': 764,\n",
       " 'allee': 765,\n",
       " 'ouvrez': 766,\n",
       " 'thomas': 767,\n",
       " 'desolee': 768,\n",
       " 'parlez': 769,\n",
       " 'amuse': 770,\n",
       " 'enerve': 771,\n",
       " 'autre': 772,\n",
       " 'baisse': 773,\n",
       " 'danger': 774,\n",
       " 'ainsi': 775,\n",
       " 'parviens': 776,\n",
       " 'laddition': 777,\n",
       " 'fallait': 778,\n",
       " 'silencieux': 779,\n",
       " 'fache': 780,\n",
       " 'demander': 781,\n",
       " 'oubliez': 782,\n",
       " 'rentrer': 783,\n",
       " 'cool': 784,\n",
       " 'avant': 785,\n",
       " 'lachemoi': 786,\n",
       " 'livres': 787,\n",
       " 'jours': 788,\n",
       " 'compter': 789,\n",
       " 'mont': 790,\n",
       " 'ceuxci': 791,\n",
       " 'arriere': 792,\n",
       " 'hommes': 793,\n",
       " 'vues': 794,\n",
       " 'endormi': 795,\n",
       " 'arriver': 796,\n",
       " 'entrez': 797,\n",
       " 'souci': 798,\n",
       " 'presente': 799,\n",
       " 'risque': 800,\n",
       " 'completement': 801,\n",
       " 'rever': 802,\n",
       " 'partez': 803,\n",
       " 'immediatement': 804,\n",
       " 'conduis': 805,\n",
       " 'formidable': 806,\n",
       " 'emporte': 807,\n",
       " 'continuer': 808,\n",
       " 'rhume': 809,\n",
       " 'peutil': 810,\n",
       " 'nez': 811,\n",
       " 'sent': 812,\n",
       " 'prudentes': 813,\n",
       " 'manteau': 814,\n",
       " 'darriver': 815,\n",
       " 'mavez': 816,\n",
       " 'ivre': 817,\n",
       " 'aimes': 818,\n",
       " 'etudiant': 819,\n",
       " 'pizza': 820,\n",
       " 'coupe': 821,\n",
       " 'demandez': 822,\n",
       " 'confectionne': 823,\n",
       " 'echoue': 824,\n",
       " 'sorti': 825,\n",
       " 'ensemble': 826,\n",
       " 'triche': 827,\n",
       " 'silence': 828,\n",
       " 'bu': 829,\n",
       " 'vole': 830,\n",
       " 'cur': 831,\n",
       " 'leur': 832,\n",
       " 'hais': 833,\n",
       " 'dangereux': 834,\n",
       " 'moimeme': 835,\n",
       " 'abandonne': 836,\n",
       " 'patient': 837,\n",
       " 'bain': 838,\n",
       " 'jusqua': 839,\n",
       " 'possede': 840,\n",
       " 'chatte': 841,\n",
       " 'armes': 842,\n",
       " 'retour': 843,\n",
       " 'aidemoi': 844,\n",
       " 'jambes': 845,\n",
       " 'sourire': 846,\n",
       " 'grossier': 847,\n",
       " 'propre': 848,\n",
       " 'pauvre': 849,\n",
       " 'excuses': 850,\n",
       " 'amoureuse': 851,\n",
       " 'menteur': 852,\n",
       " 'faute': 853,\n",
       " 'savons': 854,\n",
       " 'envoye': 855,\n",
       " 'detendu': 856,\n",
       " 'dingues': 857,\n",
       " 'mots': 858,\n",
       " 'commencez': 859,\n",
       " 'laime': 860,\n",
       " 'doucement': 861,\n",
       " 'ufs': 862,\n",
       " 'film': 863,\n",
       " 'changer': 864,\n",
       " 'noir': 865,\n",
       " 'pardonne': 866,\n",
       " 'affaire': 867,\n",
       " 'nouvelles': 868,\n",
       " 'montrer': 869,\n",
       " 'quavezvous': 870,\n",
       " 'mas': 871,\n",
       " 'merite': 872,\n",
       " 'coupable': 873,\n",
       " 'secret': 874,\n",
       " 'amusee': 875,\n",
       " 'chaise': 876,\n",
       " 'suppose': 877,\n",
       " 'liste': 878,\n",
       " 'frappe': 879,\n",
       " 'jentends': 880,\n",
       " 'semaine': 881,\n",
       " 'cen': 882,\n",
       " 'bete': 883,\n",
       " 'file': 884,\n",
       " 'sauves': 885,\n",
       " 'jarrive': 886,\n",
       " 'pitie': 887,\n",
       " 'ouverte': 888,\n",
       " 'excuse': 889,\n",
       " 'drogue': 890,\n",
       " 'proximite': 891,\n",
       " 'verite': 892,\n",
       " 'cles': 893,\n",
       " 'gateau': 894,\n",
       " 'jour': 895,\n",
       " 'decide': 896,\n",
       " 'loi': 897,\n",
       " 'mec': 898,\n",
       " 'laissee': 899,\n",
       " 'detendstoi': 900,\n",
       " 'fumer': 901,\n",
       " 'folles': 902,\n",
       " 'plaisante': 903,\n",
       " 'veulent': 904,\n",
       " 'etudie': 905,\n",
       " 'fallu': 906,\n",
       " 'jetez': 907,\n",
       " 'amie': 908,\n",
       " 'creve': 909,\n",
       " 'professeur': 910,\n",
       " 'grosse': 911,\n",
       " 'accord': 912,\n",
       " 'route': 913,\n",
       " 'magasin': 914,\n",
       " 'portait': 915,\n",
       " 'blessee': 916,\n",
       " 'attentivement': 917,\n",
       " 'miennes': 918,\n",
       " 'reponse': 919,\n",
       " 'mangez': 920,\n",
       " 'buvez': 921,\n",
       " 'clair': 922,\n",
       " 'rencontre': 923,\n",
       " 'descends': 924,\n",
       " 'repondu': 925,\n",
       " 'pardon': 926,\n",
       " 'maman': 927,\n",
       " 'daide': 928,\n",
       " 'fantastique': 929,\n",
       " 'surpris': 930,\n",
       " 'mets': 931,\n",
       " 'neuf': 932,\n",
       " 'amies': 933,\n",
       " 'letage': 934,\n",
       " 'douche': 935,\n",
       " 'quun': 936,\n",
       " 'envie': 937,\n",
       " 'prix': 938,\n",
       " 'maniere': 939,\n",
       " 'dattendre': 940,\n",
       " 'quittez': 941,\n",
       " 'bureau': 942,\n",
       " 'lave': 943,\n",
       " 'taije': 944,\n",
       " 'surs': 945,\n",
       " 'donner': 946,\n",
       " 'sentais': 947,\n",
       " 'souris': 948,\n",
       " 'semblait': 949,\n",
       " 'nul': 950,\n",
       " 'courses': 951,\n",
       " 'mise': 952,\n",
       " 'etaitil': 953,\n",
       " 'nouvelle': 954,\n",
       " 'rappelle': 955,\n",
       " 'nu': 956,\n",
       " 'tenez': 957,\n",
       " 'balle': 958,\n",
       " 'pause': 959,\n",
       " 'fallut': 960,\n",
       " 'regle': 961,\n",
       " 'avocat': 962,\n",
       " 'verifier': 963,\n",
       " 'dure': 964,\n",
       " 'taille': 965,\n",
       " 'rit': 966,\n",
       " 'possible': 967,\n",
       " 'ment': 968,\n",
       " 'vatil': 969,\n",
       " 'tien': 970,\n",
       " 'reposer': 971,\n",
       " 'idiote': 972,\n",
       " 'depecher': 973,\n",
       " 'souhaite': 974,\n",
       " 'parfaite': 975,\n",
       " 'artiste': 976,\n",
       " 'mille': 977,\n",
       " 'seulement': 978,\n",
       " 'descendez': 979,\n",
       " 'fenetre': 980,\n",
       " 'trente': 981,\n",
       " 'celibataire': 982,\n",
       " 'rends': 983,\n",
       " 'quest': 984,\n",
       " 'miens': 985,\n",
       " 'appelez': 986,\n",
       " 'motive': 987,\n",
       " 'tenu': 988,\n",
       " 'lachezmoi': 989,\n",
       " 'allume': 990,\n",
       " 'ah': 991,\n",
       " 'sonne': 992,\n",
       " 'fete': 993,\n",
       " 'quon': 994,\n",
       " 'volontaire': 995,\n",
       " 'bons': 996,\n",
       " 'equipe': 997,\n",
       " 'meilleure': 998,\n",
       " 'decampe': 999,\n",
       " 'marre': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word index of French tokenizer\n",
    "french_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the training dataset\n",
    "x_train = encode_sequences(french_tokenizer, french_maximum_length, train[:, 1])\n",
    "y_train = encode_sequences(eng_tokenizer, english_maximum_length, train[:, 0])\n",
    "y_train = encode_output(y_train, english_vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the validation dataset\n",
    "x_valid = encode_sequences(french_tokenizer, french_maximum_length, valid[:, 1])\n",
    "y_valid = encode_sequences(eng_tokenizer, english_maximum_length, valid[:, 0])\n",
    "y_valid = encode_output(y_valid, english_vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building testing dataset\n",
    "x_test = encode_sequences(french_tokenizer, french_maximum_length, test[:, 1])\n",
    "y_test = encode_sequences(eng_tokenizer, english_maximum_length, test[:, 0])\n",
    "y_test = encode_output(y_test, english_vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Both Encoder GRU and Decoder GRU cell\n",
    "def both_gru(french_vocab, english_vocab, french_timesteps, english_timesteps, num_units):\n",
    "\tlearning_rate = 1e-2\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Embedding(french_vocab, num_units, input_length = french_timesteps, mask_zero = True))\n",
    "\tmodel.add(GRU(num_units))\n",
    "\tmodel.add(RepeatVector(english_timesteps))\n",
    "\tmodel.add(GRU(num_units,return_sequences=True))\n",
    "\tmodel.add(TimeDistributed(Dense(english_vocab, activation='softmax')))\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Model-1\n",
    "\n",
    "learning_rate = 1e-2\n",
    "both_gru = both_gru(french_vocabulary_size, english_vocabulary_size, french_maximum_length, english_maximum_length, 256)\n",
    "both_gru.compile(optimizer = Adam(learning_rate), loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 12, 256)           2053376   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 256)               394752    \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 5, 256)            394752    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 5, 3949)           1014893   \n",
      "=================================================================\n",
      "Total params: 3,857,773\n",
      "Trainable params: 3,857,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "# Summary of the Model-1\n",
    "\n",
    "print(both_gru.summary())\n",
    "plot_model(both_gru, to_file='gru_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "274/274 - 43s - loss: 3.8243 - categorical_accuracy: 0.4425 - val_loss: 3.2333 - val_categorical_accuracy: 0.5005\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.23328, saving model to both_gru.h5\n",
      "Epoch 2/30\n",
      "274/274 - 23s - loss: 2.8783 - categorical_accuracy: 0.5361 - val_loss: 2.7792 - val_categorical_accuracy: 0.5606\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.23328 to 2.77923, saving model to both_gru.h5\n",
      "Epoch 3/30\n",
      "274/274 - 23s - loss: 2.3850 - categorical_accuracy: 0.5819 - val_loss: 2.5848 - val_categorical_accuracy: 0.5777\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.77923 to 2.58478, saving model to both_gru.h5\n",
      "Epoch 4/30\n",
      "274/274 - 23s - loss: 2.0623 - categorical_accuracy: 0.6075 - val_loss: 2.4162 - val_categorical_accuracy: 0.5967\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.58478 to 2.41619, saving model to both_gru.h5\n",
      "Epoch 5/30\n",
      "274/274 - 23s - loss: 1.7980 - categorical_accuracy: 0.6337 - val_loss: 2.3461 - val_categorical_accuracy: 0.6007\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.41619 to 2.34607, saving model to both_gru.h5\n",
      "Epoch 6/30\n",
      "274/274 - 23s - loss: 1.5926 - categorical_accuracy: 0.6549 - val_loss: 2.2943 - val_categorical_accuracy: 0.6141\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.34607 to 2.29431, saving model to both_gru.h5\n",
      "Epoch 7/30\n",
      "274/274 - 23s - loss: 1.4281 - categorical_accuracy: 0.6753 - val_loss: 2.2745 - val_categorical_accuracy: 0.6162\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.29431 to 2.27447, saving model to both_gru.h5\n",
      "Epoch 8/30\n",
      "274/274 - 23s - loss: 1.2948 - categorical_accuracy: 0.6962 - val_loss: 2.2713 - val_categorical_accuracy: 0.6178\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.27447 to 2.27131, saving model to both_gru.h5\n",
      "Epoch 9/30\n",
      "274/274 - 23s - loss: 1.1869 - categorical_accuracy: 0.7120 - val_loss: 2.2383 - val_categorical_accuracy: 0.6245\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.27131 to 2.23828, saving model to both_gru.h5\n",
      "Epoch 10/30\n",
      "274/274 - 23s - loss: 1.0771 - categorical_accuracy: 0.7310 - val_loss: 2.2319 - val_categorical_accuracy: 0.6279\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.23828 to 2.23190, saving model to both_gru.h5\n",
      "Epoch 11/30\n",
      "274/274 - 23s - loss: 1.0008 - categorical_accuracy: 0.7459 - val_loss: 2.2528 - val_categorical_accuracy: 0.6272\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.23190\n",
      "Epoch 12/30\n",
      "274/274 - 23s - loss: 0.9609 - categorical_accuracy: 0.7534 - val_loss: 2.2765 - val_categorical_accuracy: 0.6305\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.23190\n",
      "Epoch 13/30\n",
      "274/274 - 23s - loss: 0.9051 - categorical_accuracy: 0.7662 - val_loss: 2.3017 - val_categorical_accuracy: 0.6318\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.23190\n",
      "Epoch 14/30\n",
      "274/274 - 23s - loss: 0.8494 - categorical_accuracy: 0.7757 - val_loss: 2.2804 - val_categorical_accuracy: 0.6335\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.23190\n",
      "Epoch 15/30\n",
      "274/274 - 23s - loss: 0.8074 - categorical_accuracy: 0.7846 - val_loss: 2.2993 - val_categorical_accuracy: 0.6374\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 2.23190\n",
      "Epoch 16/30\n",
      "274/274 - 23s - loss: 0.7749 - categorical_accuracy: 0.7905 - val_loss: 2.3362 - val_categorical_accuracy: 0.6358\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 2.23190\n",
      "Epoch 17/30\n",
      "274/274 - 23s - loss: 0.7589 - categorical_accuracy: 0.7948 - val_loss: 2.3584 - val_categorical_accuracy: 0.6355\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 2.23190\n",
      "Epoch 18/30\n",
      "274/274 - 23s - loss: 0.7468 - categorical_accuracy: 0.7981 - val_loss: 2.3801 - val_categorical_accuracy: 0.6342\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 2.23190\n",
      "Epoch 19/30\n",
      "274/274 - 23s - loss: 0.7375 - categorical_accuracy: 0.7959 - val_loss: 2.3762 - val_categorical_accuracy: 0.6398\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 2.23190\n",
      "Epoch 20/30\n",
      "274/274 - 23s - loss: 0.7185 - categorical_accuracy: 0.8009 - val_loss: 2.3850 - val_categorical_accuracy: 0.6424\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 2.23190\n",
      "Epoch 21/30\n",
      "274/274 - 23s - loss: 0.7111 - categorical_accuracy: 0.8023 - val_loss: 2.3984 - val_categorical_accuracy: 0.6357\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 2.23190\n",
      "Epoch 22/30\n",
      "274/274 - 25s - loss: 0.6989 - categorical_accuracy: 0.8052 - val_loss: 2.4078 - val_categorical_accuracy: 0.6402\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 2.23190\n",
      "Epoch 23/30\n",
      "274/274 - 24s - loss: 0.6765 - categorical_accuracy: 0.8106 - val_loss: 2.4123 - val_categorical_accuracy: 0.6363\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 2.23190\n",
      "Epoch 24/30\n",
      "274/274 - 23s - loss: 0.6613 - categorical_accuracy: 0.8136 - val_loss: 2.4301 - val_categorical_accuracy: 0.6344\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 2.23190\n",
      "Epoch 25/30\n",
      "274/274 - 23s - loss: 0.6596 - categorical_accuracy: 0.8136 - val_loss: 2.4384 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 2.23190\n",
      "Epoch 26/30\n",
      "274/274 - 23s - loss: 0.6517 - categorical_accuracy: 0.8169 - val_loss: 2.4442 - val_categorical_accuracy: 0.6362\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 2.23190\n",
      "Epoch 27/30\n",
      "274/274 - 23s - loss: 0.6462 - categorical_accuracy: 0.8174 - val_loss: 2.4587 - val_categorical_accuracy: 0.6369\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 2.23190\n",
      "Epoch 28/30\n",
      "274/274 - 22s - loss: 0.6540 - categorical_accuracy: 0.8145 - val_loss: 2.4795 - val_categorical_accuracy: 0.6381\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 2.23190\n",
      "Epoch 29/30\n",
      "274/274 - 21s - loss: 0.6314 - categorical_accuracy: 0.8210 - val_loss: 2.5274 - val_categorical_accuracy: 0.6351\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 2.23190\n",
      "Epoch 30/30\n",
      "274/274 - 21s - loss: 0.6292 - categorical_accuracy: 0.8214 - val_loss: 2.5146 - val_categorical_accuracy: 0.6355\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 2.23190\n"
     ]
    }
   ],
   "source": [
    "# Fitting the Model-1\n",
    "filename = 'both_gru.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "history_both_gru = both_gru.fit(x_train, y_train, epochs = 30, batch_size = 64, validation_data=(x_valid, y_valid), callbacks = [checkpoint], verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        loss  categorical_accuracy  val_loss  val_categorical_accuracy\n",
      "0   3.824321              0.442469  3.233278                   0.50048\n",
      "1   2.878268              0.536149  2.779230                   0.56064\n",
      "2   2.385043              0.581851  2.584778                   0.57768\n",
      "3   2.062311              0.607543  2.416189                   0.59672\n",
      "4   1.797962              0.633657  2.346069                   0.60072\n",
      "5   1.592628              0.654857  2.294309                   0.61408\n",
      "6   1.428137              0.675337  2.274472                   0.61616\n",
      "7   1.294781              0.696229  2.271312                   0.61776\n",
      "8   1.186878              0.711989  2.238282                   0.62448\n",
      "9   1.077078              0.731029  2.231904                   0.62792\n",
      "10  1.000781              0.745874  2.252761                   0.62720\n",
      "11  0.960913              0.753394  2.276547                   0.63048\n",
      "12  0.905064              0.766183  2.301702                   0.63184\n",
      "13  0.849388              0.775680  2.280392                   0.63352\n",
      "14  0.807384              0.784617  2.299325                   0.63744\n",
      "15  0.774924              0.790549  2.336208                   0.63576\n",
      "16  0.758930              0.794823  2.358442                   0.63552\n",
      "17  0.746780              0.798126  2.380147                   0.63424\n",
      "18  0.737528              0.795897  2.376196                   0.63984\n",
      "19  0.718454              0.800903  2.384973                   0.64240\n",
      "20  0.711106              0.802331  2.398432                   0.63568\n",
      "21  0.698877              0.805189  2.407824                   0.64024\n",
      "22  0.676536              0.810560  2.412288                   0.63632\n",
      "23  0.661329              0.813566  2.430143                   0.63440\n",
      "24  0.659575              0.813634  2.438431                   0.64000\n",
      "25  0.651705              0.816857  2.444200                   0.63616\n",
      "26  0.646221              0.817383  2.458708                   0.63688\n",
      "27  0.654013              0.814526  2.479528                   0.63808\n",
      "28  0.631368              0.820971  2.527394                   0.63512\n",
      "29  0.629233              0.821394  2.514598                   0.63552\n"
     ]
    }
   ],
   "source": [
    "# Saving the history of the Model-1\n",
    "\n",
    "df_history_both_gru = pd.DataFrame(history_both_gru.history)\n",
    "df_history_both_gru.to_csv('df_history_both_gru.csv')\n",
    "print(df_history_both_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Model-1\n",
    "both_gru = load_model('both_gru.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French(Source) : appretetoi\n",
      "Target : prepare yourself\n",
      "Predicted : sorry yourself \n",
      "\n",
      "French(Source) : avezvous pris une douche\n",
      "Target : have you showered\n",
      "Predicted : did you showered \n",
      "\n",
      "French(Source) : rentre a la maison\n",
      "Target : go home\n",
      "Predicted : come home home \n",
      "\n",
      "French(Source) : ils ont tort\n",
      "Target : they are wrong\n",
      "Predicted : they wrong cool \n",
      "\n",
      "French(Source) : aidemoi a sortir\n",
      "Target : help me out\n",
      "Predicted : show me out \n",
      "\n",
      "French(Source) : prenez les commandes\n",
      "Target : take command\n",
      "Predicted : take command \n",
      "\n",
      "French(Source) : restez ou vous etes\n",
      "Target : hold it\n",
      "Predicted : close everyone \n",
      "\n",
      "French(Source) : je suis ruinee\n",
      "Target : im ruined\n",
      "Predicted : im ruined \n",
      "\n",
      "French(Source) : je ne suis pas malheureux\n",
      "Target : im not unhappy\n",
      "Predicted : im not impressed \n",
      "\n",
      "French(Source) : je peux y parvenir\n",
      "Target : i can do it\n",
      "Predicted : i can do it \n",
      "\n",
      "French(Source) : jaime tes jambes\n",
      "Target : i like your legs\n",
      "Predicted : i like your legs \n",
      "\n",
      "French(Source) : maitrisezvous\n",
      "Target : control yourself\n",
      "Predicted : control yourself \n",
      "\n",
      "French(Source) : je suis a moitie japonaise\n",
      "Target : im half japanese\n",
      "Predicted : i came deaf \n",
      "\n",
      "French(Source) : en estu sure\n",
      "Target : are you certain\n",
      "Predicted : are you certain \n",
      "\n",
      "French(Source) : jaime ton sourire\n",
      "Target : i like your smile\n",
      "Predicted : like your your son \n",
      "\n",
      "French(Source) : klaxonnez\n",
      "Target : honk the horn\n",
      "Predicted : honk the horn \n",
      "\n",
      "French(Source) : tu es grossier\n",
      "Target : youre rude\n",
      "Predicted : youre impatient \n",
      "\n",
      "French(Source) : nous devrions partir\n",
      "Target : we should go\n",
      "Predicted : we we to go \n",
      "\n",
      "French(Source) : jai oublie la carte\n",
      "Target : i forgot the map\n",
      "Predicted : i drawn the map \n",
      "\n",
      "French(Source) : ils font des efforts\n",
      "Target : theyre trying\n",
      "Predicted : theyre staying \n",
      "\n",
      "French(Source) : tom sen allait\n",
      "Target : tom was leaving\n",
      "Predicted : tom seemed you \n",
      "\n",
      "French(Source) : ne me parle pas\n",
      "Target : dont talk to me\n",
      "Predicted : dont talk me \n",
      "\n",
      "French(Source) : tom etait ici\n",
      "Target : tom was here\n",
      "Predicted : tom was here \n",
      "\n",
      "French(Source) : jai termine premier\n",
      "Target : i finished first\n",
      "Predicted : im first first \n",
      "\n",
      "French(Source) : elle garde des secrets\n",
      "Target : she keeps secrets\n",
      "Predicted : she keeps secrets \n",
      "\n",
      "French(Source) : mordstoi la langue\n",
      "Target : bite your tongue\n",
      "Predicted : bite your tongue \n",
      "\n",
      "French(Source) : je veux faire ceci\n",
      "Target : i want to do this\n",
      "Predicted : i want to do this \n",
      "\n",
      "French(Source) : vous etes les vainqueurs\n",
      "Target : youve won\n",
      "Predicted : youre first \n",
      "\n",
      "French(Source) : inutile de discuter\n",
      "Target : dont argue\n",
      "Predicted : dont argue \n",
      "\n",
      "French(Source) : jai besoin dun taxi\n",
      "Target : i need a cab\n",
      "Predicted : i need a cab \n",
      "\n",
      "French(Source) : je nai pas dexcuse\n",
      "Target : i have no excuse\n",
      "Predicted : no dont no no \n",
      "\n",
      "French(Source) : je peux la reparer\n",
      "Target : i can fix it\n",
      "Predicted : i can do it \n",
      "\n",
      "French(Source) : je trouvais ce vin bon\n",
      "Target : i liked that wine\n",
      "Predicted : i saw that \n",
      "\n",
      "French(Source) : cesse de hurler\n",
      "Target : stop screaming\n",
      "Predicted : stop screaming \n",
      "\n",
      "French(Source) : je resterai debout\n",
      "Target : ill stand\n",
      "Predicted : ill protect down \n",
      "\n",
      "French(Source) : jaime le rock\n",
      "Target : i like rock music\n",
      "Predicted : i like rock city \n",
      "\n",
      "French(Source) : tom est juste\n",
      "Target : tom is fair\n",
      "Predicted : tom is \n",
      "\n",
      "French(Source) : regardez a nouveau\n",
      "Target : look again\n",
      "Predicted : look again again \n",
      "\n",
      "French(Source) : je protestais\n",
      "Target : i objected\n",
      "Predicted : i protested \n",
      "\n",
      "French(Source) : nous sommes au regime\n",
      "Target : were dieting\n",
      "Predicted : were dieting \n",
      "\n",
      "French(Source) : vous etes tous heureux\n",
      "Target : youre all happy\n",
      "Predicted : youre all happy \n",
      "\n",
      "French(Source) : je suis connue\n",
      "Target : im famous\n",
      "Predicted : im famous \n",
      "\n",
      "French(Source) : etesvous blesses\n",
      "Target : are you injured\n",
      "Predicted : are you injured \n",
      "\n",
      "French(Source) : fais un calin a tom\n",
      "Target : hug tom\n",
      "Predicted : hug a tom \n",
      "\n",
      "French(Source) : astu un animal domestique\n",
      "Target : do you have a pet\n",
      "Predicted : do you have me \n",
      "\n",
      "French(Source) : tom doit rester\n",
      "Target : tom has to stay\n",
      "Predicted : tom can stay stay \n",
      "\n",
      "French(Source) : ou est la voiture\n",
      "Target : wheres the car\n",
      "Predicted : wheres the car \n",
      "\n",
      "French(Source) : deverrouillele\n",
      "Target : unlock it\n",
      "Predicted : unlock it \n",
      "\n",
      "French(Source) : je paierai\n",
      "Target : ill pay\n",
      "Predicted : ill pay again \n",
      "\n",
      "French(Source) : monte au sommet\n",
      "Target : climb to the top\n",
      "Predicted : get your the \n",
      "\n",
      "BLEU score: 0.266727\n"
     ]
    }
   ],
   "source": [
    "#  Model-1 on the training sequences\n",
    "\n",
    "bleu_train_both_gru = model_evaluation(both_gru, eng_tokenizer, x_train, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French(Source) : jai oublie le livre\n",
      "Target : i forgot the book\n",
      "Predicted : i bought the book \n",
      "\n",
      "French(Source) : tom est sans domicile fixe\n",
      "Target : toms homeless\n",
      "Predicted : tom fell \n",
      "\n",
      "French(Source) : jaide tom\n",
      "Target : i help tom\n",
      "Predicted : im helping tom \n",
      "\n",
      "French(Source) : jai achete un cactus\n",
      "Target : i bought a cactus\n",
      "Predicted : i bought a stroke \n",
      "\n",
      "French(Source) : cest plutot grand\n",
      "Target : its quite large\n",
      "Predicted : this is big \n",
      "\n",
      "French(Source) : ouvrez le coffre\n",
      "Target : open the safe\n",
      "Predicted : get the \n",
      "\n",
      "French(Source) : il est trop confiant\n",
      "Target : hes too trusting\n",
      "Predicted : hes too too \n",
      "\n",
      "French(Source) : il adore les jouets\n",
      "Target : he loves toys\n",
      "Predicted : he likes oranges \n",
      "\n",
      "French(Source) : estce que vous avez le temps\n",
      "Target : do you have time\n",
      "Predicted : do i try time \n",
      "\n",
      "French(Source) : jai presente mes excuses\n",
      "Target : i have apologized\n",
      "Predicted : i got my army \n",
      "\n",
      "French(Source) : il nous faut de leau\n",
      "Target : we need water\n",
      "Predicted : we had a water \n",
      "\n",
      "French(Source) : cest simple nestce pas\n",
      "Target : simple isnt it\n",
      "Predicted : its not \n",
      "\n",
      "French(Source) : nous allons camper ici\n",
      "Target : well camp here\n",
      "Predicted : well going here \n",
      "\n",
      "French(Source) : oublie tom\n",
      "Target : never mind tom\n",
      "Predicted : let tom \n",
      "\n",
      "French(Source) : jaime les recits\n",
      "Target : i like stories\n",
      "Predicted : i like clocks dogs \n",
      "\n",
      "French(Source) : etesvous enseignante\n",
      "Target : are you a teacher\n",
      "Predicted : are you a \n",
      "\n",
      "French(Source) : cest la norme\n",
      "Target : this is the norm\n",
      "Predicted : this hers reality \n",
      "\n",
      "French(Source) : nous ne sommes pas sures\n",
      "Target : were not sure\n",
      "Predicted : were not \n",
      "\n",
      "French(Source) : ca a de limportance pour moi\n",
      "Target : it matters to me\n",
      "Predicted : it it for \n",
      "\n",
      "French(Source) : estu maniaque\n",
      "Target : are you a maniac\n",
      "Predicted : are you awake \n",
      "\n",
      "French(Source) : je dois mechapper\n",
      "Target : i have to escape\n",
      "Predicted : i must to \n",
      "\n",
      "French(Source) : arrete ca maintenant\n",
      "Target : stop that now\n",
      "Predicted : please that \n",
      "\n",
      "French(Source) : estu epuise\n",
      "Target : are you exhausted\n",
      "Predicted : are you sure \n",
      "\n",
      "French(Source) : attendez\n",
      "Target : wait\n",
      "Predicted : hold on minute minute minute \n",
      "\n",
      "French(Source) : il est sexy\n",
      "Target : hes sexy\n",
      "Predicted : he is bald \n",
      "\n",
      "French(Source) : etesvous droguee\n",
      "Target : are you on dope\n",
      "Predicted : are you drink \n",
      "\n",
      "French(Source) : jirai me changer\n",
      "Target : ill go change\n",
      "Predicted : but go go \n",
      "\n",
      "French(Source) : il est intelligent\n",
      "Target : he is intelligent\n",
      "Predicted : hes smart \n",
      "\n",
      "French(Source) : ils le connaissent\n",
      "Target : they know him\n",
      "Predicted : they know know \n",
      "\n",
      "French(Source) : lastu emporte\n",
      "Target : did you win\n",
      "Predicted : did you remember it \n",
      "\n",
      "French(Source) : nous avons pris le petit dejeuner\n",
      "Target : we had breakfast\n",
      "Predicted : we were breakfast \n",
      "\n",
      "French(Source) : personne na ete vire\n",
      "Target : no one was fired\n",
      "Predicted : no one was shy \n",
      "\n",
      "French(Source) : elle est maladroite\n",
      "Target : she is awkward\n",
      "Predicted : she is unsociable \n",
      "\n",
      "French(Source) : tom a ralenti\n",
      "Target : tom slowed down\n",
      "Predicted : tom hates \n",
      "\n",
      "French(Source) : estu seul\n",
      "Target : are you single\n",
      "Predicted : are you alone \n",
      "\n",
      "French(Source) : elles sont formidables\n",
      "Target : theyre great\n",
      "Predicted : theyre great \n",
      "\n",
      "French(Source) : sil te plait chante\n",
      "Target : please sing\n",
      "Predicted : please still \n",
      "\n",
      "French(Source) : astu fait ceci\n",
      "Target : did you make this\n",
      "Predicted : did you fix this \n",
      "\n",
      "French(Source) : sautez de lautre cote\n",
      "Target : jump across\n",
      "Predicted : grab something \n",
      "\n",
      "French(Source) : tu es si naive\n",
      "Target : youre so naive\n",
      "Predicted : youre so naive \n",
      "\n",
      "French(Source) : viens jouer avec nous\n",
      "Target : come play with us\n",
      "Predicted : come with us \n",
      "\n",
      "French(Source) : enfile ton pantalon\n",
      "Target : put your pants on\n",
      "Predicted : put your mask \n",
      "\n",
      "French(Source) : nous sommes tous en train de mourir\n",
      "Target : were all dying\n",
      "Predicted : were all die \n",
      "\n",
      "French(Source) : nous ne sommes pas perdues\n",
      "Target : were not lost\n",
      "Predicted : were not not \n",
      "\n",
      "French(Source) : jen veux un\n",
      "Target : i want one\n",
      "Predicted : i want a one \n",
      "\n",
      "French(Source) : jetais dans la voiture\n",
      "Target : i was in the car\n",
      "Predicted : i was in car car \n",
      "\n",
      "French(Source) : sil vous plait soyez prudent\n",
      "Target : please be careful\n",
      "Predicted : please honest careful \n",
      "\n",
      "French(Source) : cest juste faux\n",
      "Target : its just wrong\n",
      "Predicted : thats is untrue \n",
      "\n",
      "French(Source) : offremoi un verre\n",
      "Target : buy me a drink\n",
      "Predicted : get me my \n",
      "\n",
      "French(Source) : ils apprecient le francais\n",
      "Target : they like french\n",
      "Predicted : they sweated french \n",
      "\n",
      "BLEU score: 0.112199\n"
     ]
    }
   ],
   "source": [
    "# Testing Model-1 \n",
    "\n",
    "bleu_test_both_gru = model_evaluation(both_gru, eng_tokenizer, x_test, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Both Encoder LSTM and Decoder LSTM cell\n",
    "def both_lstm(french_vocab, english_vocab, french_timesteps, english_timesteps, num_units):\n",
    "\tlearning_rate = 1e-2\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Embedding(french_vocab, num_units, input_length= french_timesteps, mask_zero=True))\n",
    "\tmodel.add(LSTM(num_units))\n",
    "\tmodel.add(RepeatVector(english_timesteps))\n",
    "\tmodel.add(LSTM(num_units,return_sequences=True))\n",
    "\tmodel.add(TimeDistributed(Dense(english_vocab, activation='softmax')))\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Model-2\n",
    "\n",
    "learning_rate = 1e-2\n",
    "both_lstm = both_lstm(french_vocabulary_size, english_vocabulary_size, french_maximum_length, english_maximum_length, 256)\n",
    "both_lstm.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 12, 256)           2053376   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 5, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 5, 3949)           1014893   \n",
      "=================================================================\n",
      "Total params: 4,118,893\n",
      "Trainable params: 4,118,893\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "# Summarising Model-2\n",
    "\n",
    "print(both_lstm.summary())\n",
    "plot_model(both_lstm, to_file='both_lstm.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "274/274 - 31s - loss: 3.7934 - categorical_accuracy: 0.4412 - val_loss: 3.2089 - val_categorical_accuracy: 0.4913\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.20892, saving model to both_lstm.h5\n",
      "Epoch 2/30\n",
      "274/274 - 25s - loss: 2.8336 - categorical_accuracy: 0.5330 - val_loss: 2.7117 - val_categorical_accuracy: 0.5574\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.20892 to 2.71173, saving model to both_lstm.h5\n",
      "Epoch 3/30\n",
      "274/274 - 25s - loss: 2.2936 - categorical_accuracy: 0.5840 - val_loss: 2.4521 - val_categorical_accuracy: 0.5902\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.71173 to 2.45209, saving model to both_lstm.h5\n",
      "Epoch 4/30\n",
      "274/274 - 769s - loss: 1.8956 - categorical_accuracy: 0.6229 - val_loss: 2.3167 - val_categorical_accuracy: 0.6019\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.45209 to 2.31671, saving model to both_lstm.h5\n",
      "Epoch 5/30\n",
      "274/274 - 25s - loss: 1.5794 - categorical_accuracy: 0.6595 - val_loss: 2.2362 - val_categorical_accuracy: 0.6199\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.31671 to 2.23620, saving model to both_lstm.h5\n",
      "Epoch 6/30\n",
      "274/274 - 28s - loss: 1.3346 - categorical_accuracy: 0.6934 - val_loss: 2.1759 - val_categorical_accuracy: 0.6288\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.23620 to 2.17593, saving model to both_lstm.h5\n",
      "Epoch 7/30\n",
      "274/274 - 26s - loss: 1.1236 - categorical_accuracy: 0.7263 - val_loss: 2.1586 - val_categorical_accuracy: 0.6362\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.17593 to 2.15860, saving model to both_lstm.h5\n",
      "Epoch 8/30\n",
      "274/274 - 33s - loss: 0.9724 - categorical_accuracy: 0.7542 - val_loss: 2.1545 - val_categorical_accuracy: 0.6474\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.15860 to 2.15450, saving model to both_lstm.h5\n",
      "Epoch 9/30\n",
      "274/274 - 34s - loss: 0.8494 - categorical_accuracy: 0.7797 - val_loss: 2.1608 - val_categorical_accuracy: 0.6474\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.15450\n",
      "Epoch 10/30\n",
      "274/274 - 36s - loss: 0.7466 - categorical_accuracy: 0.8005 - val_loss: 2.1802 - val_categorical_accuracy: 0.6488\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.15450\n",
      "Epoch 11/30\n",
      "274/274 - 34s - loss: 0.6604 - categorical_accuracy: 0.8189 - val_loss: 2.2096 - val_categorical_accuracy: 0.6504\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.15450\n",
      "Epoch 12/30\n",
      "274/274 - 33s - loss: 0.6095 - categorical_accuracy: 0.8324 - val_loss: 2.2292 - val_categorical_accuracy: 0.6530\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.15450\n",
      "Epoch 13/30\n",
      "274/274 - 33s - loss: 0.5611 - categorical_accuracy: 0.8417 - val_loss: 2.2512 - val_categorical_accuracy: 0.6549\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.15450\n",
      "Epoch 14/30\n",
      "274/274 - 33s - loss: 0.5190 - categorical_accuracy: 0.8522 - val_loss: 2.2720 - val_categorical_accuracy: 0.6524\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.15450\n",
      "Epoch 15/30\n",
      "274/274 - 33s - loss: 0.4881 - categorical_accuracy: 0.8606 - val_loss: 2.2770 - val_categorical_accuracy: 0.6546\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 2.15450\n",
      "Epoch 16/30\n",
      "274/274 - 31s - loss: 0.4679 - categorical_accuracy: 0.8632 - val_loss: 2.3320 - val_categorical_accuracy: 0.6545\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 2.15450\n",
      "Epoch 17/30\n",
      "274/274 - 33s - loss: 0.4656 - categorical_accuracy: 0.8634 - val_loss: 2.3432 - val_categorical_accuracy: 0.6536\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 2.15450\n",
      "Epoch 18/30\n",
      "274/274 - 33s - loss: 0.4383 - categorical_accuracy: 0.8695 - val_loss: 2.3938 - val_categorical_accuracy: 0.6544\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 2.15450\n",
      "Epoch 19/30\n",
      "274/274 - 31s - loss: 0.4231 - categorical_accuracy: 0.8745 - val_loss: 2.4048 - val_categorical_accuracy: 0.6591\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 2.15450\n",
      "Epoch 20/30\n",
      "274/274 - 30s - loss: 0.4058 - categorical_accuracy: 0.8779 - val_loss: 2.4055 - val_categorical_accuracy: 0.6552\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 2.15450\n",
      "Epoch 21/30\n",
      "274/274 - 33s - loss: 0.3905 - categorical_accuracy: 0.8820 - val_loss: 2.4287 - val_categorical_accuracy: 0.6559\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 2.15450\n",
      "Epoch 22/30\n",
      "274/274 - 33s - loss: 0.3696 - categorical_accuracy: 0.8888 - val_loss: 2.4474 - val_categorical_accuracy: 0.6554\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 2.15450\n",
      "Epoch 23/30\n",
      "274/274 - 33s - loss: 0.3707 - categorical_accuracy: 0.8871 - val_loss: 2.4807 - val_categorical_accuracy: 0.6498\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 2.15450\n",
      "Epoch 24/30\n",
      "274/274 - 34s - loss: 0.3735 - categorical_accuracy: 0.8854 - val_loss: 2.5191 - val_categorical_accuracy: 0.6526\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 2.15450\n",
      "Epoch 25/30\n",
      "274/274 - 34s - loss: 0.3756 - categorical_accuracy: 0.8852 - val_loss: 2.5261 - val_categorical_accuracy: 0.6481\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 2.15450\n",
      "Epoch 26/30\n",
      "274/274 - 33s - loss: 0.3746 - categorical_accuracy: 0.8849 - val_loss: 2.5355 - val_categorical_accuracy: 0.6458\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 2.15450\n",
      "Epoch 27/30\n",
      "274/274 - 33s - loss: 0.3797 - categorical_accuracy: 0.8836 - val_loss: 2.5589 - val_categorical_accuracy: 0.6446\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 2.15450\n",
      "Epoch 28/30\n",
      "274/274 - 33s - loss: 0.3779 - categorical_accuracy: 0.8837 - val_loss: 2.5585 - val_categorical_accuracy: 0.6485\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 2.15450\n",
      "Epoch 29/30\n",
      "274/274 - 33s - loss: 0.3715 - categorical_accuracy: 0.8852 - val_loss: 2.5911 - val_categorical_accuracy: 0.6461\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 2.15450\n",
      "Epoch 30/30\n",
      "274/274 - 34s - loss: 0.3688 - categorical_accuracy: 0.8861 - val_loss: 2.6058 - val_categorical_accuracy: 0.6464\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 2.15450\n"
     ]
    }
   ],
   "source": [
    "# Fitting the Model-2\n",
    "\n",
    "filename = 'both_lstm.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "history_both_lstm = both_lstm.fit(x_train, y_train, epochs=30, batch_size=64, validation_data=(x_valid, y_valid), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the history of the Model-2\n",
    "\n",
    "df_history_both_lstm = pd.DataFrame(history_both_lstm.history)\n",
    "df_history_both_lstm\n",
    "df_history_both_lstm.to_csv('df_history_both_lstm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Model-2\n",
    "both_lstm = load_model('both_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French(Source) : appretetoi\n",
      "Target : prepare yourself\n",
      "Predicted : prepare yourself \n",
      "\n",
      "French(Source) : avezvous pris une douche\n",
      "Target : have you showered\n",
      "Predicted : have you showered \n",
      "\n",
      "French(Source) : rentre a la maison\n",
      "Target : go home\n",
      "Predicted : come home \n",
      "\n",
      "French(Source) : ils ont tort\n",
      "Target : they are wrong\n",
      "Predicted : they wrong \n",
      "\n",
      "French(Source) : aidemoi a sortir\n",
      "Target : help me out\n",
      "Predicted : get you out \n",
      "\n",
      "French(Source) : prenez les commandes\n",
      "Target : take command\n",
      "Predicted : look down \n",
      "\n",
      "French(Source) : restez ou vous etes\n",
      "Target : hold it\n",
      "Predicted : stay back \n",
      "\n",
      "French(Source) : je suis ruinee\n",
      "Target : im ruined\n",
      "Predicted : im wealthy \n",
      "\n",
      "French(Source) : je ne suis pas malheureux\n",
      "Target : im not unhappy\n",
      "Predicted : im not unhappy \n",
      "\n",
      "French(Source) : je peux y parvenir\n",
      "Target : i can do it\n",
      "Predicted : i can do it \n",
      "\n",
      "French(Source) : jaime tes jambes\n",
      "Target : i like your legs\n",
      "Predicted : i like your legs \n",
      "\n",
      "French(Source) : maitrisezvous\n",
      "Target : control yourself\n",
      "Predicted : control yourself \n",
      "\n",
      "French(Source) : je suis a moitie japonaise\n",
      "Target : im half japanese\n",
      "Predicted : im normal japanese \n",
      "\n",
      "French(Source) : en estu sure\n",
      "Target : are you certain\n",
      "Predicted : are you sure \n",
      "\n",
      "French(Source) : jaime ton sourire\n",
      "Target : i like your smile\n",
      "Predicted : i like your \n",
      "\n",
      "French(Source) : klaxonnez\n",
      "Target : honk the horn\n",
      "Predicted : think about to \n",
      "\n",
      "French(Source) : tu es grossier\n",
      "Target : youre rude\n",
      "Predicted : youre selfish \n",
      "\n",
      "French(Source) : nous devrions partir\n",
      "Target : we should go\n",
      "Predicted : we should to \n",
      "\n",
      "French(Source) : jai oublie la carte\n",
      "Target : i forgot the map\n",
      "Predicted : i forgot the map \n",
      "\n",
      "French(Source) : ils font des efforts\n",
      "Target : theyre trying\n",
      "Predicted : theyll trying \n",
      "\n",
      "French(Source) : tom sen allait\n",
      "Target : tom was leaving\n",
      "Predicted : tom is town \n",
      "\n",
      "French(Source) : ne me parle pas\n",
      "Target : dont talk to me\n",
      "Predicted : dont talk me \n",
      "\n",
      "French(Source) : tom etait ici\n",
      "Target : tom was here\n",
      "Predicted : was was here \n",
      "\n",
      "French(Source) : jai termine premier\n",
      "Target : i finished first\n",
      "Predicted : i won first \n",
      "\n",
      "French(Source) : elle garde des secrets\n",
      "Target : she keeps secrets\n",
      "Predicted : she keeps secrets \n",
      "\n",
      "French(Source) : mordstoi la langue\n",
      "Target : bite your tongue\n",
      "Predicted : bite your tongue \n",
      "\n",
      "French(Source) : je veux faire ceci\n",
      "Target : i want to do this\n",
      "Predicted : i want this do this \n",
      "\n",
      "French(Source) : vous etes les vainqueurs\n",
      "Target : youve won\n",
      "Predicted : youre won \n",
      "\n",
      "French(Source) : inutile de discuter\n",
      "Target : dont argue\n",
      "Predicted : wait argue \n",
      "\n",
      "French(Source) : jai besoin dun taxi\n",
      "Target : i need a cab\n",
      "Predicted : i need a cab \n",
      "\n",
      "French(Source) : je nai pas dexcuse\n",
      "Target : i have no excuse\n",
      "Predicted : i have no excuse \n",
      "\n",
      "French(Source) : je peux la reparer\n",
      "Target : i can fix it\n",
      "Predicted : i can try it \n",
      "\n",
      "French(Source) : je trouvais ce vin bon\n",
      "Target : i liked that wine\n",
      "Predicted : i did this books \n",
      "\n",
      "French(Source) : cesse de hurler\n",
      "Target : stop screaming\n",
      "Predicted : stop yelling \n",
      "\n",
      "French(Source) : je resterai debout\n",
      "Target : ill stand\n",
      "Predicted : ill up \n",
      "\n",
      "French(Source) : jaime le rock\n",
      "Target : i like rock music\n",
      "Predicted : i like rock \n",
      "\n",
      "French(Source) : tom est juste\n",
      "Target : tom is fair\n",
      "Predicted : tom is fair \n",
      "\n",
      "French(Source) : regardez a nouveau\n",
      "Target : look again\n",
      "Predicted : look at again \n",
      "\n",
      "French(Source) : je protestais\n",
      "Target : i objected\n",
      "Predicted : i protested \n",
      "\n",
      "French(Source) : nous sommes au regime\n",
      "Target : were dieting\n",
      "Predicted : were dieting \n",
      "\n",
      "French(Source) : vous etes tous heureux\n",
      "Target : youre all happy\n",
      "Predicted : youre all happy \n",
      "\n",
      "French(Source) : je suis connue\n",
      "Target : im famous\n",
      "Predicted : im famous \n",
      "\n",
      "French(Source) : etesvous blesses\n",
      "Target : are you injured\n",
      "Predicted : are you injured \n",
      "\n",
      "French(Source) : fais un calin a tom\n",
      "Target : hug tom\n",
      "Predicted : get tom call tom \n",
      "\n",
      "French(Source) : astu un animal domestique\n",
      "Target : do you have a pet\n",
      "Predicted : do you have a pet \n",
      "\n",
      "French(Source) : tom doit rester\n",
      "Target : tom has to stay\n",
      "Predicted : tom can to stay \n",
      "\n",
      "French(Source) : ou est la voiture\n",
      "Target : wheres the car\n",
      "Predicted : wheres the car \n",
      "\n",
      "French(Source) : deverrouillele\n",
      "Target : unlock it\n",
      "Predicted : unlock it \n",
      "\n",
      "French(Source) : je paierai\n",
      "Target : ill pay\n",
      "Predicted : ill gladly \n",
      "\n",
      "French(Source) : monte au sommet\n",
      "Target : climb to the top\n",
      "Predicted : get the the \n",
      "\n",
      "BLEU score: 0.307611\n"
     ]
    }
   ],
   "source": [
    "# Model-2 training sequences\n",
    "bleu_train_both_lstm = model_evaluation(both_lstm, eng_tokenizer, x_train, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French(Source) : jai oublie le livre\n",
      "Target : i forgot the book\n",
      "Predicted : i forgot the book \n",
      "\n",
      "French(Source) : tom est sans domicile fixe\n",
      "Target : toms homeless\n",
      "Predicted : tom is free \n",
      "\n",
      "French(Source) : jaide tom\n",
      "Target : i help tom\n",
      "Predicted : well ignored it \n",
      "\n",
      "French(Source) : jai achete un cactus\n",
      "Target : i bought a cactus\n",
      "Predicted : i had a seizure \n",
      "\n",
      "French(Source) : cest plutot grand\n",
      "Target : its quite large\n",
      "Predicted : thats very big \n",
      "\n",
      "French(Source) : ouvrez le coffre\n",
      "Target : open the safe\n",
      "Predicted : open the safe \n",
      "\n",
      "French(Source) : il est trop confiant\n",
      "Target : hes too trusting\n",
      "Predicted : he is too \n",
      "\n",
      "French(Source) : il adore les jouets\n",
      "Target : he loves toys\n",
      "Predicted : he likes oranges \n",
      "\n",
      "French(Source) : estce que vous avez le temps\n",
      "Target : do you have time\n",
      "Predicted : do you time time \n",
      "\n",
      "French(Source) : jai presente mes excuses\n",
      "Target : i have apologized\n",
      "Predicted : i finally up \n",
      "\n",
      "French(Source) : il nous faut de leau\n",
      "Target : we need water\n",
      "Predicted : we got some water \n",
      "\n",
      "French(Source) : cest simple nestce pas\n",
      "Target : simple isnt it\n",
      "Predicted : its is not \n",
      "\n",
      "French(Source) : nous allons camper ici\n",
      "Target : well camp here\n",
      "Predicted : lets stay here \n",
      "\n",
      "French(Source) : oublie tom\n",
      "Target : never mind tom\n",
      "Predicted : kill tom free \n",
      "\n",
      "French(Source) : jaime les recits\n",
      "Target : i like stories\n",
      "Predicted : i like picnics \n",
      "\n",
      "French(Source) : etesvous enseignante\n",
      "Target : are you a teacher\n",
      "Predicted : do you lost \n",
      "\n",
      "French(Source) : cest la norme\n",
      "Target : this is the norm\n",
      "Predicted : its is season \n",
      "\n",
      "French(Source) : nous ne sommes pas sures\n",
      "Target : were not sure\n",
      "Predicted : were not \n",
      "\n",
      "French(Source) : ca a de limportance pour moi\n",
      "Target : it matters to me\n",
      "Predicted : it agreed for for \n",
      "\n",
      "French(Source) : estu maniaque\n",
      "Target : are you a maniac\n",
      "Predicted : are you sure \n",
      "\n",
      "French(Source) : je dois mechapper\n",
      "Target : i have to escape\n",
      "Predicted : i wanted to stop \n",
      "\n",
      "French(Source) : arrete ca maintenant\n",
      "Target : stop that now\n",
      "Predicted : stop home it \n",
      "\n",
      "French(Source) : estu epuise\n",
      "Target : are you exhausted\n",
      "Predicted : are you knackered \n",
      "\n",
      "French(Source) : attendez\n",
      "Target : wait\n",
      "Predicted : hold on \n",
      "\n",
      "French(Source) : il est sexy\n",
      "Target : hes sexy\n",
      "Predicted : he is british \n",
      "\n",
      "French(Source) : etesvous droguee\n",
      "Target : are you on dope\n",
      "Predicted : are you on dope \n",
      "\n",
      "French(Source) : jirai me changer\n",
      "Target : ill go change\n",
      "Predicted : i refuse to \n",
      "\n",
      "French(Source) : il est intelligent\n",
      "Target : he is intelligent\n",
      "Predicted : he is \n",
      "\n",
      "French(Source) : ils le connaissent\n",
      "Target : they know him\n",
      "Predicted : they know know \n",
      "\n",
      "French(Source) : lastu emporte\n",
      "Target : did you win\n",
      "Predicted : are you win \n",
      "\n",
      "French(Source) : nous avons pris le petit dejeuner\n",
      "Target : we had breakfast\n",
      "Predicted : we made breakfast \n",
      "\n",
      "French(Source) : personne na ete vire\n",
      "Target : no one was fired\n",
      "Predicted : no was was was \n",
      "\n",
      "French(Source) : elle est maladroite\n",
      "Target : she is awkward\n",
      "Predicted : she is obstinate \n",
      "\n",
      "French(Source) : tom a ralenti\n",
      "Target : tom slowed down\n",
      "Predicted : tom testified \n",
      "\n",
      "French(Source) : estu seul\n",
      "Target : are you single\n",
      "Predicted : are you alone \n",
      "\n",
      "French(Source) : elles sont formidables\n",
      "Target : theyre great\n",
      "Predicted : theyre great \n",
      "\n",
      "French(Source) : sil te plait chante\n",
      "Target : please sing\n",
      "Predicted : please talk \n",
      "\n",
      "French(Source) : astu fait ceci\n",
      "Target : did you make this\n",
      "Predicted : did you read that \n",
      "\n",
      "French(Source) : sautez de lautre cote\n",
      "Target : jump across\n",
      "Predicted : wait across room \n",
      "\n",
      "French(Source) : tu es si naive\n",
      "Target : youre so naive\n",
      "Predicted : youre so naive \n",
      "\n",
      "French(Source) : viens jouer avec nous\n",
      "Target : come play with us\n",
      "Predicted : come sit with \n",
      "\n",
      "French(Source) : enfile ton pantalon\n",
      "Target : put your pants on\n",
      "Predicted : remove your hat \n",
      "\n",
      "French(Source) : nous sommes tous en train de mourir\n",
      "Target : were all dying\n",
      "Predicted : were dying \n",
      "\n",
      "French(Source) : nous ne sommes pas perdues\n",
      "Target : were not lost\n",
      "Predicted : were not not \n",
      "\n",
      "French(Source) : jen veux un\n",
      "Target : i want one\n",
      "Predicted : i want a \n",
      "\n",
      "French(Source) : jetais dans la voiture\n",
      "Target : i was in the car\n",
      "Predicted : i was the car \n",
      "\n",
      "French(Source) : sil vous plait soyez prudent\n",
      "Target : please be careful\n",
      "Predicted : please be careful \n",
      "\n",
      "French(Source) : cest juste faux\n",
      "Target : its just wrong\n",
      "Predicted : its no untrue \n",
      "\n",
      "French(Source) : offremoi un verre\n",
      "Target : buy me a drink\n",
      "Predicted : give me a moment \n",
      "\n",
      "French(Source) : ils apprecient le francais\n",
      "Target : they like french\n",
      "Predicted : they won french \n",
      "\n",
      "BLEU score: 0.129290\n"
     ]
    }
   ],
   "source": [
    "# Testing the Model-2 test sequences\n",
    "bleu_test_both_lstm = model_evaluation(both_lstm, eng_tokenizer, x_test, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3:  Encoder GRU and Decoder LSTM cell\n",
    "\n",
    "def gru_lstm(french_vocab, english_vocab, french_timesteps, english_timesteps, num_units):\n",
    "\tlearning_rate = 1e-2\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Embedding(french_vocab, num_units, input_length= french_timesteps, mask_zero=True))\n",
    "\tmodel.add(GRU(num_units))\n",
    "\tmodel.add(RepeatVector(english_timesteps))\n",
    "\tmodel.add(LSTM(num_units,return_sequences=True))\n",
    "\tmodel.add(TimeDistributed(Dense(english_vocab, activation='softmax')))\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Model-3\n",
    "\n",
    "learning_rate = 1e-2\n",
    "gru_lstm = gru_lstm(french_vocabulary_size, english_vocabulary_size, french_maximum_length, english_maximum_length, 256)\n",
    "gru_lstm.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 12, 256)           2053376   \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 256)               394752    \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 5, 3949)           1014893   \n",
      "=================================================================\n",
      "Total params: 3,988,333\n",
      "Trainable params: 3,988,333\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "# Summary of Model-3\n",
    "\n",
    "print(gru_lstm.summary())\n",
    "plot_model(gru_lstm, to_file='gru_lstm.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "274/274 - 36s - loss: 3.8447 - categorical_accuracy: 0.4401 - val_loss: 3.3390 - val_categorical_accuracy: 0.4846\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.33901, saving model to gru_lstm.h5\n",
      "Epoch 2/30\n",
      "274/274 - 30s - loss: 2.9326 - categorical_accuracy: 0.5280 - val_loss: 2.8344 - val_categorical_accuracy: 0.5526\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.33901 to 2.83437, saving model to gru_lstm.h5\n",
      "Epoch 3/30\n",
      "274/274 - 29s - loss: 2.4225 - categorical_accuracy: 0.5791 - val_loss: 2.5735 - val_categorical_accuracy: 0.5853\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.83437 to 2.57354, saving model to gru_lstm.h5\n",
      "Epoch 4/30\n",
      "274/274 - 29s - loss: 2.0803 - categorical_accuracy: 0.6107 - val_loss: 2.4446 - val_categorical_accuracy: 0.6018\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.57354 to 2.44455, saving model to gru_lstm.h5\n",
      "Epoch 5/30\n",
      "274/274 - 31s - loss: 1.8117 - categorical_accuracy: 0.6365 - val_loss: 2.3609 - val_categorical_accuracy: 0.6097\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.44455 to 2.36088, saving model to gru_lstm.h5\n",
      "Epoch 6/30\n",
      "274/274 - 32s - loss: 1.6185 - categorical_accuracy: 0.6553 - val_loss: 2.3196 - val_categorical_accuracy: 0.6117\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.36088 to 2.31958, saving model to gru_lstm.h5\n",
      "Epoch 7/30\n",
      "274/274 - 32s - loss: 1.4572 - categorical_accuracy: 0.6740 - val_loss: 2.2799 - val_categorical_accuracy: 0.6202\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.31958 to 2.27990, saving model to gru_lstm.h5\n",
      "Epoch 8/30\n",
      "274/274 - 31s - loss: 1.3032 - categorical_accuracy: 0.6977 - val_loss: 2.2647 - val_categorical_accuracy: 0.6242\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.27990 to 2.26471, saving model to gru_lstm.h5\n",
      "Epoch 9/30\n",
      "274/274 - 30s - loss: 1.1836 - categorical_accuracy: 0.7150 - val_loss: 2.2578 - val_categorical_accuracy: 0.6218\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.26471 to 2.25780, saving model to gru_lstm.h5\n",
      "Epoch 10/30\n",
      "274/274 - 31s - loss: 1.0954 - categorical_accuracy: 0.7309 - val_loss: 2.2587 - val_categorical_accuracy: 0.6264\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.25780\n",
      "Epoch 11/30\n",
      "274/274 - 30s - loss: 1.0152 - categorical_accuracy: 0.7455 - val_loss: 2.2845 - val_categorical_accuracy: 0.6273\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.25780\n",
      "Epoch 12/30\n",
      "274/274 - 32s - loss: 0.9468 - categorical_accuracy: 0.7576 - val_loss: 2.2722 - val_categorical_accuracy: 0.6338\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.25780\n",
      "Epoch 13/30\n",
      "274/274 - 33s - loss: 0.8901 - categorical_accuracy: 0.7682 - val_loss: 2.2604 - val_categorical_accuracy: 0.6419\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.25780\n",
      "Epoch 14/30\n",
      "274/274 - 33s - loss: 0.8244 - categorical_accuracy: 0.7801 - val_loss: 2.2985 - val_categorical_accuracy: 0.6340\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.25780\n",
      "Epoch 15/30\n",
      "274/274 - 33s - loss: 0.7926 - categorical_accuracy: 0.7874 - val_loss: 2.3139 - val_categorical_accuracy: 0.6370\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 2.25780\n",
      "Epoch 16/30\n",
      "274/274 - 34s - loss: 0.7812 - categorical_accuracy: 0.7902 - val_loss: 2.3218 - val_categorical_accuracy: 0.6353\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 2.25780\n",
      "Epoch 17/30\n",
      "274/274 - 35s - loss: 0.7486 - categorical_accuracy: 0.7949 - val_loss: 2.3252 - val_categorical_accuracy: 0.6393\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 2.25780\n",
      "Epoch 18/30\n",
      "274/274 - 34s - loss: 0.7168 - categorical_accuracy: 0.8024 - val_loss: 2.3604 - val_categorical_accuracy: 0.6392\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 2.25780\n",
      "Epoch 19/30\n",
      "274/274 - 34s - loss: 0.6881 - categorical_accuracy: 0.8095 - val_loss: 2.3495 - val_categorical_accuracy: 0.6416\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 2.25780\n",
      "Epoch 20/30\n",
      "274/274 - 31s - loss: 0.6698 - categorical_accuracy: 0.8127 - val_loss: 2.3599 - val_categorical_accuracy: 0.6407\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 2.25780\n",
      "Epoch 21/30\n",
      "274/274 - 31s - loss: 0.6488 - categorical_accuracy: 0.8177 - val_loss: 2.4033 - val_categorical_accuracy: 0.6409\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 2.25780\n",
      "Epoch 22/30\n",
      "274/274 - 31s - loss: 0.6466 - categorical_accuracy: 0.8175 - val_loss: 2.4217 - val_categorical_accuracy: 0.6319\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 2.25780\n",
      "Epoch 23/30\n",
      "274/274 - 32s - loss: 0.6345 - categorical_accuracy: 0.8195 - val_loss: 2.4265 - val_categorical_accuracy: 0.6369\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 2.25780\n",
      "Epoch 24/30\n",
      "274/274 - 32s - loss: 0.6360 - categorical_accuracy: 0.8181 - val_loss: 2.4289 - val_categorical_accuracy: 0.6398\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 2.25780\n",
      "Epoch 25/30\n",
      "274/274 - 34s - loss: 0.6180 - categorical_accuracy: 0.8226 - val_loss: 2.4723 - val_categorical_accuracy: 0.6353\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 2.25780\n",
      "Epoch 26/30\n",
      "274/274 - 33s - loss: 0.6051 - categorical_accuracy: 0.8245 - val_loss: 2.4668 - val_categorical_accuracy: 0.6364\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 2.25780\n",
      "Epoch 27/30\n",
      "274/274 - 32s - loss: 0.6219 - categorical_accuracy: 0.8194 - val_loss: 2.4973 - val_categorical_accuracy: 0.6358\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 2.25780\n",
      "Epoch 28/30\n",
      "274/274 - 32s - loss: 0.6070 - categorical_accuracy: 0.8228 - val_loss: 2.4759 - val_categorical_accuracy: 0.6447\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 2.25780\n",
      "Epoch 29/30\n",
      "274/274 - 32s - loss: 0.5925 - categorical_accuracy: 0.8273 - val_loss: 2.5090 - val_categorical_accuracy: 0.6408\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 2.25780\n",
      "Epoch 30/30\n",
      "274/274 - 32s - loss: 0.5930 - categorical_accuracy: 0.8274 - val_loss: 2.5217 - val_categorical_accuracy: 0.6354\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 2.25780\n"
     ]
    }
   ],
   "source": [
    "# Fitting the Model-3\n",
    "\n",
    "filename = 'gru_lstm.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "history_gru_lstm = gru_lstm.fit(x_train, y_train, epochs=30, batch_size=64, validation_data=(x_valid, y_valid), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        loss  categorical_accuracy  val_loss  val_categorical_accuracy\n",
      "0   3.793427              0.441154  3.208919                   0.49128\n",
      "1   2.833559              0.533040  2.711730                   0.55744\n",
      "2   2.293599              0.584000  2.452090                   0.59016\n",
      "3   1.895594              0.622914  2.316713                   0.60192\n",
      "4   1.579440              0.659497  2.236204                   0.61992\n",
      "5   1.334634              0.693417  2.175927                   0.62880\n",
      "6   1.123589              0.726263  2.158596                   0.63616\n",
      "7   0.972408              0.754206  2.154496                   0.64736\n",
      "8   0.849356              0.779669  2.160844                   0.64736\n",
      "9   0.746623              0.800469  2.180151                   0.64880\n",
      "10  0.660448              0.818903  2.209576                   0.65040\n",
      "11  0.609541              0.832423  2.229174                   0.65304\n",
      "12  0.561103              0.841737  2.251234                   0.65488\n",
      "13  0.518969              0.852217  2.271991                   0.65240\n",
      "14  0.488100              0.860594  2.277006                   0.65456\n",
      "15  0.467883              0.863246  2.331982                   0.65448\n",
      "16  0.465596              0.863417  2.343225                   0.65360\n",
      "17  0.438265              0.869474  2.393771                   0.65440\n",
      "18  0.423068              0.874480  2.404759                   0.65912\n",
      "19  0.405823              0.877863  2.405482                   0.65520\n",
      "20  0.390520              0.882046  2.428676                   0.65592\n",
      "21  0.369550              0.888834  2.447434                   0.65544\n",
      "22  0.370651              0.887051  2.480661                   0.64984\n",
      "23  0.373507              0.885417  2.519141                   0.65264\n",
      "24  0.375611              0.885211  2.526074                   0.64808\n",
      "25  0.374611              0.884926  2.535545                   0.64584\n",
      "26  0.379696              0.883554  2.558910                   0.64464\n",
      "27  0.377861              0.883703  2.558459                   0.64848\n",
      "28  0.371476              0.885154  2.591076                   0.64608\n",
      "29  0.368846              0.886137  2.605842                   0.64640\n"
     ]
    }
   ],
   "source": [
    "# Saving the history of the Model-3\n",
    "\n",
    "df_history_gru_lstm = pd.DataFrame(history_both_lstm.history)\n",
    "df_history_gru_lstm.to_csv('df_history_gru_lstm.csv')\n",
    "print(df_history_gru_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Model-3\n",
    "\n",
    "gru_lstm = load_model('gru_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French(Source) : appretetoi\n",
      "Target : prepare yourself\n",
      "Predicted : prepare yourself \n",
      "\n",
      "French(Source) : avezvous pris une douche\n",
      "Target : have you showered\n",
      "Predicted : do you showered \n",
      "\n",
      "French(Source) : rentre a la maison\n",
      "Target : go home\n",
      "Predicted : come home \n",
      "\n",
      "French(Source) : ils ont tort\n",
      "Target : they are wrong\n",
      "Predicted : they seem \n",
      "\n",
      "French(Source) : aidemoi a sortir\n",
      "Target : help me out\n",
      "Predicted : help out \n",
      "\n",
      "French(Source) : prenez les commandes\n",
      "Target : take command\n",
      "Predicted : take the \n",
      "\n",
      "French(Source) : restez ou vous etes\n",
      "Target : hold it\n",
      "Predicted : stay stay \n",
      "\n",
      "French(Source) : je suis ruinee\n",
      "Target : im ruined\n",
      "Predicted : im brave \n",
      "\n",
      "French(Source) : je ne suis pas malheureux\n",
      "Target : im not unhappy\n",
      "Predicted : im not unhappy \n",
      "\n",
      "French(Source) : je peux y parvenir\n",
      "Target : i can do it\n",
      "Predicted : i can handle it it \n",
      "\n",
      "French(Source) : jaime tes jambes\n",
      "Target : i like your legs\n",
      "Predicted : i like your legs \n",
      "\n",
      "French(Source) : maitrisezvous\n",
      "Target : control yourself\n",
      "Predicted : hide yourself \n",
      "\n",
      "French(Source) : je suis a moitie japonaise\n",
      "Target : im half japanese\n",
      "Predicted : im are japanese \n",
      "\n",
      "French(Source) : en estu sure\n",
      "Target : are you certain\n",
      "Predicted : are you certain \n",
      "\n",
      "French(Source) : jaime ton sourire\n",
      "Target : i like your smile\n",
      "Predicted : i like your \n",
      "\n",
      "French(Source) : klaxonnez\n",
      "Target : honk the horn\n",
      "Predicted : honk the horn \n",
      "\n",
      "French(Source) : tu es grossier\n",
      "Target : youre rude\n",
      "Predicted : youre direct \n",
      "\n",
      "French(Source) : nous devrions partir\n",
      "Target : we should go\n",
      "Predicted : we ought to go \n",
      "\n",
      "French(Source) : jai oublie la carte\n",
      "Target : i forgot the map\n",
      "Predicted : i brushed my word \n",
      "\n",
      "French(Source) : ils font des efforts\n",
      "Target : theyre trying\n",
      "Predicted : theyre immature \n",
      "\n",
      "French(Source) : tom sen allait\n",
      "Target : tom was leaving\n",
      "Predicted : tom smells \n",
      "\n",
      "French(Source) : ne me parle pas\n",
      "Target : dont talk to me\n",
      "Predicted : dont talk me \n",
      "\n",
      "French(Source) : tom etait ici\n",
      "Target : tom was here\n",
      "Predicted : tom was here \n",
      "\n",
      "French(Source) : jai termine premier\n",
      "Target : i finished first\n",
      "Predicted : i motivated \n",
      "\n",
      "French(Source) : elle garde des secrets\n",
      "Target : she keeps secrets\n",
      "Predicted : shes died \n",
      "\n",
      "French(Source) : mordstoi la langue\n",
      "Target : bite your tongue\n",
      "Predicted : open your tongue \n",
      "\n",
      "French(Source) : je veux faire ceci\n",
      "Target : i want to do this\n",
      "Predicted : i want to do this \n",
      "\n",
      "French(Source) : vous etes les vainqueurs\n",
      "Target : youve won\n",
      "Predicted : youre reserved \n",
      "\n",
      "French(Source) : inutile de discuter\n",
      "Target : dont argue\n",
      "Predicted : keep talk \n",
      "\n",
      "French(Source) : jai besoin dun taxi\n",
      "Target : i need a cab\n",
      "Predicted : i need a cab \n",
      "\n",
      "French(Source) : je nai pas dexcuse\n",
      "Target : i have no excuse\n",
      "Predicted : i will no excuse \n",
      "\n",
      "French(Source) : je peux la reparer\n",
      "Target : i can fix it\n",
      "Predicted : i can verify it \n",
      "\n",
      "French(Source) : je trouvais ce vin bon\n",
      "Target : i liked that wine\n",
      "Predicted : i love you you \n",
      "\n",
      "French(Source) : cesse de hurler\n",
      "Target : stop screaming\n",
      "Predicted : stop gambling \n",
      "\n",
      "French(Source) : je resterai debout\n",
      "Target : ill stand\n",
      "Predicted : ill take it \n",
      "\n",
      "French(Source) : jaime le rock\n",
      "Target : i like rock music\n",
      "Predicted : i like rock \n",
      "\n",
      "French(Source) : tom est juste\n",
      "Target : tom is fair\n",
      "Predicted : tom is silent \n",
      "\n",
      "French(Source) : regardez a nouveau\n",
      "Target : look again\n",
      "Predicted : look these again \n",
      "\n",
      "French(Source) : je protestais\n",
      "Target : i objected\n",
      "Predicted : i protested \n",
      "\n",
      "French(Source) : nous sommes au regime\n",
      "Target : were dieting\n",
      "Predicted : were dieting \n",
      "\n",
      "French(Source) : vous etes tous heureux\n",
      "Target : youre all happy\n",
      "Predicted : youre all happy \n",
      "\n",
      "French(Source) : je suis connue\n",
      "Target : im famous\n",
      "Predicted : im famous \n",
      "\n",
      "French(Source) : etesvous blesses\n",
      "Target : are you injured\n",
      "Predicted : are you injured \n",
      "\n",
      "French(Source) : fais un calin a tom\n",
      "Target : hug tom\n",
      "Predicted : everyone tom tom \n",
      "\n",
      "French(Source) : astu un animal domestique\n",
      "Target : do you have a pet\n",
      "Predicted : do you have a pet \n",
      "\n",
      "French(Source) : tom doit rester\n",
      "Target : tom has to stay\n",
      "Predicted : tom tom stay \n",
      "\n",
      "French(Source) : ou est la voiture\n",
      "Target : wheres the car\n",
      "Predicted : wheres your car \n",
      "\n",
      "French(Source) : deverrouillele\n",
      "Target : unlock it\n",
      "Predicted : unlock up \n",
      "\n",
      "French(Source) : je paierai\n",
      "Target : ill pay\n",
      "Predicted : ill pay ill \n",
      "\n",
      "French(Source) : monte au sommet\n",
      "Target : climb to the top\n",
      "Predicted : get on the top \n",
      "\n",
      "BLEU score: 0.250735\n"
     ]
    }
   ],
   "source": [
    "# Model-3 on the training sequences\n",
    "\n",
    "bleu_train_gru_lstm = model_evaluation(gru_lstm, eng_tokenizer, x_train, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French(Source) : jai oublie le livre\n",
      "Target : i forgot the book\n",
      "Predicted : i got the book \n",
      "\n",
      "French(Source) : tom est sans domicile fixe\n",
      "Target : toms homeless\n",
      "Predicted : tom is early \n",
      "\n",
      "French(Source) : jaide tom\n",
      "Target : i help tom\n",
      "Predicted : ill call \n",
      "\n",
      "French(Source) : jai achete un cactus\n",
      "Target : i bought a cactus\n",
      "Predicted : i have a fortune \n",
      "\n",
      "French(Source) : cest plutot grand\n",
      "Target : its quite large\n",
      "Predicted : thats is big \n",
      "\n",
      "French(Source) : ouvrez le coffre\n",
      "Target : open the safe\n",
      "Predicted : open the \n",
      "\n",
      "French(Source) : il est trop confiant\n",
      "Target : hes too trusting\n",
      "Predicted : hes is too \n",
      "\n",
      "French(Source) : il adore les jouets\n",
      "Target : he loves toys\n",
      "Predicted : he likes himself \n",
      "\n",
      "French(Source) : estce que vous avez le temps\n",
      "Target : do you have time\n",
      "Predicted : is you count it \n",
      "\n",
      "French(Source) : jai presente mes excuses\n",
      "Target : i have apologized\n",
      "Predicted : i my my \n",
      "\n",
      "French(Source) : il nous faut de leau\n",
      "Target : we need water\n",
      "Predicted : we need some water \n",
      "\n",
      "French(Source) : cest simple nestce pas\n",
      "Target : simple isnt it\n",
      "Predicted : thats not it \n",
      "\n",
      "French(Source) : nous allons camper ici\n",
      "Target : well camp here\n",
      "Predicted : we be right \n",
      "\n",
      "French(Source) : oublie tom\n",
      "Target : never mind tom\n",
      "Predicted : help to \n",
      "\n",
      "French(Source) : jaime les recits\n",
      "Target : i like stories\n",
      "Predicted : i like stories \n",
      "\n",
      "French(Source) : etesvous enseignante\n",
      "Target : are you a teacher\n",
      "Predicted : are you bowl \n",
      "\n",
      "French(Source) : cest la norme\n",
      "Target : this is the norm\n",
      "Predicted : this is private \n",
      "\n",
      "French(Source) : nous ne sommes pas sures\n",
      "Target : were not sure\n",
      "Predicted : were not no \n",
      "\n",
      "French(Source) : ca a de limportance pour moi\n",
      "Target : it matters to me\n",
      "Predicted : it depends for it \n",
      "\n",
      "French(Source) : estu maniaque\n",
      "Target : are you a maniac\n",
      "Predicted : are you jealous \n",
      "\n",
      "French(Source) : je dois mechapper\n",
      "Target : i have to escape\n",
      "Predicted : i must to go \n",
      "\n",
      "French(Source) : arrete ca maintenant\n",
      "Target : stop that now\n",
      "Predicted : stop that now \n",
      "\n",
      "French(Source) : estu epuise\n",
      "Target : are you exhausted\n",
      "Predicted : are you knackered \n",
      "\n",
      "French(Source) : attendez\n",
      "Target : wait\n",
      "Predicted : hold on \n",
      "\n",
      "French(Source) : il est sexy\n",
      "Target : hes sexy\n",
      "Predicted : hes is comedian \n",
      "\n",
      "French(Source) : etesvous droguee\n",
      "Target : are you on dope\n",
      "Predicted : are you on dope \n",
      "\n",
      "French(Source) : jirai me changer\n",
      "Target : ill go change\n",
      "Predicted : i you go \n",
      "\n",
      "French(Source) : il est intelligent\n",
      "Target : he is intelligent\n",
      "Predicted : hes addicted \n",
      "\n",
      "French(Source) : ils le connaissent\n",
      "Target : they know him\n",
      "Predicted : they found him \n",
      "\n",
      "French(Source) : lastu emporte\n",
      "Target : did you win\n",
      "Predicted : did you clean \n",
      "\n",
      "French(Source) : nous avons pris le petit dejeuner\n",
      "Target : we had breakfast\n",
      "Predicted : we have rain \n",
      "\n",
      "French(Source) : personne na ete vire\n",
      "Target : no one was fired\n",
      "Predicted : no one was stung \n",
      "\n",
      "French(Source) : elle est maladroite\n",
      "Target : she is awkward\n",
      "Predicted : she is gentle \n",
      "\n",
      "French(Source) : tom a ralenti\n",
      "Target : tom slowed down\n",
      "Predicted : tom relented chickens \n",
      "\n",
      "French(Source) : estu seul\n",
      "Target : are you single\n",
      "Predicted : are you alone \n",
      "\n",
      "French(Source) : elles sont formidables\n",
      "Target : theyre great\n",
      "Predicted : theyre great \n",
      "\n",
      "French(Source) : sil te plait chante\n",
      "Target : please sing\n",
      "Predicted : please please please \n",
      "\n",
      "French(Source) : astu fait ceci\n",
      "Target : did you make this\n",
      "Predicted : did you know it \n",
      "\n",
      "French(Source) : sautez de lautre cote\n",
      "Target : jump across\n",
      "Predicted : wait and \n",
      "\n",
      "French(Source) : tu es si naive\n",
      "Target : youre so naive\n",
      "Predicted : youre so tall \n",
      "\n",
      "French(Source) : viens jouer avec nous\n",
      "Target : come play with us\n",
      "Predicted : come and come us \n",
      "\n",
      "French(Source) : enfile ton pantalon\n",
      "Target : put your pants on\n",
      "Predicted : the your pants \n",
      "\n",
      "French(Source) : nous sommes tous en train de mourir\n",
      "Target : were all dying\n",
      "Predicted : we all nervous \n",
      "\n",
      "French(Source) : nous ne sommes pas perdues\n",
      "Target : were not lost\n",
      "Predicted : we not go \n",
      "\n",
      "French(Source) : jen veux un\n",
      "Target : i want one\n",
      "Predicted : i want a one \n",
      "\n",
      "French(Source) : jetais dans la voiture\n",
      "Target : i was in the car\n",
      "Predicted : i was my car \n",
      "\n",
      "French(Source) : sil vous plait soyez prudent\n",
      "Target : please be careful\n",
      "Predicted : please be careful \n",
      "\n",
      "French(Source) : cest juste faux\n",
      "Target : its just wrong\n",
      "Predicted : thats a typical \n",
      "\n",
      "French(Source) : offremoi un verre\n",
      "Target : buy me a drink\n",
      "Predicted : you me a story \n",
      "\n",
      "French(Source) : ils apprecient le francais\n",
      "Target : they like french\n",
      "Predicted : they asked \n",
      "\n",
      "BLEU score: 0.113135\n"
     ]
    }
   ],
   "source": [
    "# Testing the Model-3 on the test sequences\n",
    "\n",
    "bleu_test_gru_lstm = model_evaluation(gru_lstm, eng_tokenizer, x_test, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4:  Encoder LSTM and Decoder GRU cell\n",
    "\n",
    "def lstm_gru(french_vocab, english_vocab, french_timesteps, english_timesteps, num_units):\n",
    "\tlearning_rate = 1e-2\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Embedding(french_vocab, num_units, input_length= french_timesteps, mask_zero=True))\n",
    "\tmodel.add(LSTM(num_units))\n",
    "\tmodel.add(RepeatVector(english_timesteps))\n",
    "\tmodel.add(GRU(num_units,return_sequences=True))\n",
    "\tmodel.add(TimeDistributed(Dense(english_vocab, activation='softmax')))\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Model-4\n",
    "\n",
    "learning_rate = 1e-2\n",
    "lstm_gru = lstm_gru(french_vocabulary_size, english_vocabulary_size, french_maximum_length, english_maximum_length, 256)\n",
    "lstm_gru.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 12, 256)           2053376   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_3 (RepeatVecto (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 5, 256)            394752    \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 5, 3949)           1014893   \n",
      "=================================================================\n",
      "Total params: 3,988,333\n",
      "Trainable params: 3,988,333\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "# Summary of Model-4\n",
    "\n",
    "print(lstm_gru.summary())\n",
    "plot_model(lstm_gru, to_file='lstm_gru.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "274/274 - 30s - loss: 3.9620 - categorical_accuracy: 0.4267 - val_loss: 3.4909 - val_categorical_accuracy: 0.4603\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.49087, saving model to lstm_gru.h5\n",
      "Epoch 2/30\n",
      "274/274 - 29s - loss: 3.1797 - categorical_accuracy: 0.4939 - val_loss: 3.0734 - val_categorical_accuracy: 0.5124\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.49087 to 3.07341, saving model to lstm_gru.h5\n",
      "Epoch 3/30\n",
      "274/274 - 25s - loss: 2.6673 - categorical_accuracy: 0.5445 - val_loss: 2.7395 - val_categorical_accuracy: 0.5551\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.07341 to 2.73948, saving model to lstm_gru.h5\n",
      "Epoch 4/30\n",
      "274/274 - 25s - loss: 2.2402 - categorical_accuracy: 0.5841 - val_loss: 2.5325 - val_categorical_accuracy: 0.5758\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.73948 to 2.53248, saving model to lstm_gru.h5\n",
      "Epoch 5/30\n",
      "274/274 - 28s - loss: 1.8911 - categorical_accuracy: 0.6190 - val_loss: 2.3928 - val_categorical_accuracy: 0.5910\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.53248 to 2.39278, saving model to lstm_gru.h5\n",
      "Epoch 6/30\n",
      "274/274 - 26s - loss: 1.6161 - categorical_accuracy: 0.6499 - val_loss: 2.3126 - val_categorical_accuracy: 0.6054\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.39278 to 2.31257, saving model to lstm_gru.h5\n",
      "Epoch 7/30\n",
      "274/274 - 27s - loss: 1.3842 - categorical_accuracy: 0.6799 - val_loss: 2.2752 - val_categorical_accuracy: 0.6143\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.31257 to 2.27520, saving model to lstm_gru.h5\n",
      "Epoch 8/30\n",
      "274/274 - 29s - loss: 1.2037 - categorical_accuracy: 0.7102 - val_loss: 2.2547 - val_categorical_accuracy: 0.6223\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.27520 to 2.25474, saving model to lstm_gru.h5\n",
      "Epoch 9/30\n",
      "274/274 - 27s - loss: 1.0565 - categorical_accuracy: 0.7359 - val_loss: 2.2420 - val_categorical_accuracy: 0.6252\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.25474 to 2.24198, saving model to lstm_gru.h5\n",
      "Epoch 10/30\n",
      "274/274 - 27s - loss: 0.9380 - categorical_accuracy: 0.7587 - val_loss: 2.2631 - val_categorical_accuracy: 0.6262\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.24198\n",
      "Epoch 11/30\n",
      "274/274 - 27s - loss: 0.8626 - categorical_accuracy: 0.7742 - val_loss: 2.2543 - val_categorical_accuracy: 0.6293\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.24198\n",
      "Epoch 12/30\n",
      "274/274 - 26s - loss: 0.7932 - categorical_accuracy: 0.7877 - val_loss: 2.2883 - val_categorical_accuracy: 0.6330\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.24198\n",
      "Epoch 13/30\n",
      "274/274 - 26s - loss: 0.7178 - categorical_accuracy: 0.8037 - val_loss: 2.3132 - val_categorical_accuracy: 0.6338\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.24198\n",
      "Epoch 14/30\n",
      "274/274 - 26s - loss: 0.6751 - categorical_accuracy: 0.8158 - val_loss: 2.3466 - val_categorical_accuracy: 0.6295\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.24198\n",
      "Epoch 15/30\n",
      "274/274 - 27s - loss: 0.6379 - categorical_accuracy: 0.8238 - val_loss: 2.3581 - val_categorical_accuracy: 0.6378\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 2.24198\n",
      "Epoch 16/30\n",
      "274/274 - 25s - loss: 0.6212 - categorical_accuracy: 0.8259 - val_loss: 2.3877 - val_categorical_accuracy: 0.6326\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 2.24198\n",
      "Epoch 17/30\n",
      "274/274 - 298s - loss: 0.6047 - categorical_accuracy: 0.8287 - val_loss: 2.4111 - val_categorical_accuracy: 0.6342\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 2.24198\n",
      "Epoch 18/30\n",
      "274/274 - 209s - loss: 0.6060 - categorical_accuracy: 0.8289 - val_loss: 2.4456 - val_categorical_accuracy: 0.6273\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 2.24198\n",
      "Epoch 19/30\n",
      "274/274 - 29s - loss: 0.6017 - categorical_accuracy: 0.8297 - val_loss: 2.4642 - val_categorical_accuracy: 0.6326\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 2.24198\n",
      "Epoch 20/30\n",
      "274/274 - 28s - loss: 0.5647 - categorical_accuracy: 0.8375 - val_loss: 2.4801 - val_categorical_accuracy: 0.6317\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 2.24198\n",
      "Epoch 21/30\n",
      "274/274 - 26s - loss: 0.5349 - categorical_accuracy: 0.8459 - val_loss: 2.5183 - val_categorical_accuracy: 0.6281\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 2.24198\n",
      "Epoch 22/30\n",
      "274/274 - 28s - loss: 0.5306 - categorical_accuracy: 0.8455 - val_loss: 2.5106 - val_categorical_accuracy: 0.6314\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 2.24198\n",
      "Epoch 23/30\n",
      "274/274 - 28s - loss: 0.5352 - categorical_accuracy: 0.8440 - val_loss: 2.5320 - val_categorical_accuracy: 0.6313\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 2.24198\n",
      "Epoch 24/30\n",
      "274/274 - 26s - loss: 0.5147 - categorical_accuracy: 0.8496 - val_loss: 2.5552 - val_categorical_accuracy: 0.6319\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 2.24198\n",
      "Epoch 25/30\n",
      "274/274 - 27s - loss: 0.4831 - categorical_accuracy: 0.8573 - val_loss: 2.6142 - val_categorical_accuracy: 0.6262\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 2.24198\n",
      "Epoch 26/30\n",
      "274/274 - 27s - loss: 0.5074 - categorical_accuracy: 0.8521 - val_loss: 2.6203 - val_categorical_accuracy: 0.6319\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 2.24198\n",
      "Epoch 27/30\n",
      "274/274 - 29s - loss: 0.4987 - categorical_accuracy: 0.8541 - val_loss: 2.6273 - val_categorical_accuracy: 0.6290\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 2.24198\n",
      "Epoch 28/30\n",
      "274/274 - 28s - loss: 0.4950 - categorical_accuracy: 0.8549 - val_loss: 2.6478 - val_categorical_accuracy: 0.6274\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 2.24198\n",
      "Epoch 29/30\n",
      "274/274 - 25s - loss: 0.4855 - categorical_accuracy: 0.8572 - val_loss: 2.6646 - val_categorical_accuracy: 0.6317\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 2.24198\n",
      "Epoch 30/30\n",
      "274/274 - 25s - loss: 0.4684 - categorical_accuracy: 0.8606 - val_loss: 2.6615 - val_categorical_accuracy: 0.6316\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 2.24198\n"
     ]
    }
   ],
   "source": [
    "# Fitting the Model-4\n",
    "\n",
    "filename = 'lstm_gru.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "history_lstm_gru = lstm_gru.fit(x_train, y_train, epochs=30, batch_size=64, validation_data=(x_valid, y_valid), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        loss  categorical_accuracy  val_loss  val_categorical_accuracy\n",
      "0   3.793427              0.441154  3.208919                   0.49128\n",
      "1   2.833559              0.533040  2.711730                   0.55744\n",
      "2   2.293599              0.584000  2.452090                   0.59016\n",
      "3   1.895594              0.622914  2.316713                   0.60192\n",
      "4   1.579440              0.659497  2.236204                   0.61992\n",
      "5   1.334634              0.693417  2.175927                   0.62880\n",
      "6   1.123589              0.726263  2.158596                   0.63616\n",
      "7   0.972408              0.754206  2.154496                   0.64736\n",
      "8   0.849356              0.779669  2.160844                   0.64736\n",
      "9   0.746623              0.800469  2.180151                   0.64880\n",
      "10  0.660448              0.818903  2.209576                   0.65040\n",
      "11  0.609541              0.832423  2.229174                   0.65304\n",
      "12  0.561103              0.841737  2.251234                   0.65488\n",
      "13  0.518969              0.852217  2.271991                   0.65240\n",
      "14  0.488100              0.860594  2.277006                   0.65456\n",
      "15  0.467883              0.863246  2.331982                   0.65448\n",
      "16  0.465596              0.863417  2.343225                   0.65360\n",
      "17  0.438265              0.869474  2.393771                   0.65440\n",
      "18  0.423068              0.874480  2.404759                   0.65912\n",
      "19  0.405823              0.877863  2.405482                   0.65520\n",
      "20  0.390520              0.882046  2.428676                   0.65592\n",
      "21  0.369550              0.888834  2.447434                   0.65544\n",
      "22  0.370651              0.887051  2.480661                   0.64984\n",
      "23  0.373507              0.885417  2.519141                   0.65264\n",
      "24  0.375611              0.885211  2.526074                   0.64808\n",
      "25  0.374611              0.884926  2.535545                   0.64584\n",
      "26  0.379696              0.883554  2.558910                   0.64464\n",
      "27  0.377861              0.883703  2.558459                   0.64848\n",
      "28  0.371476              0.885154  2.591076                   0.64608\n",
      "29  0.368846              0.886137  2.605842                   0.64640\n"
     ]
    }
   ],
   "source": [
    "# Saving the history of the Model-4\n",
    "\n",
    "df_history_lstm_gru = pd.DataFrame(history_both_lstm.history)\n",
    "df_history_lstm_gru.to_csv('df_history_lstm_gru.csv')\n",
    "print(df_history_lstm_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Model-4\n",
    "lstm_gru = load_model('lstm_gru.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French(Source) : appretetoi\n",
      "Target : prepare yourself\n",
      "Predicted : prepare down \n",
      "\n",
      "French(Source) : avezvous pris une douche\n",
      "Target : have you showered\n",
      "Predicted : you you showered \n",
      "\n",
      "French(Source) : rentre a la maison\n",
      "Target : go home\n",
      "Predicted : come home home \n",
      "\n",
      "French(Source) : ils ont tort\n",
      "Target : they are wrong\n",
      "Predicted : they all wrong \n",
      "\n",
      "French(Source) : aidemoi a sortir\n",
      "Target : help me out\n",
      "Predicted : let me out \n",
      "\n",
      "French(Source) : prenez les commandes\n",
      "Target : take command\n",
      "Predicted : take control \n",
      "\n",
      "French(Source) : restez ou vous etes\n",
      "Target : hold it\n",
      "Predicted : turn home \n",
      "\n",
      "French(Source) : je suis ruinee\n",
      "Target : im ruined\n",
      "Predicted : im rested \n",
      "\n",
      "French(Source) : je ne suis pas malheureux\n",
      "Target : im not unhappy\n",
      "Predicted : im not unhappy \n",
      "\n",
      "French(Source) : je peux y parvenir\n",
      "Target : i can do it\n",
      "Predicted : i can do it \n",
      "\n",
      "French(Source) : jaime tes jambes\n",
      "Target : i like your legs\n",
      "Predicted : i love your legs \n",
      "\n",
      "French(Source) : maitrisezvous\n",
      "Target : control yourself\n",
      "Predicted : control yourself \n",
      "\n",
      "French(Source) : je suis a moitie japonaise\n",
      "Target : im half japanese\n",
      "Predicted : im an japanese \n",
      "\n",
      "French(Source) : en estu sure\n",
      "Target : are you certain\n",
      "Predicted : are you certain sure \n",
      "\n",
      "French(Source) : jaime ton sourire\n",
      "Target : i like your smile\n",
      "Predicted : i like your \n",
      "\n",
      "French(Source) : klaxonnez\n",
      "Target : honk the horn\n",
      "Predicted : whats the seat \n",
      "\n",
      "French(Source) : tu es grossier\n",
      "Target : youre rude\n",
      "Predicted : youre rude rude \n",
      "\n",
      "French(Source) : nous devrions partir\n",
      "Target : we should go\n",
      "Predicted : we must go go \n",
      "\n",
      "French(Source) : jai oublie la carte\n",
      "Target : i forgot the map\n",
      "Predicted : i forgot the map \n",
      "\n",
      "French(Source) : ils font des efforts\n",
      "Target : theyre trying\n",
      "Predicted : theyre trying \n",
      "\n",
      "French(Source) : tom sen allait\n",
      "Target : tom was leaving\n",
      "Predicted : tom has photos \n",
      "\n",
      "French(Source) : ne me parle pas\n",
      "Target : dont talk to me\n",
      "Predicted : dont deceive me me \n",
      "\n",
      "French(Source) : tom etait ici\n",
      "Target : tom was here\n",
      "Predicted : tom was here \n",
      "\n",
      "French(Source) : jai termine premier\n",
      "Target : i finished first\n",
      "Predicted : i finished first \n",
      "\n",
      "French(Source) : elle garde des secrets\n",
      "Target : she keeps secrets\n",
      "Predicted : she plays secrets \n",
      "\n",
      "French(Source) : mordstoi la langue\n",
      "Target : bite your tongue\n",
      "Predicted : bite your tongue \n",
      "\n",
      "French(Source) : je veux faire ceci\n",
      "Target : i want to do this\n",
      "Predicted : i want to this this \n",
      "\n",
      "French(Source) : vous etes les vainqueurs\n",
      "Target : youve won\n",
      "Predicted : youve anyway \n",
      "\n",
      "French(Source) : inutile de discuter\n",
      "Target : dont argue\n",
      "Predicted : dont argue \n",
      "\n",
      "French(Source) : jai besoin dun taxi\n",
      "Target : i need a cab\n",
      "Predicted : i need a cab map \n",
      "\n",
      "French(Source) : je nai pas dexcuse\n",
      "Target : i have no excuse\n",
      "Predicted : i have no excuse \n",
      "\n",
      "French(Source) : je peux la reparer\n",
      "Target : i can fix it\n",
      "Predicted : i can fix it \n",
      "\n",
      "French(Source) : je trouvais ce vin bon\n",
      "Target : i liked that wine\n",
      "Predicted : i hate this \n",
      "\n",
      "French(Source) : cesse de hurler\n",
      "Target : stop screaming\n",
      "Predicted : stop pushing \n",
      "\n",
      "French(Source) : je resterai debout\n",
      "Target : ill stand\n",
      "Predicted : ill going \n",
      "\n",
      "French(Source) : jaime le rock\n",
      "Target : i like rock music\n",
      "Predicted : i like rock music \n",
      "\n",
      "French(Source) : tom est juste\n",
      "Target : tom is fair\n",
      "Predicted : tom is fair \n",
      "\n",
      "French(Source) : regardez a nouveau\n",
      "Target : look again\n",
      "Predicted : look us again \n",
      "\n",
      "French(Source) : je protestais\n",
      "Target : i objected\n",
      "Predicted : i protested \n",
      "\n",
      "French(Source) : nous sommes au regime\n",
      "Target : were dieting\n",
      "Predicted : were dieting \n",
      "\n",
      "French(Source) : vous etes tous heureux\n",
      "Target : youre all happy\n",
      "Predicted : youre all happy \n",
      "\n",
      "French(Source) : je suis connue\n",
      "Target : im famous\n",
      "Predicted : im famous \n",
      "\n",
      "French(Source) : etesvous blesses\n",
      "Target : are you injured\n",
      "Predicted : are you injured \n",
      "\n",
      "French(Source) : fais un calin a tom\n",
      "Target : hug tom\n",
      "Predicted : everyone tom attacked \n",
      "\n",
      "French(Source) : astu un animal domestique\n",
      "Target : do you have a pet\n",
      "Predicted : do you have a pet \n",
      "\n",
      "French(Source) : tom doit rester\n",
      "Target : tom has to stay\n",
      "Predicted : tom can stay stay \n",
      "\n",
      "French(Source) : ou est la voiture\n",
      "Target : wheres the car\n",
      "Predicted : wheres is car \n",
      "\n",
      "French(Source) : deverrouillele\n",
      "Target : unlock it\n",
      "Predicted : lighten it \n",
      "\n",
      "French(Source) : je paierai\n",
      "Target : ill pay\n",
      "Predicted : ill pay \n",
      "\n",
      "French(Source) : monte au sommet\n",
      "Target : climb to the top\n",
      "Predicted : go on the box \n",
      "\n",
      "BLEU score: 0.274282\n"
     ]
    }
   ],
   "source": [
    "# Model-4 on training sequences\n",
    "\n",
    "bleu_train_lstm_gru = model_evaluation(lstm_gru, eng_tokenizer, x_train, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French(Source) : jai oublie le livre\n",
      "Target : i forgot the book\n",
      "Predicted : i wrote the book \n",
      "\n",
      "French(Source) : tom est sans domicile fixe\n",
      "Target : toms homeless\n",
      "Predicted : tom is overjoyed \n",
      "\n",
      "French(Source) : jaide tom\n",
      "Target : i help tom\n",
      "Predicted : describe tom \n",
      "\n",
      "French(Source) : jai achete un cactus\n",
      "Target : i bought a cactus\n",
      "Predicted : i have a grenade \n",
      "\n",
      "French(Source) : cest plutot grand\n",
      "Target : its quite large\n",
      "Predicted : its too large \n",
      "\n",
      "French(Source) : ouvrez le coffre\n",
      "Target : open the safe\n",
      "Predicted : lock the safe \n",
      "\n",
      "French(Source) : il est trop confiant\n",
      "Target : hes too trusting\n",
      "Predicted : hes too cloudy \n",
      "\n",
      "French(Source) : il adore les jouets\n",
      "Target : he loves toys\n",
      "Predicted : he sold his \n",
      "\n",
      "French(Source) : estce que vous avez le temps\n",
      "Target : do you have time\n",
      "Predicted : do you wasting \n",
      "\n",
      "French(Source) : jai presente mes excuses\n",
      "Target : i have apologized\n",
      "Predicted : i just an \n",
      "\n",
      "French(Source) : il nous faut de leau\n",
      "Target : we need water\n",
      "Predicted : i need more water \n",
      "\n",
      "French(Source) : cest simple nestce pas\n",
      "Target : simple isnt it\n",
      "Predicted : its doesnt fit \n",
      "\n",
      "French(Source) : nous allons camper ici\n",
      "Target : well camp here\n",
      "Predicted : were live here \n",
      "\n",
      "French(Source) : oublie tom\n",
      "Target : never mind tom\n",
      "Predicted : call tom \n",
      "\n",
      "French(Source) : jaime les recits\n",
      "Target : i like stories\n",
      "Predicted : i like you \n",
      "\n",
      "French(Source) : etesvous enseignante\n",
      "Target : are you a teacher\n",
      "Predicted : you you a lawyer \n",
      "\n",
      "French(Source) : cest la norme\n",
      "Target : this is the norm\n",
      "Predicted : its is \n",
      "\n",
      "French(Source) : nous ne sommes pas sures\n",
      "Target : were not sure\n",
      "Predicted : were not stupid \n",
      "\n",
      "French(Source) : ca a de limportance pour moi\n",
      "Target : it matters to me\n",
      "Predicted : it is to me \n",
      "\n",
      "French(Source) : estu maniaque\n",
      "Target : are you a maniac\n",
      "Predicted : are you are \n",
      "\n",
      "French(Source) : je dois mechapper\n",
      "Target : i have to escape\n",
      "Predicted : i must to to \n",
      "\n",
      "French(Source) : arrete ca maintenant\n",
      "Target : stop that now\n",
      "Predicted : stop stop \n",
      "\n",
      "French(Source) : estu epuise\n",
      "Target : are you exhausted\n",
      "Predicted : are you knackered \n",
      "\n",
      "French(Source) : attendez\n",
      "Target : wait\n",
      "Predicted : hold on \n",
      "\n",
      "French(Source) : il est sexy\n",
      "Target : hes sexy\n",
      "Predicted : hes is nasty \n",
      "\n",
      "French(Source) : etesvous droguee\n",
      "Target : are you on dope\n",
      "Predicted : are you an dope \n",
      "\n",
      "French(Source) : jirai me changer\n",
      "Target : ill go change\n",
      "Predicted : ill see on \n",
      "\n",
      "French(Source) : il est intelligent\n",
      "Target : he is intelligent\n",
      "Predicted : hes intelligent \n",
      "\n",
      "French(Source) : ils le connaissent\n",
      "Target : they know him\n",
      "Predicted : they you you \n",
      "\n",
      "French(Source) : lastu emporte\n",
      "Target : did you win\n",
      "Predicted : you you win \n",
      "\n",
      "French(Source) : nous avons pris le petit dejeuner\n",
      "Target : we had breakfast\n",
      "Predicted : weve have breakfast \n",
      "\n",
      "French(Source) : personne na ete vire\n",
      "Target : no one was fired\n",
      "Predicted : no one fired fired \n",
      "\n",
      "French(Source) : elle est maladroite\n",
      "Target : she is awkward\n",
      "Predicted : she is japanese \n",
      "\n",
      "French(Source) : tom a ralenti\n",
      "Target : tom slowed down\n",
      "Predicted : tom suggested \n",
      "\n",
      "French(Source) : estu seul\n",
      "Target : are you single\n",
      "Predicted : are you lonely \n",
      "\n",
      "French(Source) : elles sont formidables\n",
      "Target : theyre great\n",
      "Predicted : theyre green \n",
      "\n",
      "French(Source) : sil te plait chante\n",
      "Target : please sing\n",
      "Predicted : please and \n",
      "\n",
      "French(Source) : astu fait ceci\n",
      "Target : did you make this\n",
      "Predicted : did you this this \n",
      "\n",
      "French(Source) : sautez de lautre cote\n",
      "Target : jump across\n",
      "Predicted : move across \n",
      "\n",
      "French(Source) : tu es si naive\n",
      "Target : youre so naive\n",
      "Predicted : youre so naive \n",
      "\n",
      "French(Source) : viens jouer avec nous\n",
      "Target : come play with us\n",
      "Predicted : come on us us \n",
      "\n",
      "French(Source) : enfile ton pantalon\n",
      "Target : put your pants on\n",
      "Predicted : put your your pants \n",
      "\n",
      "French(Source) : nous sommes tous en train de mourir\n",
      "Target : were all dying\n",
      "Predicted : were all died \n",
      "\n",
      "French(Source) : nous ne sommes pas perdues\n",
      "Target : were not lost\n",
      "Predicted : were not no \n",
      "\n",
      "French(Source) : jen veux un\n",
      "Target : i want one\n",
      "Predicted : i want one one \n",
      "\n",
      "French(Source) : jetais dans la voiture\n",
      "Target : i was in the car\n",
      "Predicted : i like my car car \n",
      "\n",
      "French(Source) : sil vous plait soyez prudent\n",
      "Target : please be careful\n",
      "Predicted : please be careful \n",
      "\n",
      "French(Source) : cest juste faux\n",
      "Target : its just wrong\n",
      "Predicted : were so untrue \n",
      "\n",
      "French(Source) : offremoi un verre\n",
      "Target : buy me a drink\n",
      "Predicted : give me a drink \n",
      "\n",
      "French(Source) : ils apprecient le francais\n",
      "Target : they like french\n",
      "Predicted : we we french \n",
      "\n",
      "BLEU score: 0.108294\n"
     ]
    }
   ],
   "source": [
    "# Testing Model-4 on test sequences\n",
    "\n",
    "bleu_test_lstm_gru = model_evaluation(lstm_gru, eng_tokenizer, x_test, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
